{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-5600/6600 Lecture 16 - Support Vector Machines (SVM)\n",
        "\n",
        "**Instructor: Dylan Zwick**\n",
        "\n",
        "*Weber State University*\n",
        "\n",
        "References:\n",
        "* [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) by Aurélien Géron - [Support Vector Machines](https://github.com/ageron/handson-ml3/blob/main/05_support_vector_machines.ipynb)\n",
        "* [An Introduction to Statistical Learning](https://www.statlearning.com/) by James, Witten, Hastie, Tibshirani, and Taylor"
      ],
      "metadata": {
        "id": "q6fxSswH9EwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "metadata": {
        "id": "h5tyIncLRwBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we're going to talk about something that's very important in machine learning and, really, in life itself - knowing where to draw the line.\n",
        "\n",
        "But first, what is a line?\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1oXXGBPcJjtJD7fn154t6czxA_HdqS2mB\" alt=\"A Line Is A Curve\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "7NRtpcBcBBzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How To Draw Lines###"
      ],
      "metadata": {
        "id": "KFHRrZfRCdgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic formula for a line, which is the formula we used in linear regression, is:\n",
        "\n",
        "<center>\n",
        "  $y = mx + b$\n",
        "</center>\n",
        "\n",
        "This is called *slope-intercept* form, and it's the form you want to use if you're representing a function. You plug in the value of $x$, and it gives you the value of $y$.\n",
        "\n",
        "Now, there's another way you can express a line algebraically, and it's called *standard form*. In this form you write the line as:\n",
        "\n",
        "<center>\n",
        "  $ax + by = c$\n",
        "</center>\n",
        "\n",
        "The line is defined as all the values of $x$ and $y$ that satisfy this relationship. It's usually straightforward to convert from standard form to slope-intercept form:\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle y = -\\left(\\frac{a}{b}\\right)x + \\frac{c}{b}$\n",
        "</center>\n",
        "\n",
        "However, this only works if $b \\neq 0$. If $b = 0$ then the standard form equation is:\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle x = \\left(\\frac{c}{b}\\right)$\n",
        "</center>\n",
        "\n",
        "In other words, it's a vertical line. (Note we need to assume that either $a$ or $b$ is non-zero). While this would be madness for a function, it's perfectly acceptable for a line, and it's an option we'll want. So, for this lecture we'll use the standard form."
      ],
      "metadata": {
        "id": "5G3FPTqgCiQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Picking Sides###"
      ],
      "metadata": {
        "id": "eXikGIOrRG4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's bring back our old friend, the Iris dataset."
      ],
      "metadata": {
        "id": "4bgXg2B0RKLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris(as_frame=True)\n",
        "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
        "y = iris.target\n",
        "\n",
        "setosa_or_versicolor = (y == 0) | (y == 1)\n",
        "X = X[setosa_or_versicolor]\n",
        "y = y[setosa_or_versicolor]"
      ],
      "metadata": {
        "id": "HCsceUEPRtj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And take a look at its values in a chart:"
      ],
      "metadata": {
        "id": "1HlTFfeOSfje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Petal width\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rNBZ-63VSXzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we wanted to build a model that predicts - based on petal width and petal length - whether a new observation is a versicolor or a setosa? One way to do this would be to just draw a line and say anything to one side is versicolor, and anything to the other side is setosa."
      ],
      "metadata": {
        "id": "ni1iqY-bSlz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How would we formulate this mathematically? Well, our equation for a line is:\n",
        "\n",
        "<center>\n",
        "  $\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} = 0$\n",
        "</center>\n",
        "\n",
        "*Note* - We moved the constant to the same side as the variables, and switched $x$ and $y$ for $X_{1}$ and $X_{2}$, and $c$, $a$, and $b$ for $\\beta_{0}$, $\\beta_{1}$ and $\\beta_{2}$ because that will make it easier to generalize to more dimensions later.\n",
        "\n",
        "Suppose I get a new observation and its petal length and width values are $(x_{1},x_{2})$. If that observation happens to be directly on my line, then we'll have:\n",
        "\n",
        "<center>\n",
        "  $\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} = 0$.\n",
        "</center>\n",
        "\n",
        "If it's not on the line, then we will either have:\n",
        "\n",
        "<center>\n",
        "  $\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} > 0$,\n",
        "</center>\n",
        "\n",
        "or\n",
        "\n",
        "<center>\n",
        "  $\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} < 0$.\n",
        "</center>\n",
        "\n",
        "These possibilities correspond to the *side* of the line where we find $(x_{1},x_{2})$. So, our model would take the values of our observation, namely $(x_{1},x_{2})$, plug them into the formula for our line, namely $\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2}$, and check its sign."
      ],
      "metadata": {
        "id": "LPmO8QG2TDPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1mIlRJaL07QJI56tHn4R1o4hTc-xCJ-7C\" alt=\"What's Your Sign?\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "ZbAmM_APmizx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, the value of $\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2}$ will be the distance from the line to the point, where a positive value is distance on one side, and a negative is distance on the other."
      ],
      "metadata": {
        "id": "_q_kSPkxxaON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Drawing The Line###"
      ],
      "metadata": {
        "id": "guAqPsbDmzwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we know how we can create a predictive model from a simple line. But of course the big question now is - which line? There are many lines that we could draw on our iris chart. Which line is best?\n",
        "\n",
        "Let's take a look at a few:"
      ],
      "metadata": {
        "id": "wXNC2Gacm2sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bad models\n",
        "x0 = np.linspace(0, 5.5, 200)\n",
        "pred_1 = 5 * x0 - 20\n",
        "pred_2 = x0 - 1.8\n",
        "pred_3 = 0.1 * x0 + 0.5\n",
        "\n",
        "plt.plot(x0, pred_1, \"g--\", linewidth=2)\n",
        "plt.plot(x0, pred_2, \"m-\", linewidth=2)\n",
        "plt.plot(x0, pred_3, \"r-\", linewidth=2)\n",
        "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Petal width\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.axis([0, 5.5, 0, 2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N-A_-7BvnczE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think we can probably toss out that dotted green line right away, as it fails to separate the data. But what about the other two? Both the blue and red line separate the data - but do they do a good job? If a new prediction were just to the upper-left of the blue line, would you agree it should be a setosa? What about the bottom-right of the red being versicolor?\n",
        "\n",
        "I think we'd agree that neither the red nor blue line are good models. But what would be a good model?"
      ],
      "metadata": {
        "id": "r5kxmaT9oE79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where we introduce the idea of a **maximal margin classifier**. The idea is that the maximal margin classifier is the line that separates the data, and is farthest away from it. For the iris data this line would look like:"
      ],
      "metadata": {
        "id": "8SYZE7lcoow3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't worry about understanding this code. For right now it's just for plotting the picture.\n",
        "\n",
        "# SVM Classifier model\n",
        "svm_clf = SVC(kernel=\"linear\", C=1e100)\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
        "    w = svm_clf.coef_[0]\n",
        "    b = svm_clf.intercept_[0]\n",
        "\n",
        "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
        "    # => x1 = -w0/w1 * x0 - b/w1\n",
        "    x0 = np.linspace(xmin, xmax, 200)\n",
        "    decision_boundary = -w[0] / w[1] * x0 - b / w[1]\n",
        "\n",
        "    margin = 1/w[1]\n",
        "    gutter_up = decision_boundary + margin\n",
        "    gutter_down = decision_boundary - margin\n",
        "    svs = svm_clf.support_vectors_\n",
        "\n",
        "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2, zorder=-2)\n",
        "    plt.plot(x0, gutter_up, \"k--\", linewidth=2, zorder=-2)\n",
        "    plt.plot(x0, gutter_down, \"k--\", linewidth=2, zorder=-2)\n",
        "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#AAA',\n",
        "                zorder=-1)\n",
        "\n",
        "plot_svc_decision_boundary(svm_clf, 0, 5.5)\n",
        "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
        "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.axis([0, 5.5, 0, 2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BxBn3QB_pDmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line in the center is called the *maximal margin classifier*. The distance from the center line to either dotted line is the *margin*, and the two observations on the boundary of the margin are called the *support vectors*. Thus the name!"
      ],
      "metadata": {
        "id": "gl-hmPvDqXcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###To Higher Dimensions###"
      ],
      "metadata": {
        "id": "6cxcrOWwq-xA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lines in planes are simple and (relatively) easy to visualize. But most of the time our data is higher dimensional. In higher dimensions, we can generalize our equation for a line into an equation for a *hyperplane*:\n",
        "\n",
        "<center>\n",
        "  $\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{n}X_{n} = 0$\n",
        "</center>\n",
        "\n",
        "If $n = 3$ then this is just the equation for a two-dimensional flat surface in three-dimensional space. In other words, a plane.\n",
        "\n",
        "The exact same ideas, mutatis mutandis, around margin, maximum margin classifiers, and support vectors extend to hyperplanes.\n",
        "\n",
        "We note one thing here and that is that we can have more that one set of coefficients generating the same hyperplane. For example, if we multiplied all the coefficients by the same number, say $2$, then we'd have:\n",
        "\n",
        "<center>\n",
        "  $2\\beta_{0} + 2\\beta_{1}X_{1} + 2\\beta_{2}X_{2} + \\cdots + 2\\beta_{n}X_{n} = 0$\n",
        "</center>\n",
        "\n",
        "This would be true if and only if\n",
        "\n",
        "<center>\n",
        "  $\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{n}X_{n} = 0$\n",
        "</center>\n",
        "\n",
        "were true. So, we can scale our coefficients by a constant amount, and we generally choose to do so such that\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle \\sum_{j = 1}^{M} \\beta_{j}^{2} = 1$\n",
        "</center>\n",
        "\n",
        "You can view this as the coefficients $\\beta_{1}, \\beta_{2}, \\ldots, \\beta_{n}$ representing the components of a unit vector perpendicular to the plane. This sets the orientation of the plane. The constant term $\\beta_{0}$ just translates the plane away from the origin."
      ],
      "metadata": {
        "id": "PoMMfcDCrOr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Constructing the Maximal Margin Classifier###"
      ],
      "metadata": {
        "id": "xBRD07W2vPTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a set of $n$ training observations $x_{1}, x_{2}, \\ldots, x_{n}$, and each training observation has $p$ measured features, so for example $x_{1} = (x_{11}, x_{12}, \\ldots, x_{1p})$. Each observation also has a corresponding class label $y_{1}, y_{2}, \\ldots, y_{n}$ which is either $-1$ or $1$. Finding the maximal margin classifier is solving the following optimization problem:\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle \\underset{\\beta_{0},\\beta_{1},\\ldots,\\beta_{p},M}{maximize} M$\n",
        "\n",
        "  subject to $\\displaystyle \\sum_{j = 1}^{p}\\beta_{j}^{2}$\n",
        "\n",
        "  $\\displaystyle y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots + \\beta_{p}x_{ip}) \\geq M$\n",
        "</center>\n",
        "\n",
        "We've discussed the first constraint. The second constraint just says that every observation has to lie a distance greater than or equal to $M$ away from the line $\\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p} = 0$."
      ],
      "metadata": {
        "id": "YnKXzUtwvVoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a problem in [convex optimization](https://en.wikipedia.org/wiki/Convex_optimization). The idea is that you're trying to optimize a value subject to being constrained within a convex space. It's a very interesting area of mathematics, which we don't have time to study. So, we won't get into how we solve this problem, but just understand it's the problem that needs to be solved to find the maximal margin classifier."
      ],
      "metadata": {
        "id": "pNHnVPy7xw_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Support Vector Classifier##"
      ],
      "metadata": {
        "id": "mRDgZPoryTKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK. So now we know how to draw the line when we've got linearly separable data. Are we done? Well, unfortunately for us, this method has some problems.\n",
        "\n",
        "First, it's very dependent on the scale of the data. Fortunately, this can be fixed by the appropriate scaling of our features."
      ],
      "metadata": {
        "id": "REJcWDGRyVvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\n",
        "ys = np.array([0, 0, 1, 1])\n",
        "svm_clf = SVC(kernel=\"linear\", C=100).fit(Xs, ys)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(Xs)\n",
        "svm_clf_scaled = SVC(kernel=\"linear\", C=100).fit(X_scaled, ys)\n",
        "\n",
        "plt.figure(figsize=(9, 2.7))\n",
        "plt.subplot(121)\n",
        "plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\n",
        "plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\n",
        "plot_svc_decision_boundary(svm_clf, 0, 6)\n",
        "plt.xlabel(\"$x_0$\")\n",
        "plt.ylabel(\"$x_1$    \", rotation=0)\n",
        "plt.title(\"Unscaled\")\n",
        "plt.axis([0, 6, 0, 90])\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\n",
        "plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\n",
        "plot_svc_decision_boundary(svm_clf_scaled, -2, 2)\n",
        "plt.xlabel(\"$x'_0$\")\n",
        "plt.ylabel(\"$x'_1$  \", rotation=0)\n",
        "plt.title(\"Scaled\")\n",
        "plt.axis([-2, 2, -2, 2])\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oTP7DvGp83fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, it can be significantly influenced by just a single outlier datapoint. For example:"
      ],
      "metadata": {
        "id": "5SLnv63E8jKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_outlier = np.array([3.2, 0.8])\n",
        "y_outlier = 0\n",
        "\n",
        "Xo = np.concatenate([X, X_outlier.reshape(1, -1)], axis=0)\n",
        "yo = np.concatenate([y, [y_outlier]], axis=0)\n",
        "\n",
        "svm_clf2 = SVC(kernel=\"linear\", C=10**9)\n",
        "svm_clf2.fit(Xo, yo)\n",
        "\n",
        "plt.plot(Xo[:, 0][yo==1], Xo[:, 1][yo==1], \"bs\")\n",
        "plt.plot(Xo[:, 0][yo==0], Xo[:, 1][yo==0], \"yo\")\n",
        "plot_svc_decision_boundary(svm_clf2, 0, 5.5)\n",
        "plt.ylabel(\"Sepal length\")\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.annotate(\n",
        "    \"Outlier\",\n",
        "    xy=(X_outlier[0], X_outlier[1]),\n",
        "    xytext=(3.2, 0.08),\n",
        "    ha=\"center\",\n",
        "    arrowprops=dict(facecolor='black', shrink=0.1),\n",
        ")\n",
        "plt.axis([0, 5.5, 0, 2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fPCmXmu2zs_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the one outlier datapoint complete transforms our maximal margin classifier line and its margin."
      ],
      "metadata": {
        "id": "j_5Zm8wS1fcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, and worst of all, I know it's hard to believe but sometimes it's not possible to separate classes with a line!"
      ],
      "metadata": {
        "id": "gCFit6Zs1nWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_outlier = np.array([3.4, 1.3])\n",
        "y_outlier = 0\n",
        "\n",
        "Xo = np.concatenate([X, X_outlier.reshape(1, -1)], axis=0)\n",
        "yo = np.concatenate([y, [y_outlier]], axis=0)\n",
        "\n",
        "plt.plot(Xo[:, 0][yo==1], Xo[:, 1][yo==1], \"bs\")\n",
        "plt.plot(Xo[:, 0][yo==0], Xo[:, 1][yo==0], \"yo\")\n",
        "plt.text(0.3, 1.0, \"Impossible!\", color=\"red\", fontsize=18)\n",
        "plt.ylabel(\"Sepal length\")\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.annotate(\n",
        "    \"Outlier\",\n",
        "    xy=(X_outlier[0], X_outlier[1]),\n",
        "    xytext=(2.5, 1.7),\n",
        "    ha=\"center\",\n",
        "    arrowprops=dict(facecolor='black', shrink=0.1),\n",
        ")\n",
        "plt.axis([0, 5.5, 0, 2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pSrJICv51vf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do we do?!? Well, we learn how to accept some imperfection in our model. More specifically, in the interest of\n",
        "\n",
        "* Greater robustness to individual observations, and\n",
        "* Better classification of *most* of the training observations\n",
        "\n",
        "We allow our model to have a few observations on the wrong side of the margin, or even to misclassify a few training observations. We call this a *soft margin classifier*."
      ],
      "metadata": {
        "id": "MuzrAZ_02sTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Constructing The Support Vector Classifier###"
      ],
      "metadata": {
        "id": "kPKyzNRj39Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The support vector classifier classifies a test observation depending on which side of the hyperplane it's on. The hyperplane is chosen to correctly separate most of the training observations into two classes, but may misclassify a few. Precisely, it's the solution to the optimization problem:"
      ],
      "metadata": {
        "id": "5OtmUc8B4D03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  $\\displaystyle \\underset{\\beta_{0},\\beta_{1},\\ldots,\\beta_{p},\\epsilon_{1},\\ldots,\\epsilon_{n},M}{maximize} M$\n",
        "\n",
        "  subject to $\\displaystyle \\sum_{j = 1}^{p}\\beta_{j}^{2}$\n",
        "\n",
        "  $\\displaystyle y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\cdots + \\beta_{p}x_{ip}) \\geq M(1-\\epsilon_{i})$\n",
        "\n",
        "  $\\displaystyle \\epsilon_{i} \\geq 0$, $\\displaystyle \\sum_{i = 1}^{n} \\epsilon_{i} \\leq C$\n",
        "</center>"
      ],
      "metadata": {
        "id": "pRH6QeTn4nlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here $C$ is a non-negative hyperparameter that basically dictates how much slack we allow within our model. It's our budget for slack, and in the maximal margin classifier we have $C = 0$. The variables $\\epsilon_{i}$ are called *slack variables*. They basically tell us how far behind the margin observation $x_{i}$ is. If the observation is not beyond the margin, then $\\epsilon_{i} = 0$. If $\\epsilon_{i} > 1$ then not only is $x_{i}$ on the wrong side of the margin, but it's classified incorrectly. Only at most $C$ observations can be classified incorrectly."
      ],
      "metadata": {
        "id": "XLapKY-55Dcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** - This is how the term $C$ is defined in *An Introduction to Statistical Learning*. In sklearn, the term $C$ is actually the reciprocal of this $C$. So, a low value of $C$ means a high tolerance for error, while a high value of $C$ means a low tolerance of error. Sorry that it's confusing. I didn't determine the different terms different sources use."
      ],
      "metadata": {
        "id": "4jhmwaUQBmOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a look at the models we get for different values of $C$:"
      ],
      "metadata": {
        "id": "EdoDgdEY9giY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
        "y = (iris.target == 2)  # Iris virginica\n",
        "\n",
        "scaler = StandardScaler()\n",
        "svm_clf1 = LinearSVC(C=1, max_iter=10_000, dual=True, random_state=42)\n",
        "svm_clf2 = LinearSVC(C=100, max_iter=10_000, dual=True, random_state=42)\n",
        "\n",
        "scaled_svm_clf1 = make_pipeline(scaler, svm_clf1)\n",
        "scaled_svm_clf2 = make_pipeline(scaler, svm_clf2)\n",
        "\n",
        "scaled_svm_clf1.fit(X, y)\n",
        "scaled_svm_clf2.fit(X, y)\n",
        "\n",
        "# Convert to unscaled parameters\n",
        "b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\n",
        "b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\n",
        "w1 = svm_clf1.coef_[0] / scaler.scale_\n",
        "w2 = svm_clf2.coef_[0] / scaler.scale_\n",
        "svm_clf1.intercept_ = np.array([b1])\n",
        "svm_clf2.intercept_ = np.array([b2])\n",
        "svm_clf1.coef_ = np.array([w1])\n",
        "svm_clf2.coef_ = np.array([w2])\n",
        "\n",
        "# Find support vectors (LinearSVC does not do this automatically)\n",
        "t = y * 2 - 1\n",
        "support_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).to_numpy()\n",
        "support_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).to_numpy()\n",
        "svm_clf1.support_vectors_ = X[support_vectors_idx1]\n",
        "svm_clf2.support_vectors_ = X[support_vectors_idx2]\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(10, 2.7), sharey=True)\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\n",
        "plot_svc_decision_boundary(svm_clf1, 4, 5.9)\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Petal width\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.title(f\"$C = {svm_clf1.C}$\")\n",
        "plt.axis([4, 5.9, 0.8, 2.8])\n",
        "plt.grid()\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "plot_svc_decision_boundary(svm_clf2, 4, 5.99)\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.title(f\"$C = {svm_clf2.C}$\")\n",
        "plt.axis([4, 5.9, 0.8, 2.8])\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FBb1UJiJ9kcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the classifier on the left is more tolerant of errors than the one on the right."
      ],
      "metadata": {
        "id": "wKN9CztMCAUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimization problem above has a very interpesting property: it turns out that only observations that either lie on the margin or that violate the margin will affect the hyperplane, and hence the classifier. In other words, an observation that lies strictly on the correct side of the margin does not affect the support vector classifier! Those observations that do affect the classifier are known as the *support vectors*."
      ],
      "metadata": {
        "id": "F6DcHkO47NEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Support Vector Machines##"
      ],
      "metadata": {
        "id": "byuY7nC98ERu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've got some more bad news. It turns out that for many types of classification problems, linear boundaries just don't make a lot of sense. Take for example our moons dataset:"
      ],
      "metadata": {
        "id": "FAaJOxW_8Hib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "\n",
        "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\", rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XAeA1hjMdFVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we try to build a support vector classifier for this, we're going to have a bad time (or, at least, not build a very good model):"
      ],
      "metadata": {
        "id": "CP6W_O0kdgxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf3 = SVC(kernel=\"linear\", C=1)\n",
        "svm_clf3.fit(X, y)\n",
        "\n",
        "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "plot_svc_decision_boundary(svm_clf3, -1.5, 2.5)\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\", rotation=0)\n",
        "plt.axis([-1.5, 2.5, -.75, 1.5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9kq9m12Tdwvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we do better?"
      ],
      "metadata": {
        "id": "e59RHBrbe3LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, in the same way that linear regression does poorly when there is a non-linear relationship between the predictors and the outcome, a support vector classifier does poorly when there is a non-linear boundary between our classes. With regression, we enlarged our feature space by using nonlinear terms. Well, we can do the same for support vector classifiers."
      ],
      "metadata": {
        "id": "L8FIce5Qe7J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, for example, instead of fitting a line, we could fit a quadratic defined by:\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle \\beta_{0} + \\sum_{j = 1}^{p}\\beta_{j1}X_{ij} + \\sum_{j = 1}^{p}\\beta_{j2}x_{ij}^{2} = 0$\n",
        "</center>\n",
        "\n",
        "This would mean solving the optimization problem (assuming soft margins):"
      ],
      "metadata": {
        "id": "GR-dhjcpjGWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V0U8VwpVjDHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  $\\displaystyle \\underset{\\beta_{0},\\beta_{11},\\beta_{12},\\ldots,\\beta_{p1},\\beta_{p2},\\epsilon_{1},\\ldots,\\epsilon_{n},M}{maximize} M$\n",
        "\n",
        "  subject to\n",
        "  \n",
        "  $\\displaystyle y_{i}(\\beta_{0} + \\sum_{j = 1}^{p}\\beta_{j1}X_{ij} + \\sum_{j = 1}^{p}\\beta_{j2}x_{ij}^{2}) \\geq M(1-\\epsilon_{i})$\n",
        "\n",
        "  $\\displaystyle \\epsilon_{i} \\geq 0$, $\\displaystyle \\sum_{i = 1}^{n} \\epsilon_{i} \\leq C, \\sum_{j = 1}^{p}\\sum_{k = 1}^{2}\\beta_{jk}^{2} = 1$\n",
        "</center>"
      ],
      "metadata": {
        "id": "ecVRNVtAgQAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1Fs_4m8TuYorGXS7VlUrs6HJtyHOs974s\" alt=\"ALF\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "GIAQbWBvhph1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we go down this path, it can get complicated quickly. What a support vector machine does is it allows us to enlarge the feature space used by the support vector classifier in a way that keeps computations manageable."
      ],
      "metadata": {
        "id": "8Akt5Xvnh6PC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is honestly the part of the course that I wish the most we could dive into the math, because the math behind the support vector machine is actually pretty slick. It involves the use of something called a *kernel*, which is a way of measuring similarity, and a very clever insight called the \"kernel trick\". However, that would take us too far afield. So, just understand that a support vector machine is still trying to optimize a soft margin - just like we did above, but just for a nonlinear, curved boundary."
      ],
      "metadata": {
        "id": "o0ITbP6HiXkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see how one of these would work on our moons dataset, with various values of our hyperparameter $C$."
      ],
      "metadata": {
        "id": "kOrDMP7ojzKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_kernel_svm_clf = make_pipeline(StandardScaler(),\n",
        "                                    SVC(kernel=\"poly\", degree=3, coef0 = 1, C=5))\n",
        "poly_kernel_svm_clf.fit(X, y)"
      ],
      "metadata": {
        "id": "d6nxT85ei79V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dataset(X, y, axes):\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "    plt.axis(axes)\n",
        "    plt.grid(True)\n",
        "    plt.xlabel(\"$x_1$\")\n",
        "    plt.ylabel(\"$x_2$\", rotation=0)\n",
        "\n",
        "def plot_predictions(clf, axes):\n",
        "    x0s = np.linspace(axes[0], axes[1], 100)\n",
        "    x1s = np.linspace(axes[2], axes[3], 100)\n",
        "    x0, x1 = np.meshgrid(x0s, x1s)\n",
        "    X = np.c_[x0.ravel(), x1.ravel()]\n",
        "    y_pred = clf.predict(X).reshape(x0.shape)\n",
        "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
        "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
        "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
        "\n",
        "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n",
        "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
        "plt.title(\"degree=3, C=5\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qi-Gfu5mkfTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad!"
      ],
      "metadata": {
        "id": "fIxZ4bvalX_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##References"
      ],
      "metadata": {
        "id": "SYhOdVDylY9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Support Vector Machines](https://youtu.be/efR1C6CvhmE?si=ZBO8fNh3vBhfr0cz) from StatQuest"
      ],
      "metadata": {
        "id": "ATvTOgk7laPE"
      }
    }
  ]
}