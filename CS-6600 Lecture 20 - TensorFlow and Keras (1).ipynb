{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-5600/6600 Lecture 20 - TensorFlow and Keras\n",
        "\n",
        "**Instructor: Dylan Zwick**\n",
        "\n",
        "*Weber State University*"
      ],
      "metadata": {
        "id": "JDsKFocgqwDj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov8nKCSjm9o_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TensorFlow"
      ],
      "metadata": {
        "id": "4iMzDLSqqEf7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e97de837-b5ed-410d-93a3-1070e21d9d17"
      },
      "source": [
        "**What is TensorFLow?**\n",
        "\n",
        "TensorFlow is an open-source machine learning platform developed by Google that provides a powerful suite of tools for data scientists and developers to build, deploy, and train machine learning models. It was initially released in 2015, and has evolved significantly since. The TensorFlow library allows developers to create complex neural networks using a variety of programming languages, such as Python and JavaScript. Additionally, TensorFlow makes it easy to deploy models on mobile devices or cloud platforms like Google Cloud Platform (GCP) and Amazon Web Services (AWS).\n",
        "\n",
        "TensorFlow is used in a variety of applications, ranging from natural language processing (NLP) and image recognition to predictive analytics and autonomous vehicle control. It can be used to train deep neural networks for object detection and classification, generate recommendations, classify images, and build voice-powered applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfed9285-813e-4802-bc16-9d14f40ed0e6"
      },
      "source": [
        "**What are Tensors?**\n",
        "\n",
        "A tensor is, basically, an $n$-dimensional generalization of a matrix. A zero-dimensional tensor is a scalar, which contains a single value and has no axes. A one-dimensional tensor is a vector, which contains a list of values and has one axis. A two-dimensional tensor is a matrix that contains values stored across two axes.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1gT9zfALy0q1MLieyqWeyYbbawZM8jXfw\" alt=\"Tensors\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef59d183-1bec-4e5b-a1a1-573736b969df"
      },
      "source": [
        "**How Does TensorFlow Work?**\n",
        "\n",
        "At the core of TensorFlow is a dataflow graph, which describes how data moves through a series of operations or transformations. The basic idea behind the dataflow graph is that operations are expressed as nodes, with each node performing a single operation on its inputs. The inputs and outputs of the operations are passed through edges (tensors). This makes it possible to break down complex computations into smaller, more manageable chunks.\n",
        "\n",
        "TensorFlow also provides a number of tools for constructing and training neural networks. One of the most popular tools is the utility tf.keras (much more on this below), which allows users to quickly build and train deep learning models without having to write code from scratch. It also includes powerful visualization tools to help users understand the data and model parameters.\n",
        "\n",
        "TensorFlow is also extensible, and it can be used with a variety of programming languages, including Python, C++, JavaScript, and Go. It also has support for running on GPUs (graphics processing units) for maximum performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc20adc3-d523-46c3-aed5-5a1670fa9194"
      },
      "source": [
        "Some important facts about TensorFlow:\n",
        "\n",
        "* Its core is very similar to NumPy, but with GPU support.\n",
        "* It supports distributed computing (across multiple devices and servers).\n",
        "* It includes a kind of just-in-time (JIT) compiler that allows it to optimize computations for speed and memory usage. It works by extracting the *computation graph* from a Python function, then optimizing it, and finally running it efficiently.\n",
        "* Computation graphs can be exported to a portable format, so you can train a TensorFlow model in one environment and run it in another.\n",
        "\n",
        "TensorFlow offers many more features built on top of these core features: the most important is of course *tf.keras*, but it also has data loading and preprocessing ops (*tf.data*, *tf.io*, etc.), image processing ops (*tf.image*), signal processing ops (*tf.signal*), and more. We won't cover anywhere close to everything, so it's worth checking out the [documentation](https://www.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "368cba09-2e19-444b-ac52-b003b9d280f1"
      },
      "source": [
        "At the lowest level, each TensorFlow operation (*op* for short) is implemented using highly efficient C++ code. Many operations have multiple implementations called *kernels*: each kernel is dedicated to a specific device type, such as CPUs, GPUs, or even TPUs (*tensor processing units*). As you may know, GPUs can dramatically speed up computations by splitting them into many smaller chunks and running them in parallel across many GPU threads. TPUs are even faster: they are custom ASIC chips build specifically for deep learning operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b01245b-5228-411f-986a-c3d4f3824553"
      },
      "source": [
        "Most of the time your code will use the high-level APIs (especially *tf.keras* and *tf.data*); but when you need more flexibility, you can use the lower-level Python API, handling tensors directly. Note APIs for other languages are also available. In fact, TensorFlow runs not only in Windows, Linux, and macOS, but also on mobile devices, including both iOS and Android. There's even a JavaScript implementation called *TensorFlow.js* that makes is possible to run models directly in a browser!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ea5e0dd-2051-4b13-9f3f-485d440f1013"
      },
      "source": [
        "**Using TensorFlow like Numpy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9148f6e1-97c1-4b95-89a5-66937aff9065"
      },
      "source": [
        "TensorFlow's API revolves, appropriately, around *tensors*, which flow from operation to operation - thus the name. A tensor is very similar to a NumPy *ndarray*: it is usually a multidimensional array, but it can also hold a scalar. Let's see how to create a manipulate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f0c0ab2-9e60-4eb6-a62e-6cb48c2413d0"
      },
      "outputs": [],
      "source": [
        "a = np.array([2.,4.,5.])\n",
        "t = tf.constant(a)\n",
        "t.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77b015d6-5e95-4e8a-af76-b558c6acb934"
      },
      "outputs": [],
      "source": [
        "tf.square(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d3e32bb-3ffe-4cf9-b840-f446769fb06a"
      },
      "outputs": [],
      "source": [
        "np.square(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3348002c-b5f7-4ffe-a7b2-cb32cae356e8"
      },
      "source": [
        "*Type Conversions*\n",
        "\n",
        "Type conversions can significantly hurt performance, and they can easily go unnoticed when they are done automatically. To avoid this, TensorFlow does not perform any type conversions automatically: it just raises an exception if you try to execute an operation on tensors with incompatible types. For example, you cannot add a float tensor or an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceb4920e-4279-47c9-91af-0187163bb395"
      },
      "outputs": [],
      "source": [
        "tf.constant(2.) + tf.constant(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b279e3bf-749a-4848-be97-5aee158dc985"
      },
      "outputs": [],
      "source": [
        "tf.constant(2.) + tf.constant(40, dtype=tf.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80e9ed18-5ee0-4aa7-9c51-576b53c741f2"
      },
      "outputs": [],
      "source": [
        "tf.constant(2.) + tf.constant(40, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c152e899-a054-4ad8-a214-c17202fea86f"
      },
      "source": [
        "**Variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc700bce-e56c-4369-8be1-a193e6a330ce"
      },
      "source": [
        "The *tf.Tensor* values we've seen so far are immutable: you cannot modify them. This means that we cannot use regular tensors to implement weights in a neural network, since they need to be tweaked by backpropagation. Plus, other parameters may also need to change over time. What we need is a *tf.Variable*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d5d969b-6115-4aff-98e8-7e3239a45728"
      },
      "outputs": [],
      "source": [
        "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
        "v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7671d8f2-a69f-4f92-bffc-803502657200"
      },
      "source": [
        "A *tf.Variable* acts much like a *tf.Tensor*: you can perform the same operations with it, it plays nicely with NumPy as well, and it is just as picky with types. But it can also be modified in place using the *assign()*, *assign_add()*, or *assign_sub()* methods, which increment or decrement the variable by the given value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ad6fc8d-f9a6-4b62-aaec-377134c9209d"
      },
      "outputs": [],
      "source": [
        "v.assign(2 * v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77b6ad8e-6cb8-408a-b7f9-7316f9462e17"
      },
      "outputs": [],
      "source": [
        "v[0,1].assign(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faeee86b-aa2d-42cc-ac1f-4ee5b142809c"
      },
      "outputs": [],
      "source": [
        "v[:,2].assign([0., 1.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf8a1421-1126-4c7b-b341-716fffb3af31"
      },
      "source": [
        "In practice you will rarely have to create variables manually, since Keras provides an *add_weight()* method that will take care of it for you. Moreover, model parameters will generally be updated directly by the optimizers, so you will rarely need to update variables manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "296cd890-9a88-403e-9c1f-1675dc9a680f"
      },
      "source": [
        "OK, so far TensorFlow looks a lot like NumPy, and that's by design. So what's something TensorFlow can do than NumPy can't? Well, with TensorFlow we can retrieve the gradient of any differentiable expression with respect to any of its inputs. This is done within something called a *GradientTape*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a707acf-0a3c-4eb2-9d7e-0ed611234b89"
      },
      "outputs": [],
      "source": [
        "input_var = tf.Variable(initial_value=3.)\n",
        "with tf.GradientTape() as tape:\n",
        "    result = tf.square(input_var)\n",
        "gradient = tape.gradient(result, input_var)\n",
        "gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3fa7323-a1b7-4993-8015-c8e9e1b7a38e"
      },
      "source": [
        "This is most commonly used to retrieve the gradients of the loss of a model with respect to its weights."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keras"
      ],
      "metadata": {
        "id": "dW1OnSC_qKqa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ae6675f"
      },
      "source": [
        "In our last class, we stepped through the math of back propagation, and went deeper into the implementation of a neural network. Today, we'll learn about Keras, a tool that handles it all for you - and probably does a better job than if you built it yourself!\n",
        "\n",
        "Keras is a high-level deep learning [API](https://aws.amazon.com/what-is/api/) that allows you to easily build, train, evaluate, and execute all sorts of neural networks. Its documentation can be found [here](https://keras.io/). A good book, by Francois Chollet, the person who designed it, is [Deep Learning with Python](https://www.amazon.com/Learning-Python-Second-Fran%C3%A7ois-Chollet/dp/1617296864/ref=sr_1_1), one of the recommended books for this class. It has quickly gained popularity, owing to its ease of use, flexibility, and design.\n",
        "\n",
        "To perform the heavy computations required by neural networks, Keras relies on a computations backend. At present, you can choose from (at least) TensorFlow, Microsoft Cognitive Toolkit, and Theano. In fact TensorFlow itself now comes bundled with its own Keras implementation, *tf.keras*. Unsurprisingly, it only supports TensorFlow as the backend, but it also offers some very useful extra features - for example, it supports TensorFlow's Data API, which makes it easy to load and preprocess data efficiently. For this reason, we'll use tf.keras in this class. However, most of what we do won't be TensorFlow specific, and so the code should mostly run fine on other Keras implementations as well that use Python. Note also that the PyTorch API is quite similar to Keras. This is mostly because they share a common ancestor in Scikit-Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fae53285-bbc5-4715-966e-fb1e4d8d5878"
      },
      "source": [
        "The Keras library contains a number of common machine learning datasets that you can load, including the handwritten digits dataset - [MNIST](https://keras.io/api/datasets/mnist/) - that has been our main example so far. We can import it with the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1349ad79-cc89-4c6d-9907-6557697bea4a"
      },
      "outputs": [],
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "912ffc19-8f1d-4b84-b620-a7a7a141cb51"
      },
      "source": [
        "We note here that the dataset is already split into a training set and a test set, but there is no validation set, so we'll create one now. Additionally, we'll scale the input features - which are represented as integers from $0$ to $255$, to the $0-1$ range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a52b0365-c20d-432f-9c5a-fc544d4bf251"
      },
      "outputs": [],
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24667712-65bc-4aa4-bec1-4cbb15f75854"
      },
      "source": [
        "Now, let's build the neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92a92a60-7c2c-4a75-a967-d5371ce9c27d"
      },
      "source": [
        "***Creating the Model***\n",
        "\n",
        "Here is a classification MLP with one hidden layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c77cfe4-d448-4145-8280-79885325aca9"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Input(shape=[28, 28]))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(30, activation=\"sigmoid\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84a45403-d49b-419f-a4dd-372427247d45"
      },
      "source": [
        "Let's go through this code line by line:\n",
        "\n",
        "- The first line creates a *Sequential* model. This is the simplest kind of Keras model for neural networks that are composed of a single stack of layers connected sequentially.\n",
        "- We define the shape of the input.\n",
        "- We \"flatten\" the input, which means we transform our $28 \\times 28$ array into a one-dimensional array with $28 \\times 28 = 784$ entries.\n",
        "- Next we add a dense hidden layer with 30 nodes (a.k.a *neurons*). It will use the sigmoid activation function. Each *Dense* layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per node).\n",
        "- Finally, we add a dense output layer with 10 nodes (one per class), again using the sigmoid activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9968c60e-a2af-4d04-ad95-e64dfa9b6012"
      },
      "source": [
        "The potential activation functions can be found [here](https://keras.io/api/layers/activations/). Note that instead of adding the layers one-by-one, we could pass a list of layers when we create the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bc79f39-8603-4848-9398-3aec9637ed5d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Input(shape=[28, 28]),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(30, activation=\"sigmoid\"),\n",
        "    keras.layers.Dense(10, activation=\"sigmoid\")\n",
        "    ])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08b9f5a8-e0a2-45f1-ae10-042f7a1616ba"
      },
      "source": [
        "The model's *summary()* method displays all the model's layers, including each layer's name (which is automatically generated unless you set it when creating the layer), its output shape, and the number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "010e3399-3efe-470c-86c9-26a4be95a263"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106a593d-5d45-41a2-b11d-767ec29e9fb0"
      },
      "source": [
        "Keras even includes a tool for generating an image of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9530c249-4087-446b-ad6a-e01fa463a704"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbaec145-f17f-4e74-9fba-e28f6229f56f"
      },
      "source": [
        "Perhaps this one is not so incredibly enlightening."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf0ded4a-52fd-4dbd-915e-6eb56d8c83fb"
      },
      "source": [
        "You can easily get a model's list of layers to fetch a layer by its index, or you can fetch a layer by its name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f4ccdcd-482b-4e80-a657-c41137058b87"
      },
      "outputs": [],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfc77d4-e42f-41d2-91a9-3f809ee9cf42"
      },
      "outputs": [],
      "source": [
        "hidden1 = model.layers[1]\n",
        "hidden1.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05d6a322-ccb2-48d6-a534-cba9855dca94"
      },
      "source": [
        "All the parameters of a layer can be accessed using its *get_weights()* and *set_weights()* methods. For a dense layer, this includes both the connection weights and the bias terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9971d730-370d-4741-8172-e0ae27e13f6c"
      },
      "outputs": [],
      "source": [
        "weights, biases = hidden1.get_weights()\n",
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5ac0e63-d9b9-4fac-b10b-d5d771aedba9"
      },
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc2f20d5-163d-4eb5-aae0-ea4bf75316e7"
      },
      "outputs": [],
      "source": [
        "biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79842610-518e-4d38-9248-75d6ec7d58a0"
      },
      "outputs": [],
      "source": [
        "biases.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212ebc82-4c7f-46b5-a6f6-0b33865951fc"
      },
      "source": [
        "Notice the dense layer initialized the connection weights randomly (which is needed to avoid redundancy in learning), while the biases were initialized to zeros, which is fine. If you want to use different initialization methods, you can use kernel_initializer and bias_initializer. More info [here](https://keras.io/api/layers/initializers/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3c4ca79-5785-410a-99a5-05526d92dcdb"
      },
      "source": [
        "***Compiling the Model***\n",
        "\n",
        "After a model in created, you must call its *compile()* method to specify the loss function and the optimizer to use. You can also, if desired, specify a list of extra metrics to compute during training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6812b542-4d47-400f-a08f-015175624e89"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55299dce-6029-4d46-b0a6-1504962015ac"
      },
      "source": [
        "This requires some explanation. First, we use the \"sparse_categorical_crossentropy\" loss because we have sparse labels (for each instance, there is just a target class index, from $0$ to $9$ in this case), and the classes are exclusive. If instead we had one-hot vectors we would use \"categorical_crossentropy\" loss instead.\n",
        "\n",
        "What is the cross-entropy? It's the negative of the logarithm of the predicted probability of the result:\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle L = -\\log(\\hat{y}[y])$\n",
        "</center>\n",
        "\n",
        "Here $y$ is the actual value, and $\\hat{y}[y]$ is the model's probability of the actual value. We add these up over all predictions to get our total cross-entropy.\n",
        "\n",
        "Regarding the optimizer, \"sgd\" means that we will train the model using simple stochastic gradient descent. Note the learning rate defaults to $lr = 0.01$, but we could specify it with *optimizer = keras.optimizers.SGD(lr=???)*.\n",
        "\n",
        "Finally, since it is a classifier, it's useful to describe its \"accuracy\" during training and evaluation.\n",
        "\n",
        "See these links for more info on [losses](https://keras.io/api/losses/), [optimizers](https://keras.io/api/optimizers/), and [metrics](https://keras.io/api/metrics/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "219582a9-ddf9-4f33-98f8-cb6ef9632bb6"
      },
      "source": [
        "***Training the Model***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32af3263-2bca-4041-8a24-95c8cc098be2"
      },
      "source": [
        "Now the model is ready to be trained. To do this, we just call its *fit()* method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26e6af0b-f033-4092-b439-331a9b45fdea"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs = 10, validation_data=(X_valid,y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f927b889-dc89-4dd2-a08f-48577bdc268c"
      },
      "source": [
        "Note that passing the validation set is optional. However, it's good practice. If the performance on the training set is much better than on the validation set, you've probably got an overfit model. Note that instead of passing a validation set using the *validation_data* argument, we could set *validation_split* to the ratio of the training set that you want Keras to use for validation. So, *validation_split = $0.1$* would tell Keras to use the last 10% of the data for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b693bf2-484f-4b26-a2ca-ac57709790d7"
      },
      "source": [
        "That's it! The neural network is trained. The *fit()* method returns a *History* object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training data and the validation data (if provided)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "977a0c0c-f5a1-4819-ae7e-c9dee3c476cc"
      },
      "source": [
        "We can use this dictionary to create a pandas dataframe and then plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52b60168-766d-4e6f-97cc-a6ba82497117"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(10,6))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da915824-4670-4e7f-98c7-3b6f93f1c2fb"
      },
      "source": [
        "The validation accuracy starts above the training accuracy. Why is this? Because the validation accuracy is calculated at the *end* of the epoch of training, while the training accuracy is calculated *during* training. Note that after 10 epochs both validation and training accuracy were quite close, and still growing, indicating that more training probably would be valuable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72337610-c4f6-496f-895d-ebd50adba6f3"
      },
      "source": [
        "If you're not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn't help, try another optimizer. If the performance is still not great, try tuning model hyperparameters such as the number of layers, the number of nodes per layer, and the types of activation function used for each layer.\n",
        "\n",
        "Finally, we should verify how the model performs on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56e49ad7-8648-4bb0-b998-92967ac7d7a1"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e659915c-075a-471c-90c7-f22808f8f1fe"
      },
      "source": [
        "***Predicting With The Model***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3f8e71b-73a1-4394-9340-bca1bbb3314a"
      },
      "source": [
        "We can use the model's *predict()* method to make predictions on new instances. Since we don't have actual new instances, we can just try the first three instances in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00ebc5ea-1028-47f0-afdd-9fae0b1e0dac"
      },
      "outputs": [],
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note* - These probabilities were not normalized, so they can sum to more than 1. The classifier just picks the highest value."
      ],
      "metadata": {
        "id": "TtejOmRDds7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix - Building a Linear Classifier With Just TensorFlow"
      ],
      "metadata": {
        "id": "0Wvbl-9Hqo7c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e15fee5-ed84-45e7-bec2-629cc8d22a86"
      },
      "source": [
        "**Example - A Linear Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a12eff71-8d37-41a8-8613-97f790d9c122"
      },
      "source": [
        "Alright, using just what we've learned so far, we now know enough to build any machine learning model based on gradient descent. Woo hoo!\n",
        "\n",
        "In a machine learning job interview, you may be asked to implement a linear classifier from scratch with TensorFlow. Let's see how we'd do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dc85dad-6094-4e77-b298-bb8ee65eb42a"
      },
      "source": [
        "First, let's come up with some nicely linearly separable data to work with: two classes of points in a 2D plane. We'll generate each class of points by drawing their coordinates from a random distribution with a specific covariance matrix and a specific mean. We'll use the same covariance matrix for both clouds, but we'll use two different mean values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1d47d94-6190-483d-ac6e-258bc9650ec2"
      },
      "outputs": [],
      "source": [
        "num_samples_per_class = 1000\n",
        "negative_samples = np.random.multivariate_normal(\n",
        "    mean = [0,3],\n",
        "    cov = [[1, 0.5], [0.5, 1]],\n",
        "    size = num_samples_per_class)\n",
        "\n",
        "positive_samples = np.random.multivariate_normal(\n",
        "    mean = [3,0],\n",
        "    cov = [[1, 0.5], [0.5, 1]],\n",
        "    size = num_samples_per_class)\n",
        "\n",
        "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
        "targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=\"float32\"),\n",
        "                   np.ones((num_samples_per_class, 1), dtype=\"float32\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1533c5b-1970-45bd-a7c7-7199c5e49b30"
      },
      "source": [
        "We can plot this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1277171c-8606-4235-b6f3-f116d63a66d1"
      },
      "outputs": [],
      "source": [
        "plt.scatter(inputs[:,0], inputs[:,1], c=targets[:,0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1621ff2-5c49-493c-9ceb-41dde1ad2eec"
      },
      "source": [
        "Now, let's create a linear classifier that can learn to separate these two blobs. A linear classifier is a model of the form:\n",
        "\n",
        "<center>\n",
        "    prediction = $\\displaystyle W\\textbf{x} + \\textbf{b}$,\n",
        "</center>\n",
        "\n",
        "where $W$ is a matrix and $\\textbf{b}$ is a vector. (Here $W$ stands for \"weights\" and $\\textbf{b}$ stands for \"bias\".)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77b21531-3b49-4dc1-94a6-5e686f57489a"
      },
      "source": [
        "First, let's create our initial weights and biases, initialized with random values and zeros, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "120f7115-1823-47e7-880c-d2d070c51993"
      },
      "outputs": [],
      "source": [
        "input_dim = 2\n",
        "output_dim = 1\n",
        "W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))\n",
        "b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b74cb53-14af-400c-b69d-962053eb829a"
      },
      "source": [
        "Now, we'll create our forward pass function. Note that given how we've set it up, we'll want to have our inputs on the *left* of the multiplication and our outputs on the *right*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec4425cb-b21b-4e76-90e6-cf6ff3cbc355"
      },
      "outputs": [],
      "source": [
        "def model(inputs):\n",
        "    return tf.matmul(inputs, W) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7890240f-336c-4b89-9fc0-fc5be4147fb5"
      },
      "source": [
        "Our linear classifier is operating on 2D inputs, and so $W$ is really just two scalar coefficients $w_{1}$ and $w_{2}$. Meanwhile, $b$ is just a single scalar coefficient. In other words, the prediction is:\n",
        "\n",
        "<center>\n",
        "    $\\displaystyle w_{1}x_{1} + w_{2}x_{2} + b$.\n",
        "</center>\n",
        "\n",
        "We can change the index of the variables to be more \"pythonic\", and the labels for the inputs to reflect a 2D plane:\n",
        "\n",
        "<center>\n",
        "    $\\displaystyle w_{0}x + w_{1}y + b$.\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8da7f3e9-8de8-4ff9-97de-142935d676d3"
      },
      "source": [
        "Alright, so now that we have our forward pass function, we'll also need our error function - how we measure how close our prediction was to the actual value. For this, we can use our old favorite, the square of the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d812d418-2771-48e2-b531-13be77d0113a"
      },
      "outputs": [],
      "source": [
        "def square_loss(targets, predictions):\n",
        "    per_sample_losses = tf.square(targets - predictions)\n",
        "    return tf.reduce_mean(per_sample_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f15441b5-e70d-46b8-a713-c841fbc6f5a9"
      },
      "source": [
        "*Note* - What do we mean by \"reduce_mean\" here? Why isn't it just \"mean\"? Well, because for this operation TensorFlow computes the mean through a map-reduce operation in which the order of operations isn't always the same. This matters because for certain very high precision situations, doing the operations in a different order can lead to different rounding decisions and so different final outcomes. The name of the function is meant to make that explicit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87b01683-5733-4c3a-be90-31a5f2f9114c"
      },
      "source": [
        "Alright, so now the final thing we need to do is handle the updates to our weight after a given batch. To do this, we can use TensorFlow to calculate the gradient of our loss with respect to our weights and biases, respectively, and then update our weights and biases accordingly with our learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9acdaaf9-de24-4211-af2d-1cee125ad6b7"
      },
      "outputs": [],
      "source": [
        "learning_rate = .1\n",
        "\n",
        "def training_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs)\n",
        "        loss = square_loss(targets, predictions)\n",
        "    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W,b])\n",
        "    W.assign_sub(grad_loss_wrt_W * learning_rate)\n",
        "    b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a41992f-8375-4f70-bae9-b3cc2c3f80b2"
      },
      "source": [
        "Isn't that nice! That gradient function is pretty sweet.\n",
        "\n",
        "Now, we just need to run our *training_step* function however many times we think we should. Let's try 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fb01e98-0fcf-4c51-98a7-4423ec46dffe"
      },
      "outputs": [],
      "source": [
        "for step in range(42):\n",
        "    loss = training_step(inputs, targets)\n",
        "    print(f\"Loss at step {step+1}: {loss: .4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2e6786c-c91c-401a-aedc-ed747a069ebc"
      },
      "source": [
        "OK. Looks like the loss has settled down, which means the line has too. So, what are our predicted values going to be? Well, we can plot them using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75fbc490-597d-4c3f-83d6-0e13b244ad53"
      },
      "outputs": [],
      "source": [
        "predictions = model(inputs)\n",
        "plt.scatter(inputs[:,0], inputs[:,1], c=predictions[:, 0] > 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441c5fad-8417-490f-91ce-970321a98d07"
      },
      "source": [
        "Fundamentally, our predictions are linearly separated. What's the separating line? Well, if we start with our model equation:\n",
        "\n",
        "<center>\n",
        "    $\\displaystyle w_{0}x + w_{1}y + b$,\n",
        "</center>\n",
        "\n",
        "we can note that it predicts the label $1$ when:\n",
        "\n",
        "<center>\n",
        "    $\\displaystyle w_{0}x + w_{1}y + b \\geq .5$,\n",
        "</center>\n",
        "\n",
        "and so the boundary line is:\n",
        "\n",
        "<center>\n",
        "    $\\displaystyle w_{0}x + w_{1}y + b = .5$.\n",
        "</center>\n",
        "\n",
        "We can rewrite this in slope-intercept form as:\n",
        "\n",
        "<center>\n",
        "    $\\displaystyle y = -\\frac{w_{0}}{w_{1}}x + \\frac{.5-b}{w_{1}}$.\n",
        "</center>\n",
        "\n",
        "Plotting this line against our initial labels we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07521d0c-0e0b-423e-978f-ee006fe5a277"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-2,6,200)\n",
        "y = (-W[0] / W[1]) * x + (0.5-b) / W[1]\n",
        "plt.plot(x,y,\"-r\")\n",
        "plt.scatter(inputs[:,0], inputs[:,1], c=targets[:,0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e348c9c8-8e16-418c-8c2c-4af49af881a7"
      },
      "source": [
        "Voila! A linear classifier. You move on in the interview."
      ]
    }
  ]
}