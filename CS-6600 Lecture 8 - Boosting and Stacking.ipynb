{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-5600/6600 Lecture 8 - Boosting and Stacking\n",
        "\n",
        "**Instructor: Dylan Zwick**\n",
        "\n",
        "*Weber State University*\n",
        "\n",
        "Reference: [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) by Aurélien Géron - [Ensemble Learning and Random Forests](https://github.com/ageron/handson-ml3/blob/main/07_ensemble_learning_and_random_forests.ipynb)"
      ],
      "metadata": {
        "id": "0iETW1Mm9rv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we're going to continue along the path we started with the random forest, and investigate some other approaches for turning a \"forest of stumps\" into a good predictive model.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1DOKeu75laBC7MeLEZGr0mnFWL88hVcR-\" alt=\"Forest of Stumps\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "1cD18mj_QOnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's grab the libraries we'll want to use:"
      ],
      "metadata": {
        "id": "wCXLyGh4Qd30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier"
      ],
      "metadata": {
        "id": "FoOI51eJAmgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind a random forest is to take a bunch of weak classifiers - typically decision trees with only a few (or even two) layers (a.k.a. decision \"stumps\"), and combine them into an impressively performant ensemble.\n",
        "\n",
        "Well, the idea behind \"boosting\" is similar, except that the trees are not \"grown\" in parallel, but are instead produced sequentially - each tree attempts to correct the errors of its ancestors. This process is called *boosting*. There are many boosting methods, and today we'll look at some of the most popular - [*AdaBoost*](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) (short for *adaptive boosting*), and *gradient boosting*."
      ],
      "metadata": {
        "id": "_PqPPi8vQkoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost"
      ],
      "metadata": {
        "id": "ACF4E2GzRsSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind AdaBoost is that each predictor pays more attention (gives more weight) to the training instances its precessors got wrong. The basic approach here is:\n",
        "\n",
        "1. Train a base classifier, and use it to make predictions.\n",
        "2. Increase the importance of the instances the base classifier got wrong, and train another classifier.\n",
        "3. Take a weighted combination of these classifiers, weighted by their overall performance. This is our new base classifier.\n",
        "4. Repeat.\n",
        "\n",
        "At each stage the base classifier should get better, and the additional classifiers should focus more and more on the \"harder\" cases."
      ],
      "metadata": {
        "id": "6A_BAmQ5RwBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at how this works for some \"moons\" data."
      ],
      "metadata": {
        "id": "7XMkJ2ZtUSiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "cmpHx2jVA31o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_2d_data(X, y):\n",
        "    # Separate the data based on binary labels\n",
        "    class_0 = X[y == 0]\n",
        "    class_1 = X[y == 1]\n",
        "\n",
        "    #Assign colors and markers\n",
        "    colors = [\"#78785c\", \"#c47b27\"]\n",
        "    markers = (\"o\", \"^\")\n",
        "\n",
        "    # Create a scatter plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(class_0[:, 0], class_0[:, 1], color=colors[0], marker=markers[0])\n",
        "    plt.scatter(class_1[:, 0], class_1[:, 1], color=colors[1], marker=markers[1])\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.ylabel(r\"$x_2$\", rotation=0)\n",
        "    plt.title('Moons Data')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GxAg_rktUZU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_2d_data(X_train, y_train)"
      ],
      "metadata": {
        "id": "eTtYQNNvVm1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how this type of boosting approach can work on this moons data. We'll run through five iterations using a SVM classifier with an RBF kernel (don't worry about what these means right now, just understand it's not a decision stump). We'll run through five sequences. The second plot is with the same approach, just a different learning rate (which means the incorrect instances aren't boosted as much). Don't concern yourself with the specifics of how this is implemented right now - we'll get to that soon."
      ],
      "metadata": {
        "id": "TSr2TrOcXJ08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(clf, X, y, alpha=1.0):\n",
        "    axes=[-1.5, 2.4, -1, 1.5]\n",
        "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
        "                         np.linspace(axes[2], axes[3], 100))\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "\n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n",
        "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n",
        "    colors = [\"#78785c\", \"#c47b27\"]\n",
        "    markers = (\"o\", \"^\")\n",
        "    for idx in (0, 1):\n",
        "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
        "                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n",
        "    plt.axis(axes)\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.ylabel(r\"$x_2$\", rotation=0)"
      ],
      "metadata": {
        "id": "4Zg9wXvHBVwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = len(X_train)\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
        "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
        "    sample_weights = np.ones(m) / m\n",
        "    plt.sca(axes[subplot])\n",
        "    for i in range(5):\n",
        "        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)\n",
        "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n",
        "        y_pred = svm_clf.predict(X_train)\n",
        "\n",
        "        error_weights = sample_weights[y_pred != y_train].sum()\n",
        "        r = error_weights / sample_weights.sum()\n",
        "        alpha = learning_rate * np.log((1 - r) / r)\n",
        "        sample_weights[y_pred != y_train] *= np.exp(alpha)\n",
        "        sample_weights /= sample_weights.sum()\n",
        "\n",
        "        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)\n",
        "        plt.title(f\"learning_rate = {learning_rate}\")\n",
        "    if subplot == 0:\n",
        "        plt.text(-0.75, -0.95, \"1\", fontsize=16)\n",
        "        plt.text(-1.05, -0.95, \"2\", fontsize=16)\n",
        "        plt.text(1.0, -0.95, \"3\", fontsize=16)\n",
        "        plt.text(-1.45, -0.5, \"4\", fontsize=16)\n",
        "        plt.text(1.36,  -0.95, \"5\", fontsize=16)\n",
        "    else:\n",
        "        plt.ylabel(\"\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9_al0r5kAbk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, now let's take a deeper look at the AdaBoost algorithm.\n",
        "\n",
        "We'll say our instances are indexed by $(i)$, and each instance has a corresponding weight $w^{(i)}$. These weights are *normalized*, which means they all add up to $1$, and initially they're all set to be the same. So, if there are $m$ instances in our data, the initial weights are set to $1/m$.\n",
        "\n",
        "We train our initial predictor, $r_{1}$, with these initial weights, and its weighted error rate is computed on the data. The weighted error rate for prediction $r_{j}$ is defined as:\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle r_{j} = \\sum_{\\hat{y}_{j}^{(i)} \\neq y^{(i)}}^{m} w^{(i)}$ where $\\hat{y}_{j}^{(i)}$ is the $j$th predictor's prediction for the $i$th instance.\n",
        "</center>\n",
        "\n",
        "The predictor's weight $\\alpha_{j}$ is then computer as:\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle \\alpha_{j} = \\eta \\log{\\left(\\frac{1-r_{j}}{r_{j}}\\right)}$\n",
        "</center>\n",
        "\n",
        "Based on the predictor's weight, the instance weights are then updated according to the update rule:\n",
        "\n",
        "<center>\n",
        "  for $i = 1,2, \\ldots, m$\n",
        "\n",
        "  <br>\n",
        "\n",
        "  $\\displaystyle w^{(i)} \\leftarrow \\left\\{\\begin{array}{cc} w^{(i)} & \\hat{y}_{j}^{(i)} = y^{(i)} \\\\ w^{(i)}e^{\\alpha_{j}} & \\hat{y}_{j}^{(i)} \\neq y^{(i)}\\end{array}\\right.$\n",
        "</center>\n",
        "\n",
        "Finally, all weights are normalized (divided by $\\sum_{i = 1}^{m} w^{(i)}$) and the next predictor is trained.\n",
        "\n",
        "The algorithm stops when the desired number of predictors is reached, or a perfect predictor has been found.\n",
        "\n",
        "But how are these predictions made? Well, AdaBoost simply computes the predictions of all the predictors and weights them using the predictor weights. The predicted class is the one that received the majority of the votes.\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle \\hat{y}(\\textbf{x}) = \\underset{k}{argmax} \\sum_{\\hat{y}_{j}(\\textbf{x}) = k}^{N} \\alpha_{j}$ where $N$ is the number of predictors.\n",
        "</center>\n",
        "\n",
        "[Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) has a multiclass version of AdaBoost called SAMME (for *Stagewise Additive Modeling using a Multiclass Exponential loss function). When there are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate class probabilities, then Scikit-Learn uses SAMME.R, which relies on class probabilities and usually performs better than with just class predictions."
      ],
      "metadata": {
        "id": "kHtcFj_ZYWdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put together an AdaBoost classifier using 30 decision \"stumps\", and fit it to our moons data."
      ],
      "metadata": {
        "id": "kPG2A_4Wfm3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=30, algorithm=\"SAMME\",\n",
        "    learning_rate=0.5, random_state=42)\n",
        "ada_clf.fit(X_train, y_train);"
      ],
      "metadata": {
        "id": "RG7mRFqiBbAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(ada_clf, X_train, y_train)"
      ],
      "metadata": {
        "id": "90GVBVsUCR9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad. If our ensemble is overfitting the training set, we can try reducing the number of estimators."
      ],
      "metadata": {
        "id": "ccCER1UCgFQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting"
      ],
      "metadata": {
        "id": "Ttnf8iYHgWvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A variant on the boosting approach is *gradient boosting*. As with AdaBoost, gradient boosting applies a sequence of models that attempt to correct the errors of the previous models. However, instead of tweaking the instance weights at every iteration, gradient boosting tries to fit the new predictor the the *residual errors* made by the previous predictors."
      ],
      "metadata": {
        "id": "RT950D3oL1Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go through a simple regression (numeric prediction) example, using decision trees as the base predictors. These are sometimes called *gradient boosting regression trees*, or GBRTs."
      ],
      "metadata": {
        "id": "M1a29lQ2MJ_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's generate some noisy quadretic data that we can fit."
      ],
      "metadata": {
        "id": "FqAj0H69M6KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.rand(100,1) - 0.5\n",
        "y = 3 * X[:,0]**2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg1.fit(X, y); #Note this expects X to be a 2-dimensional matrix, which is why we set X = np.random.rand(100,1) and not np.random.rand(100)"
      ],
      "metadata": {
        "id": "VzQUfQUeCbL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, this gives us a bunch of predictions, and each prediction will have an error (if it's perfect, the error is $0$). We'll produce the vector of residual errors, and then try to build a model to predict them."
      ],
      "metadata": {
        "id": "dHZ0YAZpNydM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
        "tree_reg2.fit(X, y2);"
      ],
      "metadata": {
        "id": "z8meq_MWCqTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do this again..."
      ],
      "metadata": {
        "id": "VIZM9MfoOEOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
        "tree_reg3.fit(X, y3);"
      ],
      "metadata": {
        "id": "-VfUB1lZCtDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can use our three predictions to predict the value of the function at some given points. For example, at the points $-.4, 0, .5$."
      ],
      "metadata": {
        "id": "ePNxNCngOIGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = np.array([[-0.4], [0.], [0.5]])\n",
        "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ],
      "metadata": {
        "id": "aCdG4u1vCuLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a look at how these models do on predicting our noisy data:"
      ],
      "metadata": {
        "id": "jSQtTwgiOQr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(regressors, X, y, axes, style,\n",
        "                     label=None, data_style=\"b.\", data_label=None):\n",
        "    x1 = np.linspace(axes[0], axes[1], 500)\n",
        "    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\n",
        "                 for regressor in regressors)\n",
        "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
        "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
        "    if label or data_label:\n",
        "        plt.legend(loc=\"upper center\")\n",
        "    plt.axis(axes)\n",
        "\n",
        "plt.figure(figsize=(11, 11))\n",
        "\n",
        "plt.subplot(3, 2, 1)\n",
        "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n",
        "                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\n",
        "plt.ylabel(\"$y$  \", rotation=0)\n",
        "plt.title(\"Residuals and tree predictions\")\n",
        "\n",
        "plt.subplot(3, 2, 2)\n",
        "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
        "                 label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
        "plt.title(\"Ensemble predictions\")\n",
        "\n",
        "plt.subplot(3, 2, 3)\n",
        "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
        "                 label=\"$h_2(x_1)$\", data_style=\"k+\",\n",
        "                 data_label=\"Residuals: $y - h_1(x_1)$\")\n",
        "plt.ylabel(\"$y$  \", rotation=0)\n",
        "\n",
        "plt.subplot(3, 2, 4)\n",
        "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],\n",
        "                  style=\"r-\", label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
        "\n",
        "plt.subplot(3, 2, 5)\n",
        "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
        "                 label=\"$h_3(x_1)$\", data_style=\"k+\",\n",
        "                 data_label=\"Residuals: $y - h_1(x_1) - h_2(x_1)$\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$y$  \", rotation=0)\n",
        "\n",
        "plt.subplot(3, 2, 6)\n",
        "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,\n",
        "                 axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
        "                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7fqP7CClCyS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn provides a GradientBoostingRegression task so we don't need to do this by hand. For example, the code below will create a predictor identical to the one we just build in three steps."
      ],
      "metadata": {
        "id": "OKwdbNY0PTur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n",
        "                                 learning_rate=1.0, random_state=42)\n",
        "gbrt.fit(X, y);"
      ],
      "metadata": {
        "id": "rCUHd1TAC36N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
        "                 label=\"GBRT Predictions\", data_label=\"Training set\")\n",
        "plt.title(\"GradientBoostingRegressor\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kJj6qQSfPr5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, instead of specifying the number of estimators, you can instead just keep adding estimators until your model stops improving. This is usually best to do with a small learning rate (so additional models don't change the overall model much).\n",
        "\n",
        "For example, in the code below we use up to 500 estimators, with a learning rate of .05 (so only 1/20 as impactful as our learning rate 1 models above). We tell it to stop adding new models if it adds 10 with no improvement."
      ],
      "metadata": {
        "id": "VmPrMlm5PqAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt_best = GradientBoostingRegressor(\n",
        "    max_depth=2, learning_rate=0.05, n_estimators=500,\n",
        "    n_iter_no_change=10, random_state=42)\n",
        "gbrt_best.fit(X, y);"
      ],
      "metadata": {
        "id": "GvaMdN91IJt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt_best.n_estimators_"
      ],
      "metadata": {
        "id": "BIk-ljscINR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, while it could have included up to 500 models, it stopped at 92. What do we mean by no improvement? Well, that's a parameter you can specify with the *tol* hyperparameter, which defaults to $.0001$."
      ],
      "metadata": {
        "id": "br6tY067RS_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can graph both the ensemble predictors we've constructed with the code below:"
      ],
      "metadata": {
        "id": "5Uvn732hRrYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
        "                 label=\"Ensemble predictions\")\n",
        "plt.title(f\"learning_rate={gbrt.learning_rate}, \"\n",
        "          f\"n_estimators={gbrt.n_estimators_}\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$y$\", rotation=0)\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\")\n",
        "plt.title(f\"learning_rate={gbrt_best.learning_rate}, \"\n",
        "          f\"n_estimators={gbrt_best.n_estimators_}\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-2JdrmpqIPaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking"
      ],
      "metadata": {
        "id": "bVqK8W2OSB7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last ensemble method we'll discuss is *stacking*, which is kind of a meta-ensemble. It's based on a simple idea: instead of using something like hard voting to make the prediction, instead train a model train a model to perform this aggregation and decide the weights of the various predictors.\n",
        "\n",
        "Basically, a final model sits on top of all the rest and blends them."
      ],
      "metadata": {
        "id": "421UhX7DTlsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn provides a *StackingClassifier* for doing this. The one below uses three estimators, and then a random forest classifier to blend them."
      ],
      "metadata": {
        "id": "qdJIcmeJWK7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', LogisticRegression(random_state=42)),\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('svc', SVC(probability=True, random_state=42))\n",
        "    ],\n",
        "    final_estimator=RandomForestClassifier(random_state=42),\n",
        "    cv=5  # number of cross-validation folds\n",
        ")\n",
        "stacking_clf.fit(X_train, y_train);"
      ],
      "metadata": {
        "id": "xA2tKDMdIXdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does it do?"
      ],
      "metadata": {
        "id": "j4ZhPNMuWY5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_clf.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "1ZHiM5l0Isio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bit better than our voting classifier we saw in an earlier lecture, but not much. The difference in 92.8% vs 92%."
      ],
      "metadata": {
        "id": "V4GO-C2zWcY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References"
      ],
      "metadata": {
        "id": "OX4JJeKFW6zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* [Original AdaBoost paper](https://www.sciencedirect.com/science/article/pii/S002200009791504X)\n",
        "* [AdaBoost video](https://youtu.be/LsK-xG1cLYA?si=JT2TELEsFZtC90eI)\n",
        "* [Gradient boost videos](https://youtu.be/3CC4N4z3GJc?si=n9_38p5GHjKrvp5m)\n",
        "\n"
      ],
      "metadata": {
        "id": "dXPrYodKW9HU"
      }
    }
  ]
}