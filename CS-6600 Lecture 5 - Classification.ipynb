{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6ae6128",
      "metadata": {
        "id": "f6ae6128"
      },
      "source": [
        "# CS-5600/6600 Lecture 5 - Classification\n",
        "\n",
        "**Instructor: Dylan Zwick**\n",
        "\n",
        "*Weber State University*\n",
        "\n",
        "Reference: [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) by Aurélien Géron - [Classification](https://github.com/ageron/handson-ml3/blob/main/03_classification.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.patches as patches  #for the curved arrow\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "vXuSe8cTYEkK"
      },
      "id": "vXuSe8cTYEkK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://imgs.xkcd.com/comics/tasks.png\" alt=\"Task from xkcd\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "iIl8-msTbtE6"
      },
      "id": "iIl8-msTbtE6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [comic above](https://xkcd.com/1425/) is from September, 2014. It's kind of funny because, today, that's a pretty straightforward task. In research, frequently you don't know what's hard and what's easy, and what's hard today could seem easy tomorrow. They say that in the 60s Marvin Minsky had some undergraduate researchers at MIT take pictures of common objects and then try to write a computer program to recognize them. He figured it would be a good summer project. 50 years later, researchers were still working on it.\n",
        "\n",
        "Today, we're not going to dive into approaches and algorithms that can be used for a classification task like the \"bird in the photo\" one from the above comic. Instead we're going to step back and examine some basic questions about how we approach classification problems. Specifically:\n",
        "\n",
        "\n",
        "* How do we define success for a classification algorithm?\n",
        "* How do we verify if a classification algorithm is successful?\n",
        "* How do we measure how successful a classification algorithm is?\n",
        "\n",
        "Now, these questions might at first look obvious. One goal of this lecture is to demonstrate that they are not."
      ],
      "metadata": {
        "id": "G842aSTxfwHm"
      },
      "id": "G842aSTxfwHm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifying Handwritten Digits With The MNIST Dataset"
      ],
      "metadata": {
        "id": "urpQra4YdzGe"
      },
      "id": "urpQra4YdzGe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we'll be using today will be the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. The original NIST dataset was collected in the 1980s and used to try to tackle an early, non-trivial machine learning problem - [translating handwriting into text](https://youtu.be/FwFduRA_L6Q?si=C2Y45WIFL4zMtFoH). The MNIST (\"M\" stands for \"modified\") is based on this original dataset, but with some changes.\n",
        "\n",
        "This problem and dataset has been studied so much, it's often called the [\"hello world\"](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program) of machine learning. The Scikit-Learn (sklearn) library makes many popular learning datasets available through its [sklearn.datasets](https://scikit-learn.org/stable/api/sklearn.datasets.html) package, and provides helper functions to download them. Some of [these datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) are so small they're just included with sklearn, and can be accessed with \"load\" functions. Others (like MNIST) must be downloaded online, and are accessed by \"fetch\" functions. There are also \"make\" functions that are used to generate synthetic (or \"fake\") datasets.\n",
        "\n",
        "Let's grab the MNIST dataset."
      ],
      "metadata": {
        "id": "Vj1VnlsHYHVr"
      },
      "id": "Vj1VnlsHYHVr"
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', as_frame=False, parser='auto') #We set as_frame=False because we want these as NumPy arrays, not as Pandas dataframes."
      ],
      "metadata": {
        "id": "7WSvwZS-lAiB"
      },
      "id": "7WSvwZS-lAiB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These datasets generally contain a description (DESCR) entry that describes what they are and where they came from. For example:"
      ],
      "metadata": {
        "id": "SauvPyB5a-AW"
      },
      "id": "SauvPyB5a-AW"
    },
    {
      "cell_type": "code",
      "source": [
        "print(mnist.DESCR)"
      ],
      "metadata": {
        "id": "GZrC6GIwjqmA"
      },
      "id": "GZrC6GIwjqmA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These datasets are usually returned as an (X,y) tuple containing the input data and a bunch of targets. We'll set these equal to, respectively, X and y."
      ],
      "metadata": {
        "id": "azrlY9pjbNEw"
      },
      "id": "azrlY9pjbNEw"
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = mnist.data, mnist.target"
      ],
      "metadata": {
        "id": "J22QbYc9j2J6"
      },
      "id": "J22QbYc9j2J6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the data:"
      ],
      "metadata": {
        "id": "9ocVeaC7b0UI"
      },
      "id": "9ocVeaC7b0UI"
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "TYwMOqK-j63m"
      },
      "id": "TYwMOqK-j63m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 70,000 images, and each image has 784 features. Why? Because each image is a 28 x 28 (which equals 784) grid, and each feature represents one pixel's intensity, ranging from 0 for white to 255 for black."
      ],
      "metadata": {
        "id": "3Z_xwkOLb6IG"
      },
      "id": "3Z_xwkOLb6IG"
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "sJtgYtFTkAxS"
      },
      "id": "sJtgYtFTkAxS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The y values are the target digits - numbers from 0 to 9."
      ],
      "metadata": {
        "id": "wp2LpCWZcMiX"
      },
      "id": "wp2LpCWZcMiX"
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "o8dM9RH7kDSZ"
      },
      "id": "o8dM9RH7kDSZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "k0rOWuYfkFeb"
      },
      "id": "k0rOWuYfkFeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a look at one of these digits by first reshaping the NumPy array, and then displaying it using Matplotlib's [imshow()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) function. We set cmap=\"binary\" to get a grayscale color map."
      ],
      "metadata": {
        "id": "j_HqRx3HcTUx"
      },
      "id": "j_HqRx3HcTUx"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_digit(image_data):\n",
        "  image = image_data.reshape(28,28)\n",
        "  plt.imshow(image, cmap=\"binary\") #Setting the color map (cmap) to \"binary\" produces a greyscale color map where 0 is white and 255 is black\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "rgJZ3vqTkGfx"
      },
      "id": "rgJZ3vqTkGfx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll grab the first digit and check out its values."
      ],
      "metadata": {
        "id": "Gvde3FsCctsA"
      },
      "id": "Gvde3FsCctsA"
    },
    {
      "cell_type": "code",
      "source": [
        "some_digit = X[0]\n",
        "some_digit"
      ],
      "metadata": {
        "id": "MMAvJY6-mXlW"
      },
      "id": "MMAvJY6-mXlW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how those values look as a grayscale image."
      ],
      "metadata": {
        "id": "wbtLGHjgcy4T"
      },
      "id": "wbtLGHjgcy4T"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_digit(some_digit)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-PJsxVwYl773"
      },
      "id": "-PJsxVwYl773",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks like a 5, and indeed it is."
      ],
      "metadata": {
        "id": "lCNGy_Y7dBbH"
      },
      "id": "lCNGy_Y7dBbH"
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "id": "3r0trNUmmhn_"
      },
      "id": "3r0trNUmmhn_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to check out some more figures, we can with the code below. (Don't worry about the specifics of how this code works, it's not necessary for understanding the material from this lecture.)"
      ],
      "metadata": {
        "id": "vvj5vUt1dIEz"
      },
      "id": "vvj5vUt1dIEz"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9, 9))\n",
        "for idx, image_data in enumerate(X[:100]):\n",
        "    plt.subplot(10, 10, idx + 1)\n",
        "    plot_digit(image_data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pUkNyHxWdLZL"
      },
      "id": "pUkNyHxWdLZL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Binary Classifier"
      ],
      "metadata": {
        "id": "R6v9FvKven2q"
      },
      "id": "R6v9FvKven2q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we won't try to classify all the digits - that comes later. Instead, we'll focus on an easier task: *5 or not*. Specifically, we want to build a classifier that will discriminate the 5s from the other digits. Pretty straightforward."
      ],
      "metadata": {
        "id": "VS5KXUn0esXM"
      },
      "id": "VS5KXUn0esXM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we go further, let's split the data into a training dataset and a test dataset."
      ],
      "metadata": {
        "id": "bFYkIl7k3Z8x"
      },
      "id": "bFYkIl7k3Z8x"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ],
      "metadata": {
        "id": "Mrd4i_JI3eJP"
      },
      "id": "Mrd4i_JI3eJP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a new array whose entries aren't numbers, but are just Booleans indicating whether the corresponding image is a 5 or not."
      ],
      "metadata": {
        "id": "wP0rcpojfEw0"
      },
      "id": "wP0rcpojfEw0"
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_5 = (y_train == '5') #This creates an array that is True for all 5s, and False for all other digits\n",
        "y_test_5 = (y_test =='5')\n",
        "y_train_5"
      ],
      "metadata": {
        "id": "3Why40YQmlnB"
      },
      "id": "3Why40YQmlnB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we're going to use a linear classifier, called a *stochastic gradient descent* classifier, which we can find in sklearn's [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) class. We'll learn more about linear classifiers and stochastic gradient descend later, but for now just understand that it's a model that we can train to make binary classifications."
      ],
      "metadata": {
        "id": "xvOMYzczfOta"
      },
      "id": "xvOMYzczfOta"
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_clf = SGDClassifier(random_state = 42) #Setting the random state insures we get the same \"random\" behavior every time. Why 42? IYKYK"
      ],
      "metadata": {
        "id": "zIr1iUgbn0FU"
      },
      "id": "zIr1iUgbn0FU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've now created a stochastic gradient descent classifier object. We now want to train this object on our data."
      ],
      "metadata": {
        "id": "o02ayXR54K9u"
      },
      "id": "o02ayXR54K9u"
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_clf.fit(X_train, y_train_5) #This may take a minute."
      ],
      "metadata": {
        "id": "wrUleoO94R8Z"
      },
      "id": "wrUleoO94R8Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've trained our model, we can check how well it does on our first digit (which, recall, is in fact a 5). We can do so with the predict method in the classifier class."
      ],
      "metadata": {
        "id": "8y7JPWVCf0na"
      },
      "id": "8y7JPWVCf0na"
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_clf.predict([some_digit])"
      ],
      "metadata": {
        "id": "wHsGQvKjp4-k"
      },
      "id": "wHsGQvKjp4-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nailed it!"
      ],
      "metadata": {
        "id": "3rp2nXTYf7mj"
      },
      "id": "3rp2nXTYf7mj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring Success for a Classification Algorithm"
      ],
      "metadata": {
        "id": "RSh5QQCyf9zz"
      },
      "id": "RSh5QQCyf9zz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we tell if our classification algorithm is successful? One of the more straightforward ways we could attempt to measure this is with its accuracy - the percentage of cases where it makes the right call. Let's do that for our test data. First, we generate the array of our predictions:"
      ],
      "metadata": {
        "id": "UsEt_p6LgIUO"
      },
      "id": "UsEt_p6LgIUO"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_5 = sgd_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "YSnjn98dpIMt"
      },
      "id": "YSnjn98dpIMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll check the accuracy:"
      ],
      "metadata": {
        "id": "C82RKxOnpQ7u"
      },
      "id": "C82RKxOnpQ7u"
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_5,y_pred_5)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "L1C_U6UGquMM"
      },
      "id": "L1C_U6UGquMM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's pretty good! However, this can be deceptive. Our dataset isn't one where half of the cases are 5s, and half are not. In fact, if you were to just guess the most common value using a [\"dummy classifier\"](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) we'd get an accuracy close to 90%."
      ],
      "metadata": {
        "id": "rUetok5kg-2I"
      },
      "id": "rUetok5kg-2I"
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_clf = DummyClassifier()\n",
        "dummy_clf.fit(X_train, y_train_5);"
      ],
      "metadata": {
        "id": "NCLJeyD8rl7o"
      },
      "id": "NCLJeyD8rl7o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_5_dummy = dummy_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "GtP1cBgzr3EJ"
      },
      "id": "GtP1cBgzr3EJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_5,y_pred_5_dummy)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "A1MVkZwhhrk9"
      },
      "id": "A1MVkZwhhrk9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, 94.9% is better than the dummy, but it's maybe not all *that* much better. This is one reason why accuracy might not generally be the best performance measure, particularly when there are \"skewed\" datasets (datasets where some target classes are much more common than others).\n",
        "\n",
        "A way of looking at the performance that provides us much more information than just the accuracy is through a *confusion matrix*."
      ],
      "metadata": {
        "id": "L0ItKqI1hu8K"
      },
      "id": "L0ItKqI1hu8K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Confusion Matrix"
      ],
      "metadata": {
        "id": "KWVaRNAAiT5Z"
      },
      "id": "KWVaRNAAiT5Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1lC5P5HjgkH4JFUAuckPSXN0u-yN7R1xx\" alt=\"Confusion Matrix\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "8zFWwLWJ7xZy"
      },
      "id": "8zFWwLWJ7xZy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind a confusion matrix is there are two types of errors:\n",
        "\n",
        "\n",
        "*   **False Positive** - When you predict something is true, and it's actually false. (Also called Type I errors.)\n",
        "*   **False Negative** - When you predict something is false, and it's actually true. (Also called Type II errors.)\n",
        "\n",
        "An accuracy score treats false positives and false negatives the same. Usually, they're not.\n",
        "\n",
        "For example, suppose you were working on a bomb detection system for airport security. A false positive might lead to a bit more investigation. A false negative might lead to a bomb on a plane. Pretty big difference!\n",
        "\n",
        "Now, you might think what you should try to do in a good model is minimize both, which would be nice. However, generally speaking that's impossible, as decreasing one tends to increase the other. At the extremes, if you reject everything (say everything is false) then you'll never have a false positive, but you'll maximize the false negatives. If, on the other hand, you accept everything (say everything is true) then you'll never have a false negative, but you'll maximize the false positives.\n",
        "\n",
        "Figuring out the optimal balance is partly a math problem and partly a judgement call, and it usually depends on the relative costs (or penalties) of getting them wrong."
      ],
      "metadata": {
        "id": "Z6ogbb17iWD4"
      },
      "id": "Z6ogbb17iWD4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind a confusion matrix is that each row represents the actual value, and each column represents the predicted value. So, the numbers along the diagonal are the correct predictions (the true positives and true negatives), while the numbers off the diagonal are the incorrect predictions. Let's take a look at the confusion matrix for our SGD classifier on the *5 or not* problem. First, we'll form a vector of our predictions."
      ],
      "metadata": {
        "id": "s0Sbn-NT4oLy"
      },
      "id": "s0Sbn-NT4oLy"
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = sgd_clf.predict(X_train)"
      ],
      "metadata": {
        "id": "3rj5qPsrr-IB"
      },
      "id": "3rj5qPsrr-IB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll compare our vector of predictions against the vector of actual values with the [*confusion_matrix*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#confusion-matrix) function."
      ],
      "metadata": {
        "id": "BXINDacO5ezT"
      },
      "id": "BXINDacO5ezT"
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_train_5, y_train_pred)\n",
        "cm"
      ],
      "metadata": {
        "id": "FDhwdHW9s83P"
      },
      "id": "FDhwdHW9s83P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note the first row and column here represents FALSE, while the second row and column represent TRUE, which is the reverse of the image above. Sorry."
      ],
      "metadata": {
        "id": "T8-P8E3-AKqk"
      },
      "id": "T8-P8E3-AKqk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the first row / column represents negative (false), while the second represents positive (true). So, there are 53,892 instances where the digit was not a 5 and the model predicted it was not (true negatives), 687 instances where the digit was a 5 and the model predicted it was not (false positives), 1,891 instances where the digit was a 5 and the model predicted it was not (false negatives), and 3530 times where the digit was a 5 and the model predicted it was (true positives).\n",
        "\n",
        "If we had a perfect model, there would be no entries off the diagonal, and our  confusion matrix would look like:"
      ],
      "metadata": {
        "id": "Gbl-SAkk5oey"
      },
      "id": "Gbl-SAkk5oey"
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_perfect_predictions = y_train_5 #Pretend we have a perfect predictor - sometimes called an \"oracle\".\n",
        "confusion_matrix(y_train_5, y_train_perfect_predictions)"
      ],
      "metadata": {
        "id": "2gWIe7qAtHsf"
      },
      "id": "2gWIe7qAtHsf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, in the real world, we'll never have a perfect model. So, for an imperfect model, there are a few measures of success that we can use.\n",
        "\n",
        "The first, *precision*, measures how frequently the model is correct when it predicts a positive:\n",
        "\n",
        "<center>\n",
        "$\\displaystyle \\text{Precision} = \\frac{TP}{TP + FP}$\n",
        "</center>\n",
        "\n",
        "\n",
        "*   TP = \"True Positive\"\n",
        "*   FP = \"False Positive\"\n",
        "\n",
        "For our 5 or not model, the [precision score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#precision-score) is:"
      ],
      "metadata": {
        "id": "q4LOsd8yE70D"
      },
      "id": "q4LOsd8yE70D"
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(y_train_5, y_train_pred) # = 4820 / (4820+2263)"
      ],
      "metadata": {
        "id": "nQ3Y8Fs5tnhK"
      },
      "id": "nQ3Y8Fs5tnhK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK. So, when the model predicts an image is a 5, it's right about 68% of the time. Is that good? Well, it's a lot better than random guessing, but a lot worse than a person."
      ],
      "metadata": {
        "id": "z5nTj2Y_GDia"
      },
      "id": "z5nTj2Y_GDia"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other metrics we can use to measure success, like *recall*. Recall measures the percentage of actual positives the model identified:\n",
        "\n",
        "<center>\n",
        "$\\displaystyle \\text{Recall} = \\frac{TP}{TP + FN}$\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "*   FN = \"False Negative\"\n",
        "\n",
        "For our 5 or not model, the [recall score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#recall-score) is:\n"
      ],
      "metadata": {
        "id": "8qVwfNvgKQeH"
      },
      "id": "8qVwfNvgKQeH"
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y_train_5, y_train_pred) # == 4820 / (4820 + 601)"
      ],
      "metadata": {
        "id": "6KvjB0Ept21X"
      },
      "id": "6KvjB0Ept21X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, of the actual images of 5s, the model correctly identified about 89%. Again, much better than random guessing, but really not that good, and certainly *much* worse than a person."
      ],
      "metadata": {
        "id": "SYUfX-2HQfWd"
      },
      "id": "SYUfX-2HQfWd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Precision-Recall Tradeoff"
      ],
      "metadata": {
        "id": "BBZLgWka8HJy"
      },
      "id": "BBZLgWka8HJy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally speaking, improving precision comes at the price of reducing recall, and vice versa. The precise relation depends on the model, and (for a given dataset) can be plotted and explored.\n",
        "\n",
        "Under the hood, what every binary classification model does is produce a number - or score - that correlates with whether the input is thought to be positive or negative (true or false). A higher score means it's more likely to be positive, a lower score means it's more likely to be negative. Sometimes this score is or can be interpreted as a probability, but not always. The classification decision is made by whether this score is above a given threshold.\n",
        "\n",
        "For our SGD classifier, this score is the output of its \"decision function\", and we can access the value of this function using the [*decision_function*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function) method. For example:"
      ],
      "metadata": {
        "id": "BVir4DFeRPaG"
      },
      "id": "BVir4DFeRPaG"
    },
    {
      "cell_type": "code",
      "source": [
        "y_scores = sgd_clf.decision_function([some_digit])\n",
        "y_scores"
      ],
      "metadata": {
        "id": "YayhLqkLuToY"
      },
      "id": "YayhLqkLuToY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the threshold value for this decision function is $0$. For this threshold, we see it predicts that the first digit is a 5, which indeed it is."
      ],
      "metadata": {
        "id": "YkgcTFCyS57O"
      },
      "id": "YkgcTFCyS57O"
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0\n",
        "y_some_digit_pred = (y_scores > threshold)\n",
        "y_some_digit_pred"
      ],
      "metadata": {
        "id": "to9Ewdxo1bKe"
      },
      "id": "to9Ewdxo1bKe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we could set the threshold to be something else, and then make our predictions based on this different threshold. If we set the threshold to be, say, 3000, then the first digit wouldn't make the cut:"
      ],
      "metadata": {
        "id": "23DmhHDtTTP1"
      },
      "id": "23DmhHDtTTP1"
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 3000\n",
        "y_some_digit_pred = (y_scores > threshold)\n",
        "y_some_digit_pred"
      ],
      "metadata": {
        "id": "mlr98EGK1ikH"
      },
      "id": "mlr98EGK1ikH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally speaking, the higher the threshold, the more certain the model must be before it makes a positive prediction. Having a high threshold tends to decrease the number of false positives, and this increases precision. However, it also tends to increase the number of false negatives, which decreases recall. We can graph this relation with the [*precision_recall_curve*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#precision-recall-curve) function from the metrics library in sklearn."
      ],
      "metadata": {
        "id": "W1I7QC7vThkF"
      },
      "id": "W1I7QC7vThkF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's calculate the scores using cross validation and our SGD classifier."
      ],
      "metadata": {
        "id": "gQsVozNDUvvj"
      },
      "id": "gQsVozNDUvvj"
    },
    {
      "cell_type": "code",
      "source": [
        "y_scores = sgd_clf.decision_function(X_train)\n",
        "y_scores"
      ],
      "metadata": {
        "id": "fkcyYzx71nd2"
      },
      "id": "fkcyYzx71nd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this vector of scores, along with the vector of actual values, to get the precision and recall numbers for various thresholds. Note that, for a given model and dataset, the precision and recall values are functions of the threshold."
      ],
      "metadata": {
        "id": "s1wCOIglV07p"
      },
      "id": "s1wCOIglV07p"
    },
    {
      "cell_type": "code",
      "source": [
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
      ],
      "metadata": {
        "id": "7Sfp96lY2GfO"
      },
      "id": "7Sfp96lY2GfO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use these precision, recall, and threshold values to chart how both precision and recall change with different threshold values."
      ],
      "metadata": {
        "id": "hmIKq7JWV8gR"
      },
      "id": "hmIKq7JWV8gR"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))  # Formatting\n",
        "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
        "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
        "\n",
        "idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
        "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
        "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
        "plt.axis([-50000, 50000, 0, 1])\n",
        "plt.grid()\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.legend(loc=\"center right\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dnrOTlyD2N0E"
      },
      "id": "dnrOTlyD2N0E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dotted vertical line represents the threshold of 3000. Note also that the precision value tends to wobble a bit at very high thresholds due to the small number of positive predictions and the high variability of the precision to these predictions dropping out."
      ],
      "metadata": {
        "id": "63kXbq1YWYDQ"
      },
      "id": "63kXbq1YWYDQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could also graph the precision value on the vertial axis and the recall on the horizontal, and create a curve parametrized by the threshold. Doing so gives us a curve like the one below, which allows us to more directly determine how precision changes as a function of recall."
      ],
      "metadata": {
        "id": "JT4ukpqxW5Sk"
      },
      "id": "JT4ukpqxW5Sk"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n",
        "\n",
        "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
        "\n",
        "# extra code – just beautifies and saves Figure 3–6\n",
        "plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
        "plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
        "plt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
        "         label=\"Point at threshold 3,000\")\n",
        "plt.gca().add_patch(patches.FancyArrowPatch(\n",
        "    (0.79, 0.60), (0.61, 0.78),\n",
        "    connectionstyle=\"arc3,rad=.2\",\n",
        "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
        "    color=\"#444444\"))\n",
        "plt.text(0.56, 0.62, \"Higher\\nthreshold\", color=\"#333333\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.grid()\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U7Tk2KM0339F"
      },
      "id": "U7Tk2KM0339F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we wanted our precision to be 90%. What is the corresponding threshold? Well, we could attempt to eyeball it from the chart above, but if we want to be precise (we are concerned about precision, after all) we can derive it from our precision and threshold vectors:"
      ],
      "metadata": {
        "id": "i9WIShX6XQZy"
      },
      "id": "i9WIShX6XQZy"
    },
    {
      "cell_type": "code",
      "source": [
        "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
        "threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
        "threshold_for_90_precision"
      ],
      "metadata": {
        "id": "p8Rv9uo45E1U"
      },
      "id": "p8Rv9uo45E1U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_90 = (y_scores >= threshold_for_90_precision)\n",
        "precision_score(y_train_5, y_train_pred_90)"
      ],
      "metadata": {
        "id": "4Ft1dy-u5HzO"
      },
      "id": "4Ft1dy-u5HzO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what's the recall score if we have 90% precision?"
      ],
      "metadata": {
        "id": "wIkaQBIsX2Ow"
      },
      "id": "wIkaQBIsX2Ow"
    },
    {
      "cell_type": "code",
      "source": [
        "recall_at_90_precision = recall_score(y_train_5, y_train_pred_90)\n",
        "recall_at_90_precision"
      ],
      "metadata": {
        "id": "8SYyXGqk5OcS"
      },
      "id": "8SYyXGqk5OcS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, at 90% precision, we have a recall below 50%. In other words, if we want to make sure that when the model says it's a 5 then it's right 90% of the time, we have to live with it failing to identify a majority of the 5s. Is that OK? Well, for most applications, probably not."
      ],
      "metadata": {
        "id": "Zs5hzV7xYC6h"
      },
      "id": "Zs5hzV7xYC6h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The ROC Curve"
      ],
      "metadata": {
        "id": "XMhCfy7BYXLO"
      },
      "id": "XMhCfy7BYXLO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing precision or recall on their own won't be a good strategy for building a model. Is there a way that both could be combined in a sensible manner that could be optimized?\n",
        "\n",
        "Well, one way to do this is with something called the $F_{1}$ score, which takes what's called the [*harmonic mean*](https://en.wikipedia.org/wiki/Harmonic_mean) of the precision and recall:\n",
        "\n",
        "<center>\n",
        "$\\displaystyle F_{1} = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}$\n",
        "</center>\n",
        "\n",
        "Calculating this for our SGD classifier we get:"
      ],
      "metadata": {
        "id": "miZvU83HYZdw"
      },
      "id": "miZvU83HYZdw"
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_train_5, y_train_pred)"
      ],
      "metadata": {
        "id": "y7OFDe4Fba19"
      },
      "id": "y7OFDe4Fba19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the $F_{1}$ score favors classifiers that have similar precision and recall, which isn't always what we want."
      ],
      "metadata": {
        "id": "jvXL-dnpbh77"
      },
      "id": "jvXL-dnpbh77"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another overall measure is derived from the ROC (receiver operating characteristic) curve. The ROC curve is very similar to the precision/recall curve, but instead of plotting the precision against the recall, it plots the recall (also known as the true positive rate) against the fall-out (also known as the false positive rate).\n",
        "<center>\n",
        "$\\displaystyle \\text{Fall-Out} = \\frac{FP}{FP + TN}$\n",
        "</center>\n",
        "\n",
        "* TN = \"True Negative\"\n",
        "\n",
        "We can plot the ROC curve using the [*roc_curve*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#roc-curve) function:"
      ],
      "metadata": {
        "id": "z4-s4kkTborK"
      },
      "id": "z4-s4kkTborK"
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
      ],
      "metadata": {
        "id": "Ydsg310x5Q5U"
      },
      "id": "Ydsg310x5Q5U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting this we get:"
      ],
      "metadata": {
        "id": "ndqy4zEsddYJ"
      },
      "id": "ndqy4zEsddYJ"
    },
    {
      "cell_type": "code",
      "source": [
        "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
        "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
        "\n",
        "plt.figure(figsize=(6, 5))  # Formatting\n",
        "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
        "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
        "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
        "\n",
        "# Adds style\n",
        "plt.gca().add_patch(patches.FancyArrowPatch(\n",
        "    (0.20, 0.89), (0.07, 0.70),\n",
        "    connectionstyle=\"arc3,rad=.4\",\n",
        "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
        "    color=\"#444444\"))\n",
        "plt.text(0.12, 0.71, \"Higher\\nthreshold\", color=\"#333333\")\n",
        "plt.xlabel('False Positive Rate (Fall-Out)')\n",
        "plt.ylabel('True Positive Rate (Recall)')\n",
        "plt.grid()\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.legend(loc=\"lower right\", fontsize=13)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oFlFDte05cEl"
      },
      "id": "oFlFDte05cEl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way we can use the ROC curve to get a measure of the model quality is by calculating the area underneath it. The AUC (area under the curve) will be equal to 1 for a perfect classifier, and equal to .5 for a random classifier.\n",
        "\n",
        "Sklearn provides a function, [*roc_auc_score*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#roc-auc-score), for calculating it. Using this function on our SGD classifier, we get a score of:"
      ],
      "metadata": {
        "id": "2er7yx-1fz-o"
      },
      "id": "2er7yx-1fz-o"
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_train_5, y_scores)"
      ],
      "metadata": {
        "id": "2s6wGnjHMydF"
      },
      "id": "2s6wGnjHMydF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the ROC curve is so similar to the precision/recall (PR) curve, you might wonder how to decide which one to use. Generally speaking, you should prefer the PR curve whenever the positive class is rare, or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve.\n",
        "\n",
        "For our problem, the PR curve would probably be better."
      ],
      "metadata": {
        "id": "f6-aOotfgjL5"
      },
      "id": "f6-aOotfgjL5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework\n",
        "\n",
        "You'll be working on a homework assignment to classify a bunch of images of animals based on whether or not they are an alpaca, and creating these curves for yourself."
      ],
      "metadata": {
        "id": "wzGGztmYjSwr"
      },
      "id": "wzGGztmYjSwr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References"
      ],
      "metadata": {
        "id": "N189YzgrEGdh"
      },
      "id": "N189YzgrEGdh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   [MNIST Dataset](https://youtu.be/5gLarqG8p4s?si=SfEM5cwvTVsH9CYw)\n",
        "*   [ROC and AUC](https://youtu.be/4jRBRDbJemM?si=TX0UAv7_wiVoxz28)\n",
        "\n",
        "* Song of the Day - [Should I Stay or Should I Go?](https://www.youtube.com/watch?v=BN1WwnEDWAM) by The Clash"
      ],
      "metadata": {
        "id": "Y3e3MohlEREA"
      },
      "id": "Y3e3MohlEREA"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}