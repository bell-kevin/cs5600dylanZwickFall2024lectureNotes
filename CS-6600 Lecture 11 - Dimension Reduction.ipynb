{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-5600/6600 Lecture 11 - Dimension Reduction\n",
        "\n",
        "**Instructor: Dylan Zwick**\n",
        "\n",
        "*Weber State University*\n",
        "\n",
        "Reference: [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) by Aurélien Géron - [Dimensionality Reduction](https://github.com/ageron/handson-ml3/blob/main/08_dimensionality_reduction.ipynb)"
      ],
      "metadata": {
        "id": "0iETW1Mm9rv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.spatial.transform import Rotation\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
        "\n",
        "from sklearn.random_projection import GaussianRandomProjection"
      ],
      "metadata": {
        "id": "FD6jSCkKNLSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1Ev6HunJV6MsWFmoup3MKlqSFsaFd8DQL\" alt=\"The Curse of Dimensionality\">\n",
        "</center>\n",
        "\n",
        "I asked ChatGPT to create a Vincent Price horror movie abouth \"The Curse of Dimensionality\". Not bat, except for, you know the words."
      ],
      "metadata": {
        "id": "XR8UeW4mQ8HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Curse of Dimensionality"
      ],
      "metadata": {
        "id": "fbSYoAMaPxcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Lecture 9, we discussed how we can transform categorical data into numeric data through \"one-hot encoding\". Basically, we just create a new feature - a new dimension - for each of the possible categories, and give the observation a 1 for whatever category it has, and a 0 for all the others. Suppose we tried to do this for something like words within a document. We could have a column for every word in a dictionary, and a document's values could be the number of times a given word appears within the document. This is something that is actually done - but would have *tens of thousands* of columns. This would mean each document would be some point in a space with tens of thousands of dimensions."
      ],
      "metadata": {
        "id": "1VNk1nqYRf7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So... what's wrong with that? Well, when you get to very high dimensions, you run into problems. This is knows as the **curse of dimensionality**. (Cue the spooky Halloween music.) What are some of these problems?\n",
        "\n",
        "Well, suppose you pick a random point in a unit square. It will have only about a .4% chance of being located less than .0001 from a border. So, it's unlikely to be \"extreme\" along any dimension. In a 10,000-dimensional unit hypercube, this probability is almost 100%. A more significant difference is that if you pick two points randomly in a unit square, the distance between them will be, on average, $0.52$. If you pick two random points in a three-dimensional unit cube, the average distance will be roughly $0.66%$. However, two points picked randomly in a 1,000,000-dimensional unit hypercube, will have an average distance between them of about 408.25. There's just a lot of space in high dimensions, which means that high dimensional datasets are very sparse. This means a new instance will likely be far away from any training instance, which makes predictions more difficult. We make predictions based upon similarities with things we've already seen, and in high dimensions things just aren't that similar."
      ],
      "metadata": {
        "id": "vo_CIKI8SxfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, how do we solve this? Well, one way is through that great solution that cures all problems in machine learning - MORE DATA! However, that's not always possible. And, in fact, in high dimensions it's particularly difficult. In 100 dimensions to ensure the training instances are within $0.1$ of each other on average, assuming they were spread out uniformly in all dimensions, you'd need more training instances than atoms in the observable universe.\n",
        "\n",
        "So, what do we do in practice? We usually find a projection into fewer dimensions."
      ],
      "metadata": {
        "id": "vbIuqwW6kYgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Projections"
      ],
      "metadata": {
        "id": "8rpNE6a4piqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most real-world problems, training instances are *not* spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space."
      ],
      "metadata": {
        "id": "sWLRlQoepkTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, the observations (circles) in the 3D plot below are all pretty close to a plane. So, we can project them down onto this plane (losing a bit of data), and construct a 2D representation that captures most of what we had in 3D."
      ],
      "metadata": {
        "id": "oA_Fw4LjqNbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate a small 3D dataset. It's an oval shape, rotated in 3D space, with points distributed unevenly, and with quite a lot of noise:"
      ],
      "metadata": {
        "id": "QfOXXy6nq46N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 60\n",
        "X = np.zeros((m, 3))  # initialize 3D dataset\n",
        "np.random.seed(42)\n",
        "angles = (np.random.rand(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n",
        "X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n",
        "X += 0.28 * np.random.randn(m, 3)  # add more noise\n",
        "X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
        "X += [0.2, 0, 0.2]  # shift a bit"
      ],
      "metadata": {
        "id": "HA8eAPUKq7Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)  # dataset reduced to 2D\n",
        "X3D_inv = pca.inverse_transform(X2D)  # 3D position of the projected samples\n",
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, Vt = np.linalg.svd(X_centered)\n",
        "\n",
        "axes = [-1.4, 1.4, -1.4, 1.4, -1.1, 1.1]\n",
        "x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 10),\n",
        "                     np.linspace(axes[2], axes[3], 10))\n",
        "w1, w2 = np.linalg.solve(Vt[:2, :2], Vt[:2, 2])  # projection plane coefs\n",
        "z = w1 * (x1 - pca.mean_[0]) + w2 * (x2 - pca.mean_[1]) - pca.mean_[2]  # plane\n",
        "X3D_above = X[X[:, 2] >= X3D_inv[:, 2]]  # samples above plane\n",
        "X3D_below = X[X[:, 2] < X3D_inv[:, 2]]  # samples below plane\n",
        "\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "# plot samples and projection lines below plane first\n",
        "ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], \"ro\", alpha=0.3)\n",
        "for i in range(m):\n",
        "    if X[i, 2] < X3D_inv[i, 2]:\n",
        "        ax.plot([X[i][0], X3D_inv[i][0]],\n",
        "                [X[i][1], X3D_inv[i][1]],\n",
        "                [X[i][2], X3D_inv[i][2]], \":\", color=\"#F88\")\n",
        "\n",
        "ax.plot_surface(x1, x2, z, alpha=0.1, color=\"b\")  # projection plane\n",
        "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b+\")  # projected samples\n",
        "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b.\")\n",
        "\n",
        "# now plot projection lines and samples above plane\n",
        "for i in range(m):\n",
        "    if X[i, 2] >= X3D_inv[i, 2]:\n",
        "        ax.plot([X[i][0], X3D_inv[i][0]],\n",
        "                [X[i][1], X3D_inv[i][1]],\n",
        "                [X[i][2], X3D_inv[i][2]], \"r--\")\n",
        "\n",
        "ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], \"ro\")\n",
        "\n",
        "def set_xyz_axes(ax, axes):\n",
        "    ax.xaxis.set_rotate_label(False)\n",
        "    ax.yaxis.set_rotate_label(False)\n",
        "    ax.zaxis.set_rotate_label(False)\n",
        "    ax.set_xlabel(\"$x_1$\", labelpad=8, rotation=0)\n",
        "    ax.set_ylabel(\"$x_2$\", labelpad=8, rotation=0)\n",
        "    ax.set_zlabel(\"$x_3$\", labelpad=8, rotation=0)\n",
        "    ax.set_xlim(axes[0:2])\n",
        "    ax.set_ylim(axes[2:4])\n",
        "    ax.set_zlim(axes[4:6])\n",
        "\n",
        "set_xyz_axes(ax, axes)\n",
        "ax.set_zticks([-1, -0.5, 0, 0.5, 1])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eoPEZy0qqZ9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the projection viewed as a 2D image:"
      ],
      "metadata": {
        "id": "Bifz1ssdrHe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – this cell generates and saves Figure 8–3\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
        "ax.plot(X2D[:, 0], X2D[:, 1], \"b+\")\n",
        "ax.plot(X2D[:, 0], X2D[:, 1], \"b.\")\n",
        "ax.plot([0], [0], \"bo\")\n",
        "ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True,\n",
        "         head_length=0.1, fc='b', ec='b', linewidth=4)\n",
        "ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True,\n",
        "         head_length=0.1, fc='b', ec='b', linewidth=1)\n",
        "ax.set_xlabel(\"$z_1$\")\n",
        "ax.set_yticks([-0.5, 0, 0.5, 1])\n",
        "ax.set_ylabel(\"$z_2$\", rotation=0)\n",
        "ax.set_axisbelow(True)\n",
        "ax.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kiMj1fqPrM-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did we get this plane? Using something called *principal component analysis* (PCA). The idea behind principle component analysis is that you find the linear subspace (or *hyperplane*) that maximizes the *variance* (the spread) of the projection."
      ],
      "metadata": {
        "id": "Hce2vp3QrXtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Principal Component Analysis"
      ],
      "metadata": {
        "id": "iCW3g_7e6LeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is perhaps easiest to understand in the context of a projection onto a line. Suppose you've got a set of observations in two dimensions:"
      ],
      "metadata": {
        "id": "mOJ3JlRztFV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "angle = np.pi / 5\n",
        "stretch = 5\n",
        "m = 200\n",
        "\n",
        "np.random.seed(3)\n",
        "X_line = np.random.randn(m, 2) / 10\n",
        "X_line = X_line @ np.array([[stretch, 0], [0, 1]])  # stretch\n",
        "X_line = X_line @ [[np.cos(angle), np.sin(angle)],\n",
        "                   [np.sin(angle), np.cos(angle)]]  # rotate\n",
        "\n",
        "u1 = np.array([np.cos(angle), np.sin(angle)])\n",
        "u2 = np.array([np.cos(angle - 2 * np.pi / 6), np.sin(angle - 2 * np.pi / 6)])\n",
        "u3 = np.array([np.cos(angle - np.pi / 2), np.sin(angle - np.pi / 2)])\n",
        "\n",
        "X_proj1 = X_line @ u1.reshape(-1, 1)\n",
        "X_proj2 = X_line @ u2.reshape(-1, 1)\n",
        "X_proj3 = X_line @ u3.reshape(-1, 1)\n",
        "\n",
        "plt.plot(X_line[:, 0], X_line[:, 1], \"ro\", alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AkBhkVCoLKAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many lines upon which we could project these points, and these projections would give us many distributions:"
      ],
      "metadata": {
        "id": "u3Y4AO36L5XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot2grid((3, 2), (0, 0), rowspan=3)\n",
        "plt.plot([-1.4, 1.4], [-1.4 * u1[1] / u1[0], 1.4 * u1[1] / u1[0]], \"k-\",\n",
        "         linewidth=2)\n",
        "plt.plot([-1.4, 1.4], [-1.4 * u2[1] / u2[0], 1.4 * u2[1] / u2[0]], \"k--\",\n",
        "         linewidth=2)\n",
        "plt.plot([-1.4, 1.4], [-1.4 * u3[1] / u3[0], 1.4 * u3[1] / u3[0]], \"k:\",\n",
        "         linewidth=2)\n",
        "plt.plot(X_line[:, 0], X_line[:, 1], \"ro\", alpha=0.5)\n",
        "plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=4, alpha=0.9,\n",
        "          length_includes_head=True, head_length=0.1, fc=\"b\", ec=\"b\", zorder=10)\n",
        "plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=1, alpha=0.9,\n",
        "          length_includes_head=True, head_length=0.1, fc=\"b\", ec=\"b\", zorder=10)\n",
        "plt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", color=\"blue\")\n",
        "plt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", color=\"blue\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\", rotation=0)\n",
        "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot2grid((3, 2), (0, 1))\n",
        "plt.plot([-2, 2], [0, 0], \"k-\", linewidth=2)\n",
        "plt.plot(X_proj1[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
        "plt.gca().get_yaxis().set_ticks([])\n",
        "plt.gca().get_xaxis().set_ticklabels([])\n",
        "plt.axis([-2, 2, -1, 1])\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot2grid((3, 2), (1, 1))\n",
        "plt.plot([-2, 2], [0, 0], \"k--\", linewidth=2)\n",
        "plt.plot(X_proj2[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
        "plt.gca().get_yaxis().set_ticks([])\n",
        "plt.gca().get_xaxis().set_ticklabels([])\n",
        "plt.axis([-2, 2, -1, 1])\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot2grid((3, 2), (2, 1))\n",
        "plt.plot([-2, 2], [0, 0], \"k:\", linewidth=2)\n",
        "plt.plot(X_proj3[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
        "plt.gca().get_yaxis().set_ticks([])\n",
        "plt.axis([-2, 2, -1, 1])\n",
        "plt.xlabel(\"$z_1$\")\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YVLjHLWwL_T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind principal component analysis is we want to find the projection that preserves the greatest distinction among our observations. More precisely, we want to find the projection that maximizes the *variance* (the spread around the mean). Taking a look at our dots above, we see that projection along the solid line keeps the points spread out much more than projection along the dashed line. So, the solid line would be preferable, and is in fact the line we would get through principle component analysis."
      ],
      "metadata": {
        "id": "bK0Rx7htLvx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note there is a lot of very cool linear algebra involved in principal component analysis. Specifically, it's an example of something called a *singular value decomposition*. We won't be able to go over this in detail in this class, but we definitely will in the upcoming math for data science class."
      ],
      "metadata": {
        "id": "F5_dPzxEMrEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More geneally, the first principle component is the line that, if you project onto it, preserves the most variance. The second principle component is the line *perpendicular to the first* that preserves the most variance, which would be the dotted line above. In two dimensions there will only be two principle components, but in three dimensions there will be three, and you can form a projection plane from the first two."
      ],
      "metadata": {
        "id": "PP48bbGtNAcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how we'd use PCA for our 3D dataset created above."
      ],
      "metadata": {
        "id": "laCdscN7NW2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "TvTslCPlzXG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check out the principle components using the *components* attribute of our pca object"
      ],
      "metadata": {
        "id": "i9f97Pd9zlj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.components_"
      ],
      "metadata": {
        "id": "__QRVWlUzxiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A useful bit of information about the principle components is the *explained variance ratio*, which tells you the percentage of the variance of your original dataset that is explained by the various principal components."
      ],
      "metadata": {
        "id": "7FVoMG7_z8A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "mVjhyg9L5NLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the two principal components explain:"
      ],
      "metadata": {
        "id": "yE9hbMWB5TZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_.sum()"
      ],
      "metadata": {
        "id": "6Wxc1NPB5Xkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bit more than 90% of all the variance."
      ],
      "metadata": {
        "id": "FGPoFoTo5btO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Choosing the right number of components"
      ],
      "metadata": {
        "id": "Z9DFB-Nj6Zkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of arbitrarily choosing the number of dimensions to reduce down to, it might be better to choose the number of dimensinos that add up to a sufficiently large portion of the variance - say $95\\%$. An exception here is if you're reducing dimensionality for data visualization, in which case you'd want to limit to 2 or 3. Probably 2."
      ],
      "metadata": {
        "id": "7ew-J87Q5dvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, we can do principal component analysis on the MNIST dataset to see how many components we need to explain 95% of the variance."
      ],
      "metadata": {
        "id": "4kCQl1Ur61gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', as_frame=False, parser=\"auto\")\n",
        "X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
        "X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) +1\n",
        "d"
      ],
      "metadata": {
        "id": "u0OSHCHsLo4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is significantly less than 784, the number of dimensions in our dataset. However, an easier way of doing this would be to set the 95% value when you create the PCA object:"
      ],
      "metadata": {
        "id": "vKja12337nA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 0.95)\n",
        "X_reduced = pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "bdeXRVYj70ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual number of components here will be determined during training, and is stored in the *n_components_* attribute:"
      ],
      "metadata": {
        "id": "BaatDt2X9J1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.n_components_"
      ],
      "metadata": {
        "id": "Ajjib9iz9RL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also plot the explained variance of the number of dimensions, and pick out where it tapers off:"
      ],
      "metadata": {
        "id": "fCNS5joq9TY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(cumsum, linewidth=3)\n",
        "plt.axis([0, 400, 0, 1])\n",
        "plt.xlabel(\"Dimensions\")\n",
        "plt.ylabel(\"Explained Variance\")\n",
        "plt.plot([d, d], [0, 0.95], \"k:\")\n",
        "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
        "plt.plot(d, 0.95, \"ko\")\n",
        "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
        "             arrowprops=dict(arrowstyle=\"->\"))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qu__LvZo9gl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reconstructing The Data From The Principlal Components"
      ],
      "metadata": {
        "id": "0BpmNobL9v7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can decompress the principle components back to the original number of dimensions by applying the inverse transformation of the PCA projection. This won't give you back the original data (you lose some when you project), but you can get close. For example, doing this for our MNIST data we get:"
      ],
      "metadata": {
        "id": "PYAZzKy2-EzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_recovered = pca.inverse_transform(X_reduced)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "for idx, X in enumerate((X_train[::2100], X_recovered[::2100])):\n",
        "    plt.subplot(1, 2, idx + 1)\n",
        "    plt.title([\"Original\", \"Compressed\"][idx])\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            plt.imshow(X[row * 5 + col].reshape(28, 28), cmap=\"binary\",\n",
        "                       vmin=0, vmax=255, extent=(row, row + 1, col, col + 1))\n",
        "            plt.axis([0, 5, 0, 5])\n",
        "            plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "zcVPmi5q-clK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty close!"
      ],
      "metadata": {
        "id": "2wr12Yvd-qUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Projection"
      ],
      "metadata": {
        "id": "0qWehKok-v_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's another type of projection that has no right to work as well as it  does. Namely, *random* projection. It's a bit weird, but random projections are likely to preserve distances fairly well. So, two similar instances will likely remain similar after a random projection, while two different instances will likely remain different.\n",
        "\n",
        "The more dimensions you drop, the more information you lose. Johnson and Lindenstrauss came up with an equation that determines the minumum number of dimensions to preserve in order to ensure, with high probability, that distances won't change by more than a given tolerance. This number is calculated with a function in sklearn."
      ],
      "metadata": {
        "id": "A7WOuOxl-yS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, e = 5000, .1 #Here m is the number of observations, and e is the tolerance. Note this is independent of the original number of dimensions.\n",
        "d = johnson_lindenstrauss_min_dim(m, eps=e)\n",
        "d"
      ],
      "metadata": {
        "id": "kx658S5Q_mh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can just generate a random matrix $P$ of shape $[d,n]$, where each item is sampled randomly from a Gaussian distribution with mean $0$ and variance $1/d$, and use it to project a dataset from $n$ dimensions down to $d$:"
      ],
      "metadata": {
        "id": "sfYLdu5QAFxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 20000\n",
        "np.random.seed(42)\n",
        "P = np.random.randn(d, n) / np.sqrt(d)\n",
        "X = np.random.randn(m,n) #Generate a fake dataset\n",
        "X_reduced = X @ P.T #Project it down to d dimensions"
      ],
      "metadata": {
        "id": "ic1KCLojAcwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! Simple, efficient, and no training is required."
      ],
      "metadata": {
        "id": "V8LZlkIUAtmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn offers a *GaussianRandomProjection* class to do exactly what we just did with its fit method."
      ],
      "metadata": {
        "id": "OM9rrjdeA0BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gaussian_rnd_proj = GaussianRandomProjection(eps=e, random_state=42)\n",
        "X_reduced_2 = gaussian_rnd_proj.fit_transform(X)"
      ],
      "metadata": {
        "id": "zP0z5_hlBIfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn also provides a second random projection transformer, knows as *SparseRandomProjection*. It determines the target dimensionality in the same way, genarets a random matrix of the same shape, and performs the projection identically. The main difference is the random matrix is sparse. It's usually preferable to use this method, as its much cheaper and uses much less space. The ratio $r$ of nonzero items is the spare random matrix is called its *density*. By default, it's equal to $1/\\sqrt{n}$."
      ],
      "metadata": {
        "id": "yEXCaTeeBVTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##References"
      ],
      "metadata": {
        "id": "uFpi0vjKSRvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [PCA from Statquest](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n",
        "* [Original PCA paper from 1901](https://zenodo.org/records/1430636)"
      ],
      "metadata": {
        "id": "r0t5TSMDSU4E"
      }
    }
  ]
}