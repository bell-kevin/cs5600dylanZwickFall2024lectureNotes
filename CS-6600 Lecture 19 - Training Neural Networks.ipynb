{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-5600/6600 Lecture 19 - Training Neural Networks\n",
        "\n",
        "**Instructor: Dylan Zwick**\n",
        "\n",
        "*Weber State University*"
      ],
      "metadata": {
        "id": "HJM-251HyQXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sys"
      ],
      "metadata": {
        "id": "cVuEK6zg3zgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we're going to talk about the basic idea behind a neural network, and see how one can work in action on a very famous problem - classifying hand written digits. We won't get into too much depth on the mathematics, but I've included the deviration of back propagation as an appendix."
      ],
      "metadata": {
        "id": "fOWOPhpgy42S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Single-Layer Neural Network (Perceptron) and Logistic Regression"
      ],
      "metadata": {
        "id": "2hxbuzSW0XS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest type of neural network - a neural network without any internal (or \"hidden\") layers, is called a \"perceptron\", represented below:\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=109BRkgVYMEWAEr5TQjNNetImqOKM3U-n\" alt=\"The Perceptron\">\n",
        "</center>\n",
        "\n",
        "The idea here is that it takes as its input the values from our input variables $X_{1},X_{2}, \\ldots, X_{n}$, multiplies them by their appropriate weights, adds these multiplied weights together, feeds these to an activation function, and then to a unit step function.\n",
        "\n",
        "This might look complicated, but if the activation function is a sigmoid (which it frequently is) then this is just logistic regression.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1Li6XZfZr-O6eKMiLXW3VDEarTql2wohg\" alt=\"The Sigmoid\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "IHCK0fz10O5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formulating this a bit more mathematically, if the inputs are $X = (x_{1},x_{2},\\ldots,x_{m})$, then these inputs are combined into the net input function through a weighted linear combination:\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle z(X) = w_{0} + w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{m}x_{m}$.\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "This linear combination is then sent to an activation function, in this case the sigmoid:\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle \\sigma(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-(w_{0}+w_{1}x_{1}+w_{2}x_{2}+\\cdots+w_{m}x_{m})}}$\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "The sigmoid function goes from 0 to 1 and provides a number we can interpret as a probability.\n",
        "\n",
        "We then get our final prediction by sending the output of the sigmoid to a unit step function, which predicts a $1$ if the value is above a given threshold (frequently $.5$), and $0$ otherwise."
      ],
      "metadata": {
        "id": "0QqDCAJ1G9eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###WARNING: Calculus Ahead - Gradient Descent for the Perceptron"
      ],
      "metadata": {
        "id": "8481Aj7GIw2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have an observation $X$ with output $y$. We want to choose our weights so as to maximize the likelihood of the observed outcome, which is given by:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle \\sigma(z)^{y}(1-\\sigma(z))^{1-y}$.\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Here, we're using that $y$ is either $0$ or $1$. Now, typically instead of maximizing a function, we define a *loss function*, and seek to minimize it. Also, we can note the logarithm is an increasing function, so the input that maximizes / minimizes a function will also maximize / minimize its logarithm. Using these observations, we define our loss function:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle J = -\\left(y\\log{\\sigma(z(X))} + (1-y)\\log{(1-\\sigma(z(X)))}\\right)$.\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "We want to find the values of the weights $w_{0},w_{1},\\ldots,w_{m}$ that minimize this function, and to do this we'll want to find the function's gradient. This means finding the partial derivatives with respect to each weight. We can get the partial derivative of this function with respect to the weight $w_{j}$ using the chain rule:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle \\frac{\\partial J}{\\partial w_{j}} = \\frac{\\partial J}{\\partial z}\\frac{\\partial z}{\\partial w_{j}}$.\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "This will require a bit of calculus, but the derivative of the sigmoid function is:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle \\frac{d\\sigma}{dz} = \\frac{d}{dz}\\left(\\frac{1}{1+e^{-z}}\\right) = \\frac{e^{-z}}{(1+e^{-z})^{2}} = \\left(\\frac{1}{1+e^{-z}}\\right)\\left(1-\\frac{1}{1+e^{-z}}\\right) = \\sigma(z)(1-\\sigma(z))$\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "From this we get that the derivative of the loss function:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle \\frac{\\partial J}{\\partial w_{j}} = \\left(-y\\left(\\frac{1}{\\sigma(z)}\\right)\\sigma(z)(1-\\sigma(z)) + (1-y)\\left(\\frac{1}{1-\\sigma(z)}\\right)\\sigma(z)(1-\\sigma(z))\\right)\\frac{\\partial z}{\\partial w_{j}} = (\\sigma(z)-y)x_{j}$\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "We express as $\\delta$ the partial derivative $\\displaystyle \\frac{\\partial J}{\\partial z}$, and so from this we get $\\delta = \\sigma(z)-y$. This value is typically called the *error* of the output node.\n",
        "\n",
        "From this we can see our gradient is $\\nabla J = \\delta X$."
      ],
      "metadata": {
        "id": "oGXU7xDdH3cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Multi-Layer Neural Network (a.k.a. Deep Learning)"
      ],
      "metadata": {
        "id": "dBuPXclI2FMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A multi-layer neural network (which is essentially everything that's actually called a neural network) is just a bunch of perceptrons (or whatever activation function you want to use) meshed together.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=19_1Jb8hLiv3_7tD0fzgaejFpyTXQbjNi\" alt=\"Multi-Layer Perceptron\">\n",
        "</center>\n",
        "\n",
        "The number of layers and size of each layer is a hyperparameter that is set before the model trains, and then during model training, what you do is update the weights. How these weights are updated is through something called \"back propagation\", which means the errors from the output layer are fed back to the previous layer and the weights are adjusted, the error for the previous layer is then fed back to the layer before it, and so on. It turns out that these steps can be handled sequentially in this manner, so that the entire neural network doesn't need to be modified at once. This significantly decreases the complexity of an update (a step in gradient descent), and the reason you're able to do this is, essentially, just the chain rule from calculus."
      ],
      "metadata": {
        "id": "1xWMvl_i2O0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forward Propagation"
      ],
      "metadata": {
        "id": "YTHn2YS2HxZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we've got our neural network above with $t$ output nodes, and $d$ hidden nodes.\n",
        "\n",
        "At each non-input node there is a net input function, which we denote by $z_{j}^{(l)}$, where $l \\in \\{in,h,out\\}$. So, for example, the net input to the index $1$ node in the hidden layer is:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle z_{1}^{(h)} = w_{0,1}^{(h)}a_{0}^{(in)} + w_{1,1}^{(h)}a_{1}^{(in)} + \\cdots w_{m,1}^{(h)}a_{m}^{(in)}$\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "The output value of each node is the sigmoid of its inputs, so for example the output of the index $1$ node in the hidden layer is:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle a_{1}^{(h)} = \\sigma(z_{1}^{(d)})$\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Note that we can view the set of inputs to the hidden layer as a row vector $\\textbf{z}^{(h)}$, with values given by:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "  $\\displaystyle \\textbf{z}^{(h)} = \\left(\\begin{array}{cccc} z_{1}^{(h)} & z_{2}^{(h)} & \\cdots & z_{d}^{(h)}\\end{array}\\right) = \\left(\\begin{array}{cccc} a_{0}^{(in)} & a_{1}^{(in)} & \\cdots & a_{m}^{(in)}\\end{array}\\right) \\left(\\begin{array}{cccc} w_{0,1}^{(h)} & w_{0,2}^{(h)} & \\cdots & w_{0,d}^{(h)} \\\\ w_{1,1}^{(h)} & w_{1,2}^{(h)} & \\cdots & w_{1,d}^{(h)} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{m,1}^{(h)} & w_{m,2}^{(h)} & \\cdots & w_{m,d}^{(h)}\\end{array}\\right) = \\textbf{a}^{(in)}\\textbf{W}^{(h)}$\n",
        "</center>\n",
        "<br>"
      ],
      "metadata": {
        "id": "mLP1ZrEoJjQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward propagation is, essentially, the continuous application of these transformations over each node of the network."
      ],
      "metadata": {
        "id": "7oSYzmQhEEuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back propagation is how we handle updating the weights through gradient descent. The math behind this is a bit complicated. It's \"only\" multivariable calculus, but it's a serious walk through the multivariable chain rule. I've gone over it twice before in graduate machine learning classes, and both times both I and the students have regretted it. So, I've included the math at the bottom of these notes as an appendix, in case you're curious."
      ],
      "metadata": {
        "id": "onsW7zFQEJlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing a Neural Network"
      ],
      "metadata": {
        "id": "c19yqL9t3NRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are excellent libraries that we'll be learning about and using for working with neural networks. However, today will be an exercise in self-sufficiency. We're going to create a neural network using only NumPy."
      ],
      "metadata": {
        "id": "F4GEbdXq3QNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll step through the code to implement a neural network for the famous task of identifying handwritten digits. I've broken up what would usually be a neural network class into a number of separate functions so we can better understand how they work together, although to be clear keeping them all within a single class is definitely the better way to do it!"
      ],
      "metadata": {
        "id": "CRRgUmspE7Eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's grab the data using the *fetch_openml* function we've used before:"
      ],
      "metadata": {
        "id": "D4yt_ONl4C4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False,\n",
        "                                parser='auto')"
      ],
      "metadata": {
        "id": "t1_2gDUt58Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the size of the data here is pretty significant."
      ],
      "metadata": {
        "id": "OhZQ7nqMLw3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_mnist.shape"
      ],
      "metadata": {
        "id": "uhZLqRSFL1I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't need that many, so let's just grab the first 30,000."
      ],
      "metadata": {
        "id": "R-NPpns0L7Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_mnist[:30000]\n",
        "y_train = y_mnist[:30000]"
      ],
      "metadata": {
        "id": "mseHvQWVL-4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, our $y$ values are actually stored as strings."
      ],
      "metadata": {
        "id": "BDylFiM1MD6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "A4Z3ui67MG1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's convert them to integers."
      ],
      "metadata": {
        "id": "rJoHuIKHMJZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.astype(int)"
      ],
      "metadata": {
        "id": "N1FpDajdMLcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some of these digits:"
      ],
      "metadata": {
        "id": "vywiHQX8MQaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
        "ax = ax.flatten()\n",
        "for i in range(10):\n",
        "    img = X_train[y_train == i][0].reshape(28, 28)\n",
        "    ax[i].imshow(img, cmap='Greys')\n",
        "\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i6e6YW9w6sqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purposes of our algorithm, we're going to encode our data using onehot encoding:"
      ],
      "metadata": {
        "id": "B6zNVxvPMYXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot(y, n_classes):\n",
        "    onehot = np.zeros((y.shape[0], n_classes))\n",
        "    for idx, val in enumerate(y.astype(int)):\n",
        "        onehot[idx, val] = 1.\n",
        "    return onehot"
      ],
      "metadata": {
        "id": "9Rq1hH_x7yhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = np.array([0,1,1,0,1,0,0])\n",
        "onehot(example,2)"
      ],
      "metadata": {
        "id": "DvcVcHT17zX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll even write our own sigmoid function. Note that we've added some code to help avoid some numerical approximation issues. This code will make the sigmoid slower, but that's OK for us today."
      ],
      "metadata": {
        "id": "bT_zbZzoSvUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    # Using np.where to handle large negative values\n",
        "    x = np.clip(x, -709, 709)  # np.exp(709) is close to the max float64\n",
        "    return np.where(\n",
        "        x >= 0,\n",
        "        1 / (1 + np.exp(-x)),\n",
        "        np.exp(x) / (1 + np.exp(x))\n",
        "    )"
      ],
      "metadata": {
        "id": "mGtfg_eU8Blj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sigmoid(-17))\n",
        "print(sigmoid(0))\n",
        "print(sigmoid(1))\n",
        "print(sigmoid(6000))"
      ],
      "metadata": {
        "id": "BCLLxXal8FEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's implement forward propagation:"
      ],
      "metadata": {
        "id": "RuC1d7sVSyrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X,W_h,b_h,W_out,b_out):\n",
        "\n",
        "    z_h = np.dot(X, W_h) + b_h\n",
        "\n",
        "    a_h = sigmoid(z_h)\n",
        "\n",
        "    z_out = np.dot(a_h, W_out) + b_out\n",
        "\n",
        "    a_out = sigmoid(z_out)\n",
        "\n",
        "    return z_h, a_h, z_out, a_out"
      ],
      "metadata": {
        "id": "qvJ1YrLG8KT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now back propagation (see the appendix for details)"
      ],
      "metadata": {
        "id": "8k0cq2xkS2S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back(X,y,W_h,b_h,W_out,b_out,eta):\n",
        "\n",
        "    z_h,a_h,z_out,a_out = forward(X,W_h,b_h,W_out,b_out)\n",
        "\n",
        "    delta_out = a_out - y\n",
        "\n",
        "    sigmoid_derivative_h = a_h * (1. - a_h)\n",
        "\n",
        "    delta_h = (np.dot(delta_out, W_out.T) * sigmoid_derivative_h)\n",
        "\n",
        "    grad_W_h = np.dot(X.T, delta_h)\n",
        "    grad_b_h = np.sum(delta_h, axis=0)\n",
        "\n",
        "    grad_W_out = np.dot(a_h.T, delta_out)\n",
        "    grad_b_out = np.sum(delta_out, axis=0)\n",
        "\n",
        "    W_h -= eta * grad_W_h\n",
        "    b_h -= eta * grad_b_h\n",
        "\n",
        "    W_out -= eta * grad_W_out\n",
        "    b_out -= eta * grad_b_out\n",
        "    return W_h,b_h,W_out,b_out"
      ],
      "metadata": {
        "id": "V0XBjBpW8Nhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll use forward propagation and back propagation to build our fit function."
      ],
      "metadata": {
        "id": "OOboG850TDYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X_train, y_train, epochs, n_hidden, eta, seed=1):\n",
        "\n",
        "    random = np.random.RandomState(seed)\n",
        "\n",
        "    n_output = np.unique(y_train).shape[0]  # number of class labels\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    # weights for input -> hidden\n",
        "    b_h = np.zeros(n_hidden)\n",
        "    W_h = random.normal(loc=0.0, scale=0.1,size=(n_features,n_hidden))\n",
        "\n",
        "    # weights for hidden -> output\n",
        "    b_out = np.zeros(n_output)\n",
        "    W_out = random.normal(loc=0.0, scale=0.1,size=(n_hidden, n_output))\n",
        "\n",
        "    y_train_enc = onehot(y_train, n_output)\n",
        "\n",
        "    # iterate over training epochs\n",
        "    for i in range(epochs):\n",
        "\n",
        "        indices = np.arange(X_train.shape[0])\n",
        "\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        for idx in range(indices.shape[0]):\n",
        "            W_h,b_h,W_out,b_out = back(X_train[[idx]],y_train_enc[[idx]],W_h,b_h,W_out,b_out,eta)\n",
        "\n",
        "        # Evaluation after each epoch during training\n",
        "        z_h, a_h, z_out, a_out = forward(X_train,W_h,b_h,W_out,b_out)\n",
        "        cost = np.sum(-y_train_enc * (np.log(a_out)) - (1. - y_train_enc) * np.log(1. - a_out))\n",
        "\n",
        "        y_train_pred = np.argmax(z_out, axis=1)\n",
        "\n",
        "        train_acc = ((np.sum(y_train == y_train_pred)).astype(float) / X_train.shape[0])\n",
        "\n",
        "        sys.stderr.write('\\r%d/%d | Cost: %.2f '\n",
        "                             '| Train Acc.: %.2f%% ' %\n",
        "                             (i+1, epochs, cost,train_acc*100))\n",
        "        sys.stderr.flush()"
      ],
      "metadata": {
        "id": "B_4r1YhG8RPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it out!"
      ],
      "metadata": {
        "id": "19p2kqX0TIdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "\n",
        "epochs = 10\n",
        "n_hidden = 30\n",
        "eta = .001\n",
        "seed = 42\n",
        "\n",
        "fit(X_train, y_train, epochs, n_hidden, eta, seed)"
      ],
      "metadata": {
        "id": "YDsqks-V8Uts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad! Now, with better neural network architectures we can do much better. We'll be discussing those later."
      ],
      "metadata": {
        "id": "1kKcfRkjHsUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Back Propagation (Appendix)"
      ],
      "metadata": {
        "id": "3cJkNZb3Bo1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall what we said above about forward propagation, repeated here for reference.\n",
        "\n",
        "Suppose we've got our neural network above with $t$ output nodes, and $d$ hidden nodes.\n",
        "\n",
        "At each non-input node there is a net input function, which we denote by $z_{j}^{(l)}$, where $l \\in \\{in,h,out\\}$. So, for example, the net input to the index $1$ node in the hidden layer is:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle z_{1}^{(h)} = w_{0,1}^{(h)}a_{0}^{(in)} + w_{1,1}^{(h)}a_{1}^{(in)} + \\cdots w_{m,1}^{(h)}a_{m}^{(in)}$\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "The output value of each node is the sigmoid of its inputs, so for example the output of the index $1$ node in the hidden layer is:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle a_{1}^{(h)} = \\sigma(z_{1}^{(d)})$\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Note that we can view the set of inputs to the hidden layer as a row vector $\\textbf{z}^{(h)}$, with values given by:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "  $\\displaystyle \\textbf{z}^{(h)} = \\left(\\begin{array}{cccc} z_{1}^{(h)} & z_{2}^{(h)} & \\cdots & z_{d}^{(h)}\\end{array}\\right) = \\left(\\begin{array}{cccc} a_{0}^{(in)} & a_{1}^{(in)} & \\cdots & a_{m}^{(in)}\\end{array}\\right) \\left(\\begin{array}{cccc} w_{0,1}^{(h)} & w_{0,2}^{(h)} & \\cdots & w_{0,d}^{(h)} \\\\ w_{1,1}^{(h)} & w_{1,2}^{(h)} & \\cdots & w_{1,d}^{(h)} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{m,1}^{(h)} & w_{m,2}^{(h)} & \\cdots & w_{m,d}^{(h)}\\end{array}\\right) = \\textbf{a}^{(in)}\\textbf{W}^{(h)}$\n",
        "</center>\n",
        "<br>"
      ],
      "metadata": {
        "id": "9SZe0a39BrbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For any given input $X$, the output will be one of the categories $1,\\ldots,t$. We can encode this as a \"onehot\" vector, in which every element is zero except the one corresponding to the output category, which is $1$. So, for example, if the output is the third category, the output vector $\\textbf{y}$ will be:\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "  $\\displaystyle \\textbf{y} = \\left(\\begin{array}{cccccc} 0 & 0 & 1 & 0 & \\cdots & 0\\end{array}\\right)$\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "In this case, the likelihood of an observation will be:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle \\prod_{i = 1}^{t}\\sigma(z_{i}^{(out)})^{y_{i}}(1-\\sigma(z_{i}^{(out)}))^{1-y_{i}}$,\n",
        "    <br>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "and the loss function will be:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    <br>\n",
        "    $\\displaystyle J = -\\sum_{i = 1}^{t}\\left(y_{i}\\log{\\sigma(z_{i}^{(out)})} + (1-y_{i})\\log{(1-\\sigma(z_{i}^{(out)}))}\\right)$.\n",
        "    <br>\n",
        "</center>\n",
        "<br>"
      ],
      "metadata": {
        "id": "KrGs3YhYB1S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define $\\delta_{j}^{(l)}$ to be the rate of change of the loss function with respect to the input to node $j$ of layer $l$. For example:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle \\delta_{1}^{(h)} = \\frac{\\partial J}{\\partial z_{1}^{(h)}}.$\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "From the chain rule, we know this will be:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle \\frac{\\partial J}{\\partial z_{1}^{(h)}} = \\sum_{i = 1}^{t}\\frac{\\partial J}{\\partial z_{i}^{(out)}}\\frac{\\partial z_{i}^{(out)}}{\\partial z_{1}^{(h)}}$.\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Now, by definition we have:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle z_{i}^{(out)} = \\sum_{j = 1}^{d}w_{j,i}^{(out)}\\sigma(z_{j}^{(h)})$.\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Taking the partial derivative of this with respect to $z_{1}^{(h)}$ we have:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle \\frac{\\partial z_{i}^{(out)}}{\\partial z_{1}^{(h)}} = w_{1,i}^{(out)}\\sigma'(z_{1}^{(h)}) = w_{1,i}^{(out)}\\sigma(z_{1}^{(h)})(1-\\sigma(z_{1}^{(h)}))$\n",
        "</center>\n",
        "<br>"
      ],
      "metadata": {
        "id": "ltq1rGeyCSah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this to calculate $\\delta_{1}^{(h)}$, and noting that $\\displaystyle \\delta_{i}^{(out)} = \\frac{\\partial J}{\\partial z_{i}^{(out)}}$, we get:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle \\delta_{1}^{(h)} = \\sum_{i = 1}^{t}\\delta_{i}^{(out)}w_{1,i}^{(out)}\\sigma(z_{1}^{(h)})(1-\\sigma(z_{1}^{(h)}))$\n",
        "</center>\n",
        "<br>"
      ],
      "metadata": {
        "id": "utqO-LwICoBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "  $\\displaystyle \\delta^{(h)} = \\left(\\begin{array}{cccc} \\delta_{1}^{(h)} & \\delta_{2}^{(h)} & \\cdots && \\delta_{d}^{(h)}\\end{array}\\right)$\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "we have more generally that\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle \\delta^{(h)} = \\delta^{(out)}(\\textbf{W}^{(out)})^{T} \\odot (a^{(h)} \\odot (1-a^{(h)}))$,\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "where $\\odot$ is element-wise multiplication. In an identical fashion we can get:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle \\delta^{(in)} = \\delta^{(h)}(\\textbf{W}^{(h)})^{T} \\odot (a^{(in)} \\odot (1-a^{(in)}))$.\n",
        "</center>\n",
        "<br>"
      ],
      "metadata": {
        "id": "fu10OQhOCy7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This idea of propagating these $\\delta$ terms, as mentioned earlier known as *error* terms, backwards through the network is the reason for the term backpropagation. Once we have these taking the partial derivatives is easy. For any weight in $\\textbf{W}^{(out)}$ (the weights connecting the hidden layer and the output layer) we have:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle \\frac{\\partial J}{\\partial w_{i,j}^{(out)}} = a_{j}^{(h)}\\delta_{i}^{(out)}$,\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "and for any weight in $\\textbf{W}^{(h)}$ (the weights connecting the input layer and the hidden layer) we have:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "    $\\displaystyle \\frac{\\partial J}{\\partial w_{i,j}^{(h)}} = a_{j}^{(in)}\\delta_{i}^{(h)}$.\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Using these, we can calculate our updates for gradient descent."
      ],
      "metadata": {
        "id": "bAHdRWV8C5WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note throughout we've assumed we're doing stochastic gradient descent - updating just based upon one observation. This is what we'll do in our program too, but to do this for batch stochastic gradient descent, or just standard gradient descent, the math is almost identical - just with even more summations and indices to keep straight!"
      ],
      "metadata": {
        "id": "y3E4HE5BDMs1"
      }
    }
  ]
}