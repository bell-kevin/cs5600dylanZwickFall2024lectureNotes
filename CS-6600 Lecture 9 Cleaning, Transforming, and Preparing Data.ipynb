{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-5600/6600 Lecture 9 - Cleaning, Transforming, and Preparing Data\n",
        "\n",
        "**Instructor: Dylan Zwick**\n",
        "\n",
        "*Weber State University*\n",
        "\n",
        "Reference: [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) by Aurélien Géron - [End-to-end Machine Learning Project](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb)"
      ],
      "metadata": {
        "id": "0iETW1Mm9rv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we'll look at methods for cleaning, transforming, and preparing data. We'll also look a bit into hyperparameter optimization."
      ],
      "metadata": {
        "id": "i9ZNQtvw-qQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer\n",
        "\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
        "\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score"
      ],
      "metadata": {
        "id": "nQX6VsbVA1nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {
        "id": "rY5ZBtNC50-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data that we'll be using for this lecture is a California housing dataset. The data consists of publicly available information for \"block groups\" from the 1990 California census. So, it's old data (don't get excited by the amazing low prices), but we can still analyze it."
      ],
      "metadata": {
        "id": "8sC86G1mi48_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's saved as a .csv file on my Google Drive, and we can import it as a Pandas dataframe with the code below."
      ],
      "metadata": {
        "id": "9G8AYuuvjdkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the California housing data\n",
        "url = 'https://drive.google.com/uc?export=download&id=1V1Dw4dg7HYpPa7x2MzcUvzNb4ixVgxeL'\n",
        "housing = pd.read_csv(url)\n",
        "housing.head()"
      ],
      "metadata": {
        "id": "chw-o_SbAlx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, for each housing block group we know:\n",
        "* The longitude and latitude\n",
        "* The median age of the houses\n",
        "* The total number of rooms (note this is the total number of rooms in the block group)\n",
        "* The total number of bedrooms\n",
        "* The total number of households\n",
        "* The \"median income\" of those who live there.\n",
        "* The median house value (what we're trying to predict)\n",
        "* A categorical ocean_proximity"
      ],
      "metadata": {
        "id": "i6dTsRKojoGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can get some more info about our data:"
      ],
      "metadata": {
        "id": "TkdXiqX0kIhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing.info()"
      ],
      "metadata": {
        "id": "WTqtKV52AkDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the only feature for which we have missing data is total_bedrooms. We'll return to what we can do about this soon."
      ],
      "metadata": {
        "id": "3ezt-NT9kPuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's take a look at that categorical feature. How many categories are there, and what type of counts do we get for them?"
      ],
      "metadata": {
        "id": "EgRUkVXdkY0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing[\"ocean_proximity\"].value_counts()"
      ],
      "metadata": {
        "id": "5nVbmuJ2BEFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK. Let's check out some  descriptive stats on our numeric features:"
      ],
      "metadata": {
        "id": "d89FycEMkoWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing.describe()"
      ],
      "metadata": {
        "id": "F0kaowcaBO_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the median income isn't in dollars. It also looks like it might be capped at 15. Maybe we can get an even better idea of how these features are distributed through a histogram:"
      ],
      "metadata": {
        "id": "UnkESTEokyyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing.hist(bins=50, figsize=(12, 8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nDV8hoNaBTrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like there's a maximum cutoff for median income, median age, and median house value. This is something that might be a problem, or it might not be, depending on what we want to do with these predictions. We'll pretend here that it is not."
      ],
      "metadata": {
        "id": "JWzWW-vDlC5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, before we go any further, let's set aside a test set."
      ],
      "metadata": {
        "id": "FR-FIk81lRP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1f5eGQicCCO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Transformation"
      ],
      "metadata": {
        "id": "zLxH0lZu58sL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To figure out what variables might be useful for our model, we can first check out the correlations between each variable and the median house value."
      ],
      "metadata": {
        "id": "PM2d6vwzlU7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = housing.corr(numeric_only=True)\n",
        "\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "h1N2-9huCbiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like median income will be important. Not surprising.\n",
        "\n",
        "If we want a more granular look at how some of the variables correlate with median house price, we can use plot the scatter matrix:"
      ],
      "metadata": {
        "id": "bmXb-DSJlgIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
        "              \"housing_median_age\"]\n",
        "pd.plotting.scatter_matrix(housing[attributes], figsize=(12, 8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pndg7xg7EnzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the total number of rooms, bedrooms, or even people in a block group isn't really that interesting to us, given the number of households within a block group can vary significantly. What would probably be more valuable for our analysis is the number of rooms per house, the percentage of rooms that are bedrooms, and the average household size. We can derive these for each block group:"
      ],
      "metadata": {
        "id": "xdBwUZPgo0h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
        "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
        "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
      ],
      "metadata": {
        "id": "dwxDwNssE-Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking whether these derives values correlate with the median house value:"
      ],
      "metadata": {
        "id": "Va1vOqpMpMi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = housing.corr(numeric_only=True)\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "oMcAOJQ2FAbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break our data into two sets - the predictors and what we want to predict."
      ],
      "metadata": {
        "id": "_vALXwtFy-cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing = train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = train_set[\"median_house_value\"].copy()"
      ],
      "metadata": {
        "id": "jMDDXqRoFI4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, what can we do about those missing *total_bedroom* values?\n",
        "\n",
        "Well, we could drop the rows with missing values. Or, we could just drop the column.\n",
        "\n",
        "Or, we could *impute* those values with something that makes sense. Let's do that."
      ],
      "metadata": {
        "id": "bB38GjezzIpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_fillna = housing.copy()\n",
        "\n",
        "median = housing[\"total_bedrooms\"].median()\n",
        "housing_fillna[\"total_bedrooms\"].fillna(median, inplace=True)"
      ],
      "metadata": {
        "id": "Kkp6DHXzFY96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a fairly unsophisticated way to do it. We can also use a class called an *imputer*."
      ],
      "metadata": {
        "id": "DjqXbSSe2iYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy=\"median\")"
      ],
      "metadata": {
        "id": "pATywzQ-F2CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the imputer to calculate the median for all numeric columns."
      ],
      "metadata": {
        "id": "kX60qdqU3O0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_num = housing.select_dtypes(include=[np.number]) #First pick out the numeric columns."
      ],
      "metadata": {
        "id": "wBx7y07JGFbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.fit(housing_num); #Then calculate the medians."
      ],
      "metadata": {
        "id": "6vWh9ILFGPNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.statistics_ #Here are the medians."
      ],
      "metadata": {
        "id": "Rh0r2UnlGYJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_num.median().values #We can check."
      ],
      "metadata": {
        "id": "2uGz9QFeGZHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll use this imputer to fill in all our missing values."
      ],
      "metadata": {
        "id": "3TlH4okZ3g-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = imputer.transform(housing_num)"
      ],
      "metadata": {
        "id": "SDefDjcfGb1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the imputer comes with the names of all its columns."
      ],
      "metadata": {
        "id": "ASct5gfx3qRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.feature_names_in_"
      ],
      "metadata": {
        "id": "zx22ryscGg2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, what about that categorical variable?"
      ],
      "metadata": {
        "id": "tBohMwVG4on9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.head(8)"
      ],
      "metadata": {
        "id": "wC6cdZJUHBZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make it numeric with an *ordinal encoder*"
      ],
      "metadata": {
        "id": "vMTI-VP04tDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
      ],
      "metadata": {
        "id": "Qh6zIBQEHfG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_cat_encoded[:8]"
      ],
      "metadata": {
        "id": "nnH5QyqxJn0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_encoder.categories_"
      ],
      "metadata": {
        "id": "C_Nxye97Jrel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this suggests proximity relations among the categories that don't make sense. We can instead use a \"one hot\" encoder:"
      ],
      "metadata": {
        "id": "zA4r1-aW4yZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
      ],
      "metadata": {
        "id": "h1_n4gKjJuFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_cat_1hot"
      ],
      "metadata": {
        "id": "iLwacHdmJ5e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually one hot encodings are stored as a sparse matrix, which is sensible. However, for now we'll use a dense matrix."
      ],
      "metadata": {
        "id": "NHNDlXRx5PQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder = OneHotEncoder(sparse_output=False)\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ],
      "metadata": {
        "id": "OsW7vDBQKA_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder.categories_"
      ],
      "metadata": {
        "id": "1xkym5mEKUOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note we want to do this because we want to remember exactly what our training categories were."
      ],
      "metadata": {
        "id": "UWvV06hj5a4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\n",
        "pd.get_dummies(df_test)"
      ],
      "metadata": {
        "id": "vb_gZUYNKYf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder.transform(df_test)"
      ],
      "metadata": {
        "id": "Z2INaribKnsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n",
        "pd.get_dummies(df_test_unknown)"
      ],
      "metadata": {
        "id": "OpSLmu-TKtaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder.handle_unknown = \"ignore\"\n",
        "cat_encoder.transform(df_test_unknown)"
      ],
      "metadata": {
        "id": "O-V2w6BPK1vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scaling"
      ],
      "metadata": {
        "id": "M6uGjXuJ6Elw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's scale our data so everything has, more or less, the same range. We can do this with a simple MinMaxScaler:"
      ],
      "metadata": {
        "id": "LiQivD4t5hyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
      ],
      "metadata": {
        "id": "ebWdDhLAK59S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But it probably makes sense to use a standard scalar, which shifts everything by the average and divides everything by the standard deviation."
      ],
      "metadata": {
        "id": "QqHOKhDN6RMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "std_scaler = StandardScaler()\n",
        "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
      ],
      "metadata": {
        "id": "aioEpRp5Qpmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our population seems to follow a power law distribution:"
      ],
      "metadata": {
        "id": "G137SQZw6YTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
        "housing[\"population\"].hist(ax=axs[0], bins=50)\n",
        "housing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
        "axs[0].set_xlabel(\"Population\")\n",
        "axs[1].set_xlabel(\"Log of population\")\n",
        "axs[0].set_ylabel(\"Number of districts\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "An1UgQ1AQzbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get a more useful dataset if we take its logarithm."
      ],
      "metadata": {
        "id": "AXrl7h4_6du_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipelines"
      ],
      "metadata": {
        "id": "7gAzRPEI6ljj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a pipeline that combines these cleaning and scaling operations."
      ],
      "metadata": {
        "id": "wjt_tI146ndL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
        "log_pop = log_transformer.transform(housing[[\"population\"]])"
      ],
      "metadata": {
        "id": "OFE2UTFxRA0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipeline = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"standardize\", StandardScaler()),\n",
        "])"
      ],
      "metadata": {
        "id": "HK6c1lAgRlQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
      ],
      "metadata": {
        "id": "nZgEPm6aTSRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
        "housing_num_prepared[:2].round(2)"
      ],
      "metadata": {
        "id": "h-ErE5VLTV7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing_num_prepared = pd.DataFrame(\n",
        "    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n",
        "    index=housing_num.index)"
      ],
      "metadata": {
        "id": "AiIWS87BTwnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's combine the numeric and categorical transformations."
      ],
      "metadata": {
        "id": "-drGJ8Db6yoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
        "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "cat_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"most_frequent\"),\n",
        "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "\n",
        "preprocessing = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_attribs),\n",
        "    (\"cat\", cat_pipeline, cat_attribs),\n",
        "])"
      ],
      "metadata": {
        "id": "3SLYQK0rT7Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing = make_column_transformer(\n",
        "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
        "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
        ")"
      ],
      "metadata": {
        "id": "EfTeEHhbUfNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_prepared = preprocessing.fit_transform(housing)"
      ],
      "metadata": {
        "id": "_qngyJD2VQJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can combine all our transformations into one pipeline:"
      ],
      "metadata": {
        "id": "Xqloyhhm7WEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def column_ratio(X):\n",
        "    return X[:, [0]] / X[:, [1]]\n",
        "\n",
        "def ratio_name(function_transformer, feature_names_in):\n",
        "    return [\"ratio\"]  # feature names out\n",
        "\n",
        "def ratio_pipeline():\n",
        "    return make_pipeline(\n",
        "        SimpleImputer(strategy=\"median\"),\n",
        "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
        "        StandardScaler())\n",
        "\n",
        "log_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\"),\n",
        "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
        "    StandardScaler())\n",
        "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
        "                                     StandardScaler())\n",
        "default_location_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"))\n",
        "preprocessing = ColumnTransformer([\n",
        "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
        "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
        "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
        "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
        "                               \"households\", \"median_income\"]),\n",
        "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
        "        (\"location\", default_location_pipeline, [\"longitude\", \"latitude\"]),\n",
        "    ],\n",
        "    remainder=default_num_pipeline)"
      ],
      "metadata": {
        "id": "04RJxbsdVU1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying this transformation to our data:"
      ],
      "metadata": {
        "id": "AgmuAW4X9hFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_prepared = preprocessing.fit_transform(housing)\n",
        "housing_prepared.shape"
      ],
      "metadata": {
        "id": "DNuZf6MwWPGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are our feature names."
      ],
      "metadata": {
        "id": "xbIqS-3H9lrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing.get_feature_names_out()"
      ],
      "metadata": {
        "id": "PgRJs_CzWUP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Our Model"
      ],
      "metadata": {
        "id": "E-BlbwR-9pyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a decision tree."
      ],
      "metadata": {
        "id": "r_t5mCTm9uT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
        "tree_reg.fit(housing, housing_labels);"
      ],
      "metadata": {
        "id": "E-dHAo9WWeWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_predictions = tree_reg.predict(housing)\n",
        "tree_rmse = root_mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse"
      ],
      "metadata": {
        "id": "KCamiN0zcIlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nailed it!\n",
        "\n",
        "Clearly, we've overfit. How can we get around that? Cross-validation!"
      ],
      "metadata": {
        "id": "Kek8l8taFzJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)"
      ],
      "metadata": {
        "id": "HKVLkhPLcX4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(tree_rmses).describe()"
      ],
      "metadata": {
        "id": "m1U0xjhRcsIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a random forest model instead."
      ],
      "metadata": {
        "id": "7WGJTKegF9F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_reg = make_pipeline(preprocessing,\n",
        "                           RandomForestRegressor(random_state=42))\n",
        "forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n",
        "                                scoring=\"neg_root_mean_squared_error\", cv=10)"
      ],
      "metadata": {
        "id": "GdnJWRuTczvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(forest_rmses).describe()"
      ],
      "metadata": {
        "id": "ADWZZv-pc6ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_reg.fit(housing, housing_labels)\n",
        "housing_predictions = forest_reg.predict(housing)\n",
        "forest_rmse = root_mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse"
      ],
      "metadata": {
        "id": "19aV2ftPdDsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better!\n",
        "\n",
        "Let's see if we can find the optimal number of features for the random forest to consider."
      ],
      "metadata": {
        "id": "VZSaqv8XGBT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessing),\n",
        "    (\"random_forest\", RandomForestRegressor(random_state=42)),\n",
        "])\n",
        "param_grid = [\n",
        "    {'random_forest__max_features': [4, 6, 8, 10]}\n",
        "]\n",
        "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
        "                           scoring='neg_root_mean_squared_error')\n",
        "grid_search.fit(housing, housing_labels);"
      ],
      "metadata": {
        "id": "ilvUlYzpdOe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "PQr5I16cfVE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
        "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
        "cv_res.head()"
      ],
      "metadata": {
        "id": "OoCOi81cdT8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}