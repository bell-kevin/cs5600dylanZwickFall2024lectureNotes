{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-6600 Lecture 2 - The Tools of the Trade\n",
        "\n",
        "**Instructor: Dylan Zwick**\n",
        "\n",
        "*Weber State University*\n",
        "\n",
        "References:\n",
        "* [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) by Aurélien Géron - [Chapter 2: End-to-End Machine Learning Project](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb)\n",
        "\n",
        "* [Python for Data Analysis](https://wesmckinney.com/book/) by Wes McKinney"
      ],
      "metadata": {
        "id": "SI2EVDOR9tCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://imgs.xkcd.com/comics/python_environment.png\" alt=\"Python Environment\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "ZaWU1HnrN5SS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the programming work we do in this class will be in Python, and much of the work will be done in Jupyter notebooks. A Jupyter notebook is a powerful tool for interactively developing and presenting data science projects.\n",
        "\n",
        "There are many options for running Jupyter notebooks. There are cloud-based options like:\n",
        "\n",
        "*   [Google Collab](https://colab.google/)\n",
        "*   [Anaconda Cloud](https://anaconda.cloud/)\n",
        "\n",
        "You can also download and run [Anaconda](https://www.anaconda.com/) locally on your computer. Please note that Anaconda is big (on the order of gigabytes), and can take a little while (but not too long) to install. If you're running locally, once you have Anaconda installed, you should open Anaconda Navigator, and launch Jupyter Notebook.\n",
        "\n",
        "For this class I'll be using Google Collab, and that's what I'd encourage you to use as well, although it's not strictly required. If you're going to set up a Google Collab or other cloud account, please do so with your Weber State email. There may be a fee involved you'll need to pay. It's cheaper than textbooks - which are available online for free.\n",
        "\n",
        "I must admit I do feel a bit silly writing this, as the very fact that you're reading it means you've probably got this part figured out.\n"
      ],
      "metadata": {
        "id": "gfBRBBc8-0Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_What is a Jupyter notebook_?\n",
        "\n",
        "A notebook integrates code and its output into a single document that combines visualizations, narrative text, mathematical equations, and other rich media (like comics!). It's a single document where you can run code, display the output, and add explanations.\n",
        "\n",
        "Note that it is possible to use many different programming languages in a Jupyter Notebook. However, Jupyter was built with Python in mind (it's the \"Py\" in \"JuPYter\", and Python is the most common language used in Jupyter notebooks. It's what we'll be using in this class."
      ],
      "metadata": {
        "id": "8CqzwLWhA__Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Notebook Interface"
      ],
      "metadata": {
        "id": "Xbkkwc4yBkVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have an open notebook in front of you, its interface will hopefully not look entirely alien. After all, Jupyter is kind of like an advanced word processor. Take a look around."
      ],
      "metadata": {
        "id": "aLDDyFAkBnUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Had a chance to look around? Good. Now, let's talk a bit about some important terms for Jupyter Notebooks.\n",
        "\n",
        "First, there are two fairly prominent terms which you'll need to know: _cells_ and _kernels_.\n",
        "\n",
        "* A _cell_ is a container for text to be displayed in the notebook or code to be executed by the notebook’s kernel.\n",
        "\n",
        "* A _kernel_ is a “computational engine” that executes the code contained in a notebook document."
      ],
      "metadata": {
        "id": "w2pL8y1eBxPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cells**\n",
        "\n",
        "We’ll return to kernels a little later, but first let's talk about cells. Cells form the body of a notebook. There are two main cell types:\n",
        "\n",
        "* A _code cell_ contains code to be executed in the kernel. When the code is run, the notebook displays the output below the code cell that generated it.\n",
        "\n",
        "* A _Markdown cell_ contains text formatted using Markdown and displays its output in-place when the Markdown cell is run.\n",
        "\n",
        "The default first cell in a new notebook is always a code cell."
      ],
      "metadata": {
        "id": "f5m2ZJHIRtBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s test out a classic hello world example: Type _print('Hello World!')_ into the cell below and then press _Ctrl + Enter_."
      ],
      "metadata": {
        "id": "QmLu6Ii8CTS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Hello World!')"
      ],
      "metadata": {
        "id": "pfM04Q6mCn2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we run the cell, its output is displayed below it and the label to its left will have changed from In [ ] to In [1].\n",
        "\n",
        "The output of a code cell also forms part of the document, which is why you can see it after you run the cell. You can always tell the difference between code and Markdown cells because code cells have that label on the left and Markdown cells do not.\n",
        "\n",
        "The label number indicates when the cell was executed on the kernel — in this case the cell was executed first.\n",
        "\n",
        "Run the cell again and the label will change to a higher number. It will become clearer why this is useful later on when we take a closer look at kernels.\n",
        "\n",
        "Now, try running the cell below."
      ],
      "metadata": {
        "id": "KaUGpBsaC2kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "Y7hzvfSgG6ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whoops! ERROR.\n",
        "\n",
        "What happened? Well, we're calling a function from the *time* library, but in order to do this, we need to import the library. Let's do that and try again."
      ],
      "metadata": {
        "id": "USQwy0dGG_Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "IxXhsgrgGv-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's better. Note that for this lecture we'll be importing libraries inline as we need them, but that won't always be the case. Usually, instead of importing libraries \"in line\", I'll import all the libraries we need right at the top. Perhaps this is just because I'm old and was trained writing C programs where you need to specify all this up front, but I prefer doing this because I find it makes it easier to debug. You don't need to go hunting through your notebook trying to find if and where you've imported a given library. However, this is just my style, and yours may differ."
      ],
      "metadata": {
        "id": "VNqFK6ORHMFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you notice anything different? (Besides that it ran.)\n",
        "\n",
        "This cell doesn’t produce any output, but it does take three seconds to execute. Notice how Jupyter signifies when the cell is currently running by changing its label to In [*].\n",
        "\n",
        "In general, the output of a cell comes from any text data specifically printed during the cell’s execution, as well as the value of the last line in the cell, be it a lone variable, a function call, or something else."
      ],
      "metadata": {
        "id": "25iU7WnwG0Qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keyboard Shortcuts**\n",
        "\n",
        "One thing you may have already observed is that some cells have a white background, while others have a gray background without these options. A white background means it's for text, while a gray background means it's for code. There is always one “active” cell highlighted within a notebook. Active cells can also be in either \"command\" mode or \"edit\" mode.\n",
        "\n",
        "How can we create or modify cells? Well, keyboard shortcuts are a very popular feature of the Jupyter environment because they facilitate a speedy cell-based workflow. Many of these are actions you can carry out on the active cell when it’s in command mode.\n",
        "\n",
        "Below, you’ll find a list of some of Jupyter’s keyboard shortcuts. You don’t need to memorize them all immediately (or ever), but this list should give you a good idea of what’s possible, and you'll find you quickly memorize the ones you frequently need.\n",
        "\n",
        "* Toggle between edit and command mode with Esc and Enter, respectively.\n",
        "\n",
        "* Once in command mode:\n",
        "    * Scroll up and down your cells with your Up and Down keys.\n",
        "    * Press A or B to insert a new cell above or below the active cell.\n",
        "    * Ctrl-M + M will transform an active code cell to a Markdown cell.\n",
        "    * Ctrl-M + Y will transform an active Markdown cell to a code cell.\n",
        "    * Ctrl-M + D will delete the active cell.\n",
        "    * Ctrl-M + Z will undo cell deletion.\n",
        "\n",
        "Go ahead and give some of these a try.\n"
      ],
      "metadata": {
        "id": "caCS-MV8HwbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to practice deleting and undoing deletion, you can use this cell."
      ],
      "metadata": {
        "id": "X3mUzES-KhlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown**\n",
        "\n",
        "[Markdown](https://www.markdownguide.org/) is a lightweight, easy to learn markup language for formatting plain text. Its syntax has a one-to-one correspondence with HTML tags, so some prior knowledge of HTML would be helpful but is definitely not a prerequisite.\n",
        "\n",
        "All the narrative text above was written in Markdown. Let’s cover the basics with a quick example:\n",
        "````\n",
        "# This is a level 1 heading\n",
        "\n",
        "## This is a level 2 heading\n",
        "\n",
        "This is some plain text that forms a paragraph. Add emphasis via **bold** and __bold__, or *italic* and _italic_.\n",
        "\n",
        "Paragraphs must be separated by an empty line.\n",
        "\n",
        "* Sometimes we want to include lists.\n",
        "* Which can be bulleted using asterisks.\n",
        "\n",
        "1. Lists can also be numbered.\n",
        "2. If we want an ordered list.\n",
        "\n",
        "[It is possible to include hyperlinks](https://www.example.com)\n",
        "\n",
        "Inline code uses single backticks: `foo()`, and code blocks use triple backticks:\n",
        "```\n",
        "bar()\n",
        "```\n",
        "Or can be indented by 4 spaces:\n",
        "\n",
        "    foo()\n",
        "\n",
        "And finally, adding images is easy: ![This is alt text](https://imgs.xkcd.com/comics/selection_bias.png)\n",
        "````\n",
        "\n",
        "Below is the text above rendered with Markdown:"
      ],
      "metadata": {
        "id": "VssT-C9IKoe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is a level 1 heading\n",
        "\n",
        "## This is a level 2 heading\n",
        "\n",
        "This is some plain text that forms a paragraph. Add emphasis via **bold** and __bold__, or *italic* and _italic_.\n",
        "\n",
        "Paragraphs must be separated by an empty line.\n",
        "\n",
        "* Sometimes we want to include lists.\n",
        "* Which can be bulleted using asterisks.\n",
        "\n",
        "1. Lists can also be numbered.\n",
        "2. If we want an ordered list.\n",
        "\n",
        "[It is possible to include hyperlinks](https://www.example.com)\n",
        "\n",
        "Inline code uses single backticks: `foo()`, and code blocks use triple backticks:\n",
        "```\n",
        "bar()\n",
        "```\n",
        "Or can be indented by 4 spaces:\n",
        "\n",
        "    foo()\n",
        "\n",
        "And finally, adding images is easy: ![This is alt text](https://imgs.xkcd.com/comics/selection_bias.png)"
      ],
      "metadata": {
        "id": "ypuDew7wK9Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note the image above comes from the [\"xkcd\" comic](https://xkcd.com/). I'll be using images from xkcd frequently in this class."
      ],
      "metadata": {
        "id": "DWk6M87eMuNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernels**\n",
        "\n",
        "Behind every notebook runs a kernel. When you run a code cell, that code is executed within the kernel. Any output is returned back to the cell to be displayed. The kernel’s state persists over time and between cells — it pertains to the document as a whole and not individual cells.\n",
        "\n",
        "For example, if you import libraries or declare variables in one cell, they will be available in another. Let’s try this out to get a feel for it. First, we’ll import a Python package and define a function:"
      ],
      "metadata": {
        "id": "8ajlo2-lNAGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def square(x):\n",
        "    return x * x"
      ],
      "metadata": {
        "id": "EMRmpc2RNTw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we’ve executed the cell above, we can reference np and square in any other cell."
      ],
      "metadata": {
        "id": "khLj7OGGNaHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randint(1, 10)\n",
        "y = square(x)\n",
        "print('%d squared is %d' % (x, y))"
      ],
      "metadata": {
        "id": "ce_8Dm_YNdJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will work regardless of the order of the cells in your notebook. As long as a cell has been run, any variables you declared or libraries you imported will be available in other cells.\n",
        "\n",
        "For example, remember that *time* command that didn't work the first time we ran it? Scroll up and try running the cell again. It should run now, because you've imported the required library.\n",
        "\n",
        "Most of the time when you create a notebook, the flow will be top-to-bottom. But it’s common to go back to make changes. When we do need to make changes to an earlier cell, the order of execution we can see on the left of each cell can help us diagnose problems by seeing the order in which the cells have run. You see why they're numbered? I told you it would be explained later. :)\n",
        "\n",
        "But what happens if we change the value of y specified in the code cell right above?"
      ],
      "metadata": {
        "id": "q5BShsc0NgiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = 10\n",
        "print('Is %d squared %d?' % (x, y))"
      ],
      "metadata": {
        "id": "pSWSbBVMN83a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No. No it's not. We get this output because once we’ve run the y = 10 code cell, y is no longer equal to the square of x in the kernel."
      ],
      "metadata": {
        "id": "67Ixw16nOAo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if we ever wish to reset things, there are several incredibly useful options from the Runtime menu, including:\n",
        "\n",
        "* Restart session: restarts the kernel, thus clearing all the variables etc that were defined.\n",
        "* Restart session and run all: same as above but will also run all your cells in order from first to last.\n",
        "\n",
        "If your kernel is ever stuck on a computation and you wish to stop it, you can choose the *Interrupt execution* option."
      ],
      "metadata": {
        "id": "PUTiZLxyOIwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing a Runtime**\n",
        "\n",
        "When you're running a Jupyter notebook in Collab, you've got a few options that determine your *runtime*. One, is you need to pick which language you're using - for us it will almost always be Python 3. We also need to pick which hardware we're using. This won't be important at the start, but later on we'll definitely want to be using GPUs.\n",
        "\n",
        "There are kernels for different versions of Python, and also for over 100 languages including Java, C, and even Fortran. Data scientists may be particularly interested in the kernels for R and Julia, as well as both imatlab and the Calysto MATLAB Kernel for Matlab."
      ],
      "metadata": {
        "id": "NDcV0E4iO7rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Numpy Library"
      ],
      "metadata": {
        "id": "7iFT6M6UPwMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go over the basics of NumPy, which is short for \"Numerical Python\". It's one of the great data science workhorse libraries of Python, and we'll be using it all the time. We'll just be scratching the surface of its features and capabilities today.\n",
        "\n",
        "The basic object of interest in NumPy is the array, or the *ndarray*, which is an efficient, multidimensional array optimized for storage and computation. NumPy also comes with tools for reading/writing array data to disk and some nice built-in math functions, plus tools for integrating with C libraries.\n",
        "\n",
        "The basic idea behind NumPy is that, while Python is great for doing many things, it's not an inherently optimized language for handling large-scale numeric computations or storing large data files. Instead of requiring that data analysts needing to work with such things do so using a different language, NumPy provides tools that, essentially, translate basic numeric analysis needs into much more efficient, lower-level (C-style) implementations.\n",
        "\n",
        "To give you an idea of the performance difference, check out the following:"
      ],
      "metadata": {
        "id": "fj18kjokPxtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Numpy is almost always abbreviated as np"
      ],
      "metadata": {
        "id": "i1Yl92lhQYQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_arr = np.arange(1000000)\n",
        "my_list = list(range(1000000))"
      ],
      "metadata": {
        "id": "467totbRQc3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit my_arr2 = my_arr*2"
      ],
      "metadata": {
        "id": "MEwlxyE0QfXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit my_list2 = [x * 2 for x in my_list]"
      ],
      "metadata": {
        "id": "j3kB_AH8QlZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's quite the difference! Generally, NumPy-based algorithms are 10 to 100 times faster (or more!) than their pure Python counterparts and use significantly less memory. So, if you're writing for loops to go through lists for large numeric computations - you're probably doing it wrong."
      ],
      "metadata": {
        "id": "eSnvqIwXQrGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The NumPy ndarray: A Multidimensional Array Object"
      ],
      "metadata": {
        "id": "MGpj-EdnQzVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic data object in NumPy is the *ndarray*, and applying standard arithmetic operations to these arrays uses syntax very similar to standard arithmetic.\n",
        "\n",
        "For example, let's create an array:"
      ],
      "metadata": {
        "id": "4bRF4NtWQ2Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([[1, 4.2, 7], [5, 8, 2.71]])\n",
        "data"
      ],
      "metadata": {
        "id": "7G89mbGoQ9MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can multiply every element in the array by 2 with the following:"
      ],
      "metadata": {
        "id": "DCazFSzwRDlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data * 2"
      ],
      "metadata": {
        "id": "T3H6aNOeRHy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, we can get the same result by adding the array to itself:\n"
      ],
      "metadata": {
        "id": "EEVf2GKZRKyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data + data"
      ],
      "metadata": {
        "id": "Gm45AuFLRPRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could even square every element in the array:"
      ],
      "metadata": {
        "id": "gioqE0VORSpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data ** 2"
      ],
      "metadata": {
        "id": "O0_NGyzMRV8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, these operations are generally quite fast relative to trying to implement them with a for loop."
      ],
      "metadata": {
        "id": "IUlmibMyRYMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Types for ndarrays"
      ],
      "metadata": {
        "id": "A4umsyWwRda9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b5d47bf-c270-4231-9686-dbd363394e4d"
      },
      "source": [
        "In general, an array is  for homogeneous data; in other words, all the elements must be the same type. Every array also has a *shape*, which is a tuple indicating the size of each dimension, and a *dtype*, which describes the data type of the array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57853a26-c98c-469f-b739-11f2f97a4067"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6394533-8b13-4c6c-a0a5-025f995e58a0"
      },
      "outputs": [],
      "source": [
        "data.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282080f8-24f6-4c39-8a52-964d129a95d1"
      },
      "source": [
        "How do we create ndarrays? The easiest way is with the *array* function used above, where we just specify the elements in a list or nested lists. Unless explicitly told otherwise, the array function will try to infer a good data type for the array based upon its inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b71ad7e-f789-4231-8669-e48012fd3f0f"
      },
      "source": [
        "In addition to the *array* function, there are a number of other functions for creating new arrays. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2635f7e7-5849-4625-97f1-13dc2d23ecc8"
      },
      "outputs": [],
      "source": [
        "np.zeros(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0805202e-27f7-4f57-af4c-2ce4c4ec7317"
      },
      "outputs": [],
      "source": [
        "np.zeros((2,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b580bd6-986f-41ab-accb-b57d5ea74b06"
      },
      "outputs": [],
      "source": [
        "np.ones((4,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "000610f7-a2c3-46ec-b82c-4ddf876645e7"
      },
      "source": [
        "The function *arange* is like the *range* function is Python, except it creates a NumPy array instead of a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8ffbef2-e00f-4bfb-898a-1a9c5d77106c"
      },
      "outputs": [],
      "source": [
        "np.arange(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82ee025b-83df-429c-9ccf-ae11684ff80b"
      },
      "outputs": [],
      "source": [
        "np.arange(3,11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1c90d2e-a5a5-47a3-9e8c-5a5350811604"
      },
      "source": [
        "We can, if desired, explicitly convert or *cast* an array from one data type to another using the *astype* method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2bbc1c9-2b19-4152-8580-6c243236151b"
      },
      "outputs": [],
      "source": [
        "arr = np.array([1,2,3,4,5])\n",
        "arr.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31c39657-698f-465d-a6c3-b0aee763cb39"
      },
      "outputs": [],
      "source": [
        "float_arr = arr.astype(np.float64)\n",
        "float_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccbe5acb-29a7-4b9d-9a17-6761c1ee071d"
      },
      "source": [
        "If we go the other way, from floats to ints, that will not generate an error, but the decimal parts will be truncated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f847d79-f15f-4037-b59c-42b961d7a605"
      },
      "outputs": [],
      "source": [
        "arr = np.array([2.3, 5.0, 4.999, 2.71828])\n",
        "arr.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67bc07d8-9b20-47db-bb33-2ebcb49ef2f0"
      },
      "outputs": [],
      "source": [
        "int_arr = arr.astype(np.int64)\n",
        "int_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f48e414a-9553-42f1-9ac1-f9be5566b3bf"
      },
      "source": [
        "We can convert strings that make sense as numbers to numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2282c0d7-033c-4938-8caa-09d5b7f46bf7"
      },
      "outputs": [],
      "source": [
        "arr = np.array([\"2.3\", \"1\", \"42\"])\n",
        "arr.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4fc3c91-7e07-42d9-9685-beb33b0f65a5"
      },
      "outputs": [],
      "source": [
        "float_arr = arr.astype(np.float64)\n",
        "float_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61d6600b-02a4-410e-a8a3-05a6ef3be4f9"
      },
      "source": [
        "But, this will cause an error if the strings don't make sense as numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a66a607a-8526-4083-8fc1-fbc3f1445ba0"
      },
      "outputs": [],
      "source": [
        "arr = np.array([\"2.3\", \"1\", \"42\", \"Shrubbery\"])\n",
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bb43ef6-5794-46f6-b832-6ed221354063"
      },
      "outputs": [],
      "source": [
        "float_arr = arr.astype(np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59a7f16c-d767-4d66-8469-4ffe73824930"
      },
      "source": [
        "The datatype for an array can also be specified when you create it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "279fd232-15bb-4633-9013-0082f6ad6b21"
      },
      "outputs": [],
      "source": [
        "arr = np.array([1, 2, 3])\n",
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aab5d99-4fe4-48b3-96db-2a6425d8a6b1"
      },
      "outputs": [],
      "source": [
        "arr = np.array([1, 2, 3], dtype=np.float64)\n",
        "arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b17ee15-a18b-49e2-a60d-37172d8576f7"
      },
      "source": [
        "#### Arithmetic with NumPy Arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "695a5f5f-5b4b-4969-9447-36cfac1b9edc"
      },
      "source": [
        "Arrays enable you to express batch operations on data without writing any for loops. In NumPy this is called \"vectorization\". Any arithmetic operations between equal-sized arrays apply the operation element-wise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35641203-4113-40c0-810f-a996917bcfb7"
      },
      "outputs": [],
      "source": [
        "arr = np.array([[1,2,3],[3,2,1]])\n",
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "529659b3-6101-4579-acfb-ae853e55ec4b"
      },
      "outputs": [],
      "source": [
        "arr * arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88a2f9ab-62db-4522-958f-ba16848db11e"
      },
      "outputs": [],
      "source": [
        "arr + 2 * arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ec15817-d96b-4185-9cdc-f5e2a8b9dcbf"
      },
      "source": [
        "#### Basic indexing and slicing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ed8e0d-5a26-4166-a638-b31ed42c1a76"
      },
      "source": [
        "Selecting a subset of an array is a somewhat deep topic that we'll only touch on here. One-dimensional arrays on the surface act similarly to Python lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "708f7999-7494-483f-9ab7-d891da9fde62"
      },
      "outputs": [],
      "source": [
        "arr = np.arange(10)\n",
        "arr[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51220396-940d-4e4a-a9dd-aedec1992d45"
      },
      "outputs": [],
      "source": [
        "arr[5:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ea7617-02e5-4891-bc25-0a71cf0f0692"
      },
      "outputs": [],
      "source": [
        "arr[5:8] = 12\n",
        "arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "248c7872-e887-4b33-98a4-89c1531528d9"
      },
      "source": [
        "As you can see, assigning a scalar value to a slice propagates (or *broadcasts*) the value to the entire selection.\n",
        "\n",
        "An important first  distinction here is that array slices are views **on the original array**, which means any modification to them is reflected in the source array.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0167721-e602-4d80-8652-b40353710a15"
      },
      "outputs": [],
      "source": [
        "arr_slice = arr[5:8]\n",
        "arr_slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94c037ba-eac9-4c1e-8bf4-08dc5b8b45e9"
      },
      "outputs": [],
      "source": [
        "arr_slice[1] = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ab9d235-6f60-4607-90f7-47f2695a2716"
      },
      "outputs": [],
      "source": [
        "arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec022c5c-7ee4-48f3-8d6a-f2b78cfa1aa3"
      },
      "source": [
        "This might seem surprising if you are used to Python lists. The idea behind this is that NumPy has been designed to work with very large arrays, and you can image that lots of copying of big arrays could lead to performance and memory problems.\n",
        "\n",
        "If you do want to create a copy and not just a view, you can do so with the *copy* function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5da4628-78f6-4d9f-97b9-00c784bf2e5e"
      },
      "outputs": [],
      "source": [
        "arr = np.arange(10)\n",
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acd668b3-9b7c-4e56-a8c9-b16178137681"
      },
      "outputs": [],
      "source": [
        "arr_slice = arr[3:6]\n",
        "arr_slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69f006cd-b0c3-42db-aba2-4402d00af62a"
      },
      "outputs": [],
      "source": [
        "arr_copy = arr[3:6].copy()\n",
        "arr_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d6c6940-fee3-4026-8655-1449995c3bdd"
      },
      "outputs": [],
      "source": [
        "arr_slice[0] = 23\n",
        "arr_copy[1] = 42\n",
        "arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95277c82-c1b9-449c-a641-4a5df857663a"
      },
      "source": [
        "For higher dimensional arrays there are more options.\n",
        "\n",
        "In a two-dimensional array, the elements at each index are no longer scalars but rather one-dimensional arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bb69b7f-24e8-4d7c-b943-8b267dbc9c4e"
      },
      "outputs": [],
      "source": [
        "arr2d = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "arr2d[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03bdb001-5774-4d94-a082-ab6a5b13f572"
      },
      "source": [
        "If we wanted to access the third element of the first array, we could do so either recursively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6d1654b-9793-4bc2-b5d8-1118383372cd"
      },
      "outputs": [],
      "source": [
        "arr2d[0][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3b1ef29-5457-413f-9221-1207dd2aff0d"
      },
      "source": [
        "Or, using a comma-separated list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "018b78a0-15d6-4445-9018-4b9586bf8158"
      },
      "outputs": [],
      "source": [
        "arr2d[0,2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f9fbea7-61e7-4b06-af9d-fa8c08862835"
      },
      "source": [
        "If we assign a scalar value to an entire array, it broadcasts that value to every entry:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c857385a-e2ca-4a66-963a-5744a41c9098"
      },
      "outputs": [],
      "source": [
        "arr2d[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2699ad6d-3d55-4add-97df-fc713998029e"
      },
      "outputs": [],
      "source": [
        "arr2d[0] = 42\n",
        "arr2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2010ce4e-8107-4521-b60b-6c180101d7fb"
      },
      "source": [
        "Multiple slices can be passed to multi-dimensional arrays just like we can pass multiple indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee2d35e6-7ebb-4efa-8609-7a4633e9e218"
      },
      "outputs": [],
      "source": [
        "arr2d[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "789cb154-9fe9-4d32-afe8-538065ba3010"
      },
      "outputs": [],
      "source": [
        "arr2d[:2,1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a512fce5-413e-4edf-91cb-f9ce6a43d654"
      },
      "source": [
        "By mixing indexes and slices, you can get lower dimensional slices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd64a815-1d9d-450f-ae96-645b2285e657"
      },
      "outputs": [],
      "source": [
        "arr2d[:2,2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcfaf026-5213-4249-a89a-a0f8bb341bda"
      },
      "source": [
        "Keep in mind, these slices are *views*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd04a425-3f12-497d-866e-fbe4550720cf"
      },
      "outputs": [],
      "source": [
        "arr2d[:2,2] = 801\n",
        "arr2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8d20e72-0f16-4f45-a6fe-fceb1084f6bb"
      },
      "source": [
        "We can index the elements in an array with **boolean indexing**. For example, suppose we have the following array of names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "485ec368-4519-4857-b886-60904743b349"
      },
      "outputs": [],
      "source": [
        "names = np.array([\"Bob\", \"Joe\", \"Will\", \"Bob\", \"Will\", \"Joe\", \"Joe\"])\n",
        "data = np.array([[4,7], [0,2], [-5,6], [0,0], [1,2], [-12,-4], [3,4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96b3a0e6-8ca3-43ac-9258-dde75ef79b8e"
      },
      "outputs": [],
      "source": [
        "names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eaf7a99-bae4-4328-a384-abb06dfb23e2"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53055f67-bb9f-4547-bae6-7d9d8fbb3853"
      },
      "source": [
        "Suppose each name corresponds with a row in the *data* array and we wanted to select all the rows with the corresponding name \"Bob\". Like arithmetic operations, comparisons (like ==) with arrays are also vectorized. So, this command produces a Boolean array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ef0d686-242e-41e6-82a3-d22a84bc400f"
      },
      "outputs": [],
      "source": [
        "names == \"Bob\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dee74e60-75a2-4756-817c-d3dde96e897b"
      },
      "source": [
        "If we pass this as an index to our array we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c6e69a2-4b2d-4bac-8235-e66b7ce16247"
      },
      "outputs": [],
      "source": [
        "data[names==\"Bob\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dc06d05-1f90-48d5-bef1-30f655b96c3a"
      },
      "source": [
        "Note of course the Boolean array must be of the same length as the array axis it's indexing.\n",
        "\n",
        "You can even mix and match Boolean arrays with slices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2f0316e-c75b-41c6-9554-ecad218e3395"
      },
      "outputs": [],
      "source": [
        "data[names == \"Bob\", 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe44408c-b8ad-4b6c-b508-cf861ede831c"
      },
      "outputs": [],
      "source": [
        "data[names == \"Bob\", 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76feae9c-e951-4cc6-8d23-170f524d0335"
      },
      "source": [
        "We can also use the standard *and* (&), *or* (|), and *not* (~) operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c4bda3a-b572-4008-b80a-d50652c7b124"
      },
      "outputs": [],
      "source": [
        "data[names != \"Bob\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40052817-0621-41a6-95f4-9f3ef134dc5b"
      },
      "outputs": [],
      "source": [
        "data[~(names == \"Bob\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fabe818-190c-47df-b7b7-c401e883b1e4"
      },
      "outputs": [],
      "source": [
        "data[(names == \"Bob\") | (names == \"Will\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546daba0-fbc4-40ba-aaea-6c4da1db2ce6"
      },
      "source": [
        "Note selecting data from an array by Boolean indexing and assigning the results to a new variable creates a *copy* of the data. I know it's a bit confusing when a copy is created and when it's not. Sorry, I didn't make the rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bedb4a2-69ee-49bc-8f90-7c729bee01ee"
      },
      "source": [
        "Fancy indexing is a term adopted by NumPy to describe indexing using integer arrays. Sorry, I didn't make this terminology either. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42aa594-6df3-4f54-bb8d-a3999f2341d2"
      },
      "outputs": [],
      "source": [
        "arr = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2176e4da-1111-4fdb-b4ff-6e895e0c26dd"
      },
      "outputs": [],
      "source": [
        "arr[[1,0,2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d91bfd32-d7d5-4fad-8824-cd054b7368cc"
      },
      "source": [
        "Using negative indices selects rows from the end:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e0ac038-73dc-4f49-aedc-5f2859f975f2"
      },
      "outputs": [],
      "source": [
        "arr[[-1,-2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecaade1c-ffc2-478a-8621-53a0e62437fa"
      },
      "source": [
        "Passing multiple index arrays selects an array of elements corresponding to each tuple of indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f21a41c3-1275-4c99-b42d-b254f015184b"
      },
      "outputs": [],
      "source": [
        "arr[[1,0,2],[2,0,1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c727d11f-8fdc-48c3-853d-34bb5e81420c"
      },
      "source": [
        "Fancy indexing, unlike slicing, always copies the data into a new array when assigning the results to a new variable. Again, not my rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "250e2ed3-a876-4833-abe8-a987388808b6"
      },
      "source": [
        "Finally, we can transpose an array with the *T* method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e55e5098-d9c5-49ce-abc2-4311ae07b9e0"
      },
      "outputs": [],
      "source": [
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad198571-46c9-4a29-bba6-4e274617f5c9"
      },
      "outputs": [],
      "source": [
        "arr.T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Pandas Library"
      ],
      "metadata": {
        "id": "jg6-yyiqS5wJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a078e87b-984f-41ae-ba17-cf9146f65200"
      },
      "source": [
        "The other great workhorse library for data science with Python is *pandas*. We will not cover, or even come close to covering, the entire library today. Also, there are many aspects and facets of Pandas that you'll learn and internalize only by using it. However, today should - ideally - give you a starting point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "832954c2-e03b-476c-9454-5ac5834ae8ab"
      },
      "source": [
        "First, some notational conventions. As with NumPy and \"np\", almost always the name of the library \"pandas\" is abbreviated as \"pd\" when it's imported, and pd is used to reference it afterwards."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Joke - How do you ruin a data scientist's day?\n",
        "\n",
        "```\n",
        "import numpy as pd\n",
        "import pandas as np\n",
        "```"
      ],
      "metadata": {
        "id": "S4IXAUJQT7YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do it right:"
      ],
      "metadata": {
        "id": "KL9qXf5FUL0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "P5QrBWjWUIsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7743bcd-2e67-45a0-bf29-4fcf98634178"
      },
      "source": [
        "### Pandas Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e04d40c-fc34-4bfe-8553-cd61ad64f8cf"
      },
      "source": [
        "The dataset with which we'll play around is the \"royal line\" dataset, which was created from public sources and contains family history information about Elizabeth II, the Queen of England at the time the dataset was compiled. Note a few things about the command below:\n",
        "\n",
        "* It uses the read_csv command from pandas, which is used to read in \"comma separated value\" files. This is a very common format for storing tabular data, as it's not tied to a particular program like, for example, Excel files are. However, Pandas also has functionality for reading in pretty much any type of data format commonly found in practice.\n",
        "\n",
        "* The basic data object in Pandas is a \"dataframe\", which is created by the read_csv command. Frequently, a dataframe is denoted with the abbreviation \"df\", which we do here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?export=download&id=1k7k-ObAKWhW0iPbyCplogGMGsy29Mif5' #This URL points to the royal_line.csv file stored on my (Dylan's) Google Drive. You should all be able to access it."
      ],
      "metadata": {
        "id": "h_v4pnx4ZWAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b654ef4-d06d-47ad-b29b-dba2d6ee568a"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ddf6766-0af1-4058-acd1-e16db226f438"
      },
      "source": [
        "We can then take a look at this dataframe using the \"print\" command, which will by default print the first five and last five rows of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b6e2f3e-0326-469d-9080-0ac13057e3a3"
      },
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "379d6271-ef99-4c6a-ba8d-64383f2242d8"
      },
      "source": [
        "If we just want to check out the first $n$ values, we can use the \"head\" function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5a2ce89-8c3a-4b2b-b92a-2ad06f43c401"
      },
      "outputs": [],
      "source": [
        "print(df.head())\n",
        "print(df.head(12))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bb24c84-99be-4ca5-8157-8474886bbd8c"
      },
      "source": [
        "And similarly the \"tail\" function returns the last $n$ values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f91fcc5-8337-44dd-8e00-6ce4db819a4e"
      },
      "outputs": [],
      "source": [
        "print(df.tail(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "273a2b8f-b570-4f61-930d-0263965e1672"
      },
      "source": [
        "You may have noticed in the printed dataframe an additional column of numbers located to the far left of the data. For instance, if we just call df.head() with the default option (which is 5), we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a0a8352-6670-4ebe-ad85-5536b69abf5b"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff14535b-8db7-4c1d-abf2-380701c522ad"
      },
      "source": [
        "That first column is an index column created by Pandas for the dataframe. It starts at $0$ and enumerates from there. Note this column is *not* in our original csv - it's created.\n",
        "\n",
        "In this example, our original csv already has an index column, called ID, that starts at 1, so this additional data column is a bit redundant. We can specify an index column when we read in the data using the \"index_col\" parameter in read_csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82ee6711-e489-48e2-b700-bc7a9793c25e"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f0a2622-a410-4209-bba4-cb49be58f5ea"
      },
      "source": [
        "Now, the index column is the \"ID\" column from the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3885c260-c613-4cef-8db0-bf622d854868"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ff3cb33-06f0-40ee-af16-6e2ff34eeeba"
      },
      "source": [
        "If we only wanted to see the \"title\" column, we could do so as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "050dff20-40c8-4317-9043-fd6ba84678d7"
      },
      "outputs": [],
      "source": [
        "df['title'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e730a088-78be-414b-af2a-5cbc266ba35b"
      },
      "source": [
        "We can specify more than one column in this way as well. Note the *double brackets*. Think of this as the outer brackets accessing the dataframe, while the inner brackets specify a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75790c6e-3620-44e3-8c43-7e63a7a82f46"
      },
      "outputs": [],
      "source": [
        "df[['title', 'first_name']].tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae2c04ae-80b7-42cc-9745-9c4aca791d15"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de4b6432-1c34-483e-91eb-8d93e8bad22d"
      },
      "source": [
        "Note this returns an index object that behaves as an iterable list, so you could, for example, go through the colums with a for loop.\n",
        "\n",
        "You can also find out more information about a dataframe using the \"info\" function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1feccfe-818e-4173-a22c-d8092847f337"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75fca811-b1f7-4292-8e32-d7c49210dbb4"
      },
      "source": [
        "### Dropping or Removing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95012357-e987-480b-bd9c-05ba496f2f49"
      },
      "source": [
        "There are many reasons why you might want to drop or remove data from a dataframe. For example, it could be that only some data is relevant to your analysis. Or, it could be that some data is insufficient or corrupt, and leaving it in would lead to incorrect conclusions. Also, sometimes certain columns aren't of interest to the analysis in question.\n",
        "\n",
        "If we want to drop entire columns, we can use the \"drop\" function and specify the columns as a list in the argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c889c77e-8d6b-498c-81d9-bcf4afc6a645"
      },
      "outputs": [],
      "source": [
        "df.drop(columns = ['birth_place', 'death_place'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26609b10-405b-4a64-a22b-d81485ea2945"
      },
      "source": [
        "However, while the dataframe above only has six columns, if we call it again we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0427866-039b-4527-a97d-467b26455230"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "120c0217-59de-422b-a0bd-e302e1a18290"
      },
      "source": [
        "What?!? I thought we dropped two!\n",
        "\n",
        "What's going on here is that the drop command creates a new dataframe as its output. It *does not* modify the original dataframe. So, for example, we could say:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52a96faf-0653-47df-8731-925e03e5bad0"
      },
      "outputs": [],
      "source": [
        "df2 = df.drop(columns = ['birth_place', 'death_place'])\n",
        "df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fa429fd-7949-4936-9189-8851c7cf101d"
      },
      "source": [
        "Here, df2 is the dataframe with those two columns dropped, while df is the original, unchanged dataframe.\n",
        "\n",
        "If we want to actually make the change to the original dataframe, we can do this with the \"inplace\" argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63878a6b-6510-4670-a470-4698ded367b7"
      },
      "outputs": [],
      "source": [
        "df.drop(columns = ['birth_place', 'death_place'], inplace = True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bad50605-58d4-4919-b5f3-3fe0a0867b0d"
      },
      "source": [
        "This is the same as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9cdc1af-5344-4053-973b-ef661396ba8c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "df = df.drop(columns = ['birth_place', 'death_place'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ac04471-232d-4520-a7f0-af886a0e3a52"
      },
      "source": [
        "You can also drop rows by indicating specific indices with the *index* argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0560e109-79dc-40a2-8897-27e21484bf24"
      },
      "outputs": [],
      "source": [
        "df.drop(index=[4,5,6], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb81377b-eb4e-4269-ae7a-1cf3c695a505"
      },
      "source": [
        "Or, by using df.index, which avoids potential variations in index numbering and always references the first row starting with $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab3a1093-1d93-4bcf-bf7c-85b0c455df45"
      },
      "outputs": [],
      "source": [
        "df.drop(df.index[0], inplace=True)\n",
        "df.drop(df.index[0], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6596fb-e520-4697-a1cc-698369089d7b"
      },
      "source": [
        "Each line drops the first row on the dataframe, whatever that first row might be. So, these two lines together drop the first two rows. We can also use standard Python indexing and slicing notation to specify indices here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d9d056d-9a35-4480-8343-db04bb8d0f27"
      },
      "outputs": [],
      "source": [
        "df.drop(df.index[2:5], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79d096b8-66c3-4a78-85f2-feaf0ec09f48"
      },
      "source": [
        "The \"drop_duplicates\" function can be used to drop duplicate rows, while the \"dropna\" function drops every row that includes at least one NA entry. Be careful with this one, as it could potentially drop a lot of rows!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcb8ea07-b205-4564-a4a2-2e85646cd2d3"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df555446-a12d-4742-b757-863d6c37d731"
      },
      "source": [
        "### Adding, Modifying Data, and Mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea77d54b-881c-4084-a32a-c7873bd85629"
      },
      "source": [
        "Suppose we have a dataframe with NA values, and instead of dropping them we want to fill them with some values we determine. Here are a few ways to do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e853119-f6cc-4023-80f9-b3276e16c3ea"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "# replace ALL NA entries with a fixed value:\n",
        "df.fillna(0, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80ae1393-f72e-4225-8976-2744678cd036"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "# replace the first 2 NA entries in each column with a fixed value:\n",
        "df.fillna(0, limit=2, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02bc6a29-05f3-4943-a8c1-8b7bb193e1bc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "# replace ALL NA first names with a fixed value:\n",
        "df['first_name'].fillna('no first name', inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0d4c834-1b68-49a0-afc1-b679de106fb8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "# replace specific columns with specific values provided by a dictionary:\n",
        "values = {'first_name': 'no_first_name', 'last_name': 'no_last_name', 'sex': 'no_sex', 'title': 'no_title', 'birth_date': 'no_birth_date', 'birth_place': 'no_birth_place', 'death_date': 'no_death_date', 'death_place': 'no_death_place'}\n",
        "df.fillna(value=values, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3723f77a-c590-4b09-8498-d93e1943aac3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "# ffill and pad: from first row to last row, propagate the most recent row that is not an NA forward until next valid row\n",
        "df.ffill(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ce784cd-1220-49fb-914d-27e9db304852"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "# bfill and backfill: like ffill, except from last row to first row\n",
        "df.bfill(inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcadf24b-fc15-4385-893e-ca66818332e0"
      },
      "source": [
        "We can also create new columns from existing ones. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c470389-3fb9-411c-b7e8-25bb09899a13"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "df['full_name'] = df['first_name'] + ' ' + df['last_name']\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f4479a-72d6-4984-9b65-9c55063c130a"
      },
      "source": [
        "This illustrates a problem. Anytime we have an NaN value, the string concatenation is also NaN. How could we get around this? Well, we could create our own specific function that handles this, and then apply that to our dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eabbdfde-574e-43de-831a-75d37fe41f1a"
      },
      "outputs": [],
      "source": [
        "def create_full_name(row):\n",
        "    if isinstance(row['first_name'], str) and isinstance(row['last_name'], str):  # both first_name and last_name are strings\n",
        "        result = row['first_name'] + ' ' + row['last_name']\n",
        "    elif isinstance(row['first_name'], str):  # only first_name is a string\n",
        "        result = row['first_name']\n",
        "    elif isinstance(row['last_name'], str):  # only last_name is a string\n",
        "        result = row['last_name']\n",
        "    else:  # neither first_name nor last_name are strings, they are both NaN\n",
        "        result = np.nan\n",
        "    return result\n",
        "\n",
        "df = pd.read_csv(url, index_col='ID')\n",
        "\n",
        "df['full_name'] = df.apply(create_full_name, axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ca2580-8a6b-41fb-a0ad-fd7f0e5f0197"
      },
      "source": [
        "This \"apply\" operation applies the specified function. You could also use Python lambda functions to create a function inline if needed. Note the option \"axis = 1\" means to process the data row by row. The option \"axis = 0\" would process the data column by column. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b0b2f46-361c-43a6-8591-ed30eba26290"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe that is a 6 x 2 array formed from a list of 12 numbers ordered from 0 to 11.\n",
        "df = pd.DataFrame(np.arange(12).reshape(6,2), columns = ['column 1', 'column 2'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61e299f2-6c20-44ce-af30-b83df2157fe3"
      },
      "outputs": [],
      "source": [
        "# Create a new dataframe that takes the maximum value of each column in the dataframe we just created.\n",
        "new_df = df.apply(lambda column: column.max())\n",
        "print(new_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cb14ac9-b8df-4817-a825-5f9a19066ae4"
      },
      "source": [
        "There are three main functions used to create or change data in dataframes: apply, map, and applymap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee2931ae-fe6c-4ab3-8f56-c5390f757710"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.arange(8).reshape(4,2), columns = ['column 1', 'column 2'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *apply* function can be used to apply a function along either axis of a dataframe."
      ],
      "metadata": {
        "id": "6aWVXYC9mpuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a98b426-d1c2-49ed-8f0d-2df4483c439a"
      },
      "outputs": [],
      "source": [
        "print(df.apply(np.max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2795f539-800e-4238-971c-abb0815db548"
      },
      "outputs": [],
      "source": [
        "print(df.apply(np.max, axis = 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *map* function is a bit more limited and is used to apply a function element-wise to a series. It's more efficient than *apply* when used in this way."
      ],
      "metadata": {
        "id": "wifC8UrqnOGf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6eb233d-6efd-4314-80ce-6e60bb115f64"
      },
      "outputs": [],
      "source": [
        "print(df['column 1'].map(lambda x: x*2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *applymap* function is used for element-wise operations across all elements of a dataframe, not just those within a series."
      ],
      "metadata": {
        "id": "E1iXflLpnmAQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e537aba3-5649-4c76-acac-ee3f84925a50"
      },
      "outputs": [],
      "source": [
        "print(df.applymap(lambda x: x*2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f30070a0-2f01-47c4-b09e-efa38cae0b24"
      },
      "source": [
        "### Changing Datatypes of Series or Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a6ccb6d-271c-472f-ad09-f3178205d482"
      },
      "source": [
        "The datatypes for our \"royal_line\" examples have all been 'objects' because every column has had data that's been interpreted as a string. This is a general, default datatype that is quite encompassing in what it can handle. However, there are some functions, like maximum or average, that make sense for certain types of numeric data, but not for general data, and if we try to apply these functions to objects we'll have a bad time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "519bd940-bb43-496e-9f59-93bb695c1781"
      },
      "outputs": [],
      "source": [
        "# Let's create a simple dataframe with three columns containing different types of data:\n",
        "df = pd.DataFrame({'ints': [1,2,3,4], 'strings': ['a','b','c','d'], 'floats': [1.1, '2.2', '3.3', 4]})\n",
        "print(df)\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ddf3da-b2c2-4790-ad49-b9b4c130554b"
      },
      "source": [
        "Here, the second and third column are interpreted as objects because both contained strings (the values 2.2 and 3.3 in the floats column were entered as strings).\n",
        "\n",
        "To convert these to a different datatype, we can convert a single column, or multiple columns using a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6890b9c-8178-41f2-a7b7-f65ecb49e628"
      },
      "outputs": [],
      "source": [
        "df['floats'] = df['floats'].astype(float)\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9820c35-2fcc-4fa0-9aeb-743764cd1788"
      },
      "outputs": [],
      "source": [
        "convert_dict = {'ints': int, 'strings': str, 'floats': float}\n",
        "df = df.astype(convert_dict)\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcc7104e-5a9f-40cf-8ebf-a9ce40238615"
      },
      "outputs": [],
      "source": [
        "# The following command would also work:\n",
        "df['ints'] = df['ints'].astype(float)\n",
        "print(df)\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbab0f98-2c1e-471e-964a-d02203de8274"
      },
      "outputs": [],
      "source": [
        "# But this one won't:\n",
        "df['strings'] = df['strings'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4aec381-2539-4374-b675-d0045e17dcc8"
      },
      "source": [
        "Pandas has many built in conversion functions (to_datetime, to_timedelta, to_numeric, etc...) but you'll sometimes encounter data that's formatted in such a way that it's not possible to immediately convert it to the format you want using one of the built in functions. To deal with this, sometimes you need to write your own conversion function.\n",
        "\n",
        "For example, if we check out the 'birth_date' column in our royal_line dataset, we see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a4138d8-46a4-4961-88be-4f7c9781b43c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "print(df['birth_date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cfcde0c-8970-49bd-8e7d-5a735401d8c9"
      },
      "source": [
        "A lot of NaN. OK, let's remove these and see what we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4e45386-b275-46b0-a55a-e323db423f0f"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset = ['birth_date'], inplace=True)\n",
        "print(df['birth_date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c80ea057-3a0b-4270-b162-963690201063"
      },
      "source": [
        "If we then try to convert these values to datetimes we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fd6623b-8a79-4db1-9c3c-5b800cb85921"
      },
      "outputs": [],
      "source": [
        "df['birth_date'] = pd.to_datetime(df['birth_date']) # This fails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebf58be3-7345-426b-b6da-4eb4122babc0"
      },
      "source": [
        "This generates errors due to several issues.\n",
        "\n",
        "First, there are entries in the dataset formatted like the following: ABT 751. This notation means that the family history experts believe the person was born about (ABT) 751.\n",
        "\n",
        "The second is an out of bounds nanosecond timestamp error related to Pandas only supporting approximately 580 years in the range from around 1677 to 2262.\n",
        "\n",
        "To get around these issue, we'll write and then apply our own function. Note we're not dropping the NaN values here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9331569f-e76a-4627-b28a-606d939f178e"
      },
      "outputs": [],
      "source": [
        "def get_year(x):\n",
        "    if pd.isna(x):\n",
        "        year_result = np.nan  # if the birth_year is nan then return nan\n",
        "    else:  # checking a number of edge cases in the data and stripping it out:\n",
        "        if \"ABT\" in x:  # for example: ABT  1775\n",
        "            x = x[3:]\n",
        "            x = x.strip()\n",
        "        if \"/\" in x:  #  For example: 1775/1776\n",
        "            x = x[:x.find('/')]\n",
        "        num_spaces = x.count(' ')\n",
        "        if num_spaces == 0:  # only has the year\n",
        "            year_result = int(x)\n",
        "        elif num_spaces == 1:  # example: FEB 1337\n",
        "            x = x[x.rfind(' ') + 1:]  # 'rfind' finds the last space. The 'r' stands for 'reverse.'\n",
        "            if x.isnumeric():\n",
        "                year_result = int(x)\n",
        "            else:  # This could happen if there is only a day and month, like '10 JAN'\n",
        "                year_result = np.nan\n",
        "        elif num_spaces == 2:  # example: 16 FEB 1337\n",
        "            x = x[x.rfind(' ') + 1:]\n",
        "            year_result = int(x)\n",
        "        else:\n",
        "            year_result = np.nan  # There are a few other strange dates that aren't worth our time to fix, so just return nan for those.\n",
        "    return year_result\n",
        "\n",
        "df['birth_year'] = df['birth_date'].map(get_year)\n",
        "\n",
        "print(df['birth_year'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9874cbe-98da-43cc-b683-cceda54e2f25"
      },
      "source": [
        "### Conditionals in Dataframes and Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d83ad58c-ddd0-45de-af0e-0ab6d4fc4364"
      },
      "source": [
        "Conditionals are a very useful feature of Pandas which typically produce a Numpy array of Booleans or a Pandas Boolean series.\n",
        "\n",
        "For example, consider the following code that produces True if the birth_year column (calculated above) is greater than or equal to 1990, and False otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f284c3f3-44e5-4c02-9092-8358591fade5"
      },
      "outputs": [],
      "source": [
        "boolean_mask = (df.birth_year >= 1990)\n",
        "print(boolean_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "887524ad-cfb3-408b-8721-7809cd4aeca5"
      },
      "source": [
        "We can then use this to, for example, only print the entries for which the boolean is True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d98cbd8-ecf1-42c8-96a8-c6d29f810a9f"
      },
      "outputs": [],
      "source": [
        "print(df[boolean_mask][['first_name', 'last_name', 'birth_year']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b27ad5f-eba7-48f9-9269-c3aa268db111"
      },
      "source": [
        "We can combine Boolean expressions using the logical operators & (\"and\"), | (\"or\"), and ~ (\"not\"). For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5072497e-b16f-4209-a9bc-099a1b35c4e2"
      },
      "outputs": [],
      "source": [
        "print(df[(df.birth_year >= 1500) & (df.title.str.contains('Queen'))][['first_name', 'title', 'birth_year']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65223f3d-26df-42bb-8b37-2739eff4496a"
      },
      "source": [
        "### loc and iloc Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d28ffab-caaf-40a5-85a7-e018097f79c0"
      },
      "source": [
        "One of the most common tasks for data scientists is filtering information to more efficiently derive actionable insights. Marketers also like saying things like that.\n",
        "\n",
        "We've seen the \"head\" and \"tail\" functions, which provide a quick, truncated view of the beginning or end, respectively, of the dataframe or series. But what if you're interested in examining results that are not necessarily at the very beginning or end of the dataset.\n",
        "\n",
        "For this purpose, the loc function is designed to access rows and columns by label. In contrast, the iloc function is used to access rows and columns by integer value - the \"i\" stands for \"integer\".\n",
        "\n",
        "A quick example of the difference is illustrated below, where both commands do the same thing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "319b2da6-b061-463b-afb1-d7cccfee40e2"
      },
      "outputs": [],
      "source": [
        "print(df.loc[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0379c05-3a0b-4a9b-acb2-2b681e5857ff"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77b533f4-7b8d-4319-97a4-34bb363c199c"
      },
      "source": [
        "However, the following will produce an error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f968edf-6c88-4025-bea2-9fb153afe95f"
      },
      "outputs": [],
      "source": [
        "print(df.loc[0]) #This fails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7089a36e-0975-444a-aabc-681345bbef34"
      },
      "source": [
        "Because there is no row with label 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ae7c61-e041-4945-ad75-48878aa18397"
      },
      "source": [
        "Now, row indices (labels) don't need to be unique. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d795d66a-7d1f-482d-b87f-d2c78476913b"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.arange(10).reshape(5, 2), columns=['A', 'B'], index=['cat', 42, 'stone', 42, 12345])# Five rows each with an associated index\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bea90b54-d3e1-423c-8e15-cfab251fa88a"
      },
      "source": [
        "The index \"42\" appears twice, and some indices are numbers, while some are strings. Let's look at some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faa1ed1f-4f5e-4e39-8e62-585e58953c8a"
      },
      "outputs": [],
      "source": [
        "print(df.loc[12345])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b357af6-dc91-4acb-a3c6-fb2d1c282a2a"
      },
      "outputs": [],
      "source": [
        "print(df.loc['stone'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d39c236-b1c2-4d71-8f0d-a205a3def9eb"
      },
      "outputs": [],
      "source": [
        "print(df.loc[42])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0068e31a-5b7d-4ea0-9aa6-810a06aed3c2"
      },
      "outputs": [],
      "source": [
        "print(df.loc['A']) #This will fail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "461c326d-edc3-4dfc-aa94-54b1b132e103"
      },
      "outputs": [],
      "source": [
        "print(df.loc['cat':'stone'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bcaea12-7f75-4970-989c-fe66aa2d9908"
      },
      "outputs": [],
      "source": [
        "print(df.loc[['cat','stone']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "854ce347-18c8-443a-bbb3-b0a463cedcc8"
      },
      "outputs": [],
      "source": [
        "print(df.loc['stone', 'B'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "067bf67c-561e-459b-ba9e-0954304eba0a"
      },
      "outputs": [],
      "source": [
        "print(df.loc[df['A'] > 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bc6b9cd-fa6b-415b-91f1-46fc6e92c75f"
      },
      "source": [
        "Now let's take a look at some iloc examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92ecf740-998a-4b97-a835-a461717302ff"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7600da8f-87ee-46ca-99b8-4134c018f206"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ae548e-b769-4039-9067-79c1464fcace"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[[0,2,4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "113c4b23-4641-4a76-b630-f895968eec0e"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84851fa7-8695-4926-a0b7-331af4f42aad"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[0:3,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b7543b0-2786-4e49-aee5-36d664874643"
      },
      "source": [
        "Returning to the royal family history data as an example, let's create a new column named \"era\". The \"era\" column signifies if a person was born in one of three distinct time periods: 'ancient', 'middle_years', or 'modern'. The following creates a new column and initially assigns the value 'unknown' to every entry within it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d81bf457-469b-472d-9be9-ece9a9584b25"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "df['birth_year'] = df['birth_date'].map(get_year)\n",
        "df['era'] = 'unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "167a9716-28bd-439a-bdee-90a03282eb00"
      },
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327c8a9a-a070-45c3-90a9-13460be2f0ea"
      },
      "source": [
        "The next question is how to divide the birth years. If we check out their maximum and minimum values, we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "573260ad-ff91-4ad9-b27a-76d98fbcdce3"
      },
      "outputs": [],
      "source": [
        "print(f\"The earliest year = {df['birth_year'].min()} and the latest year = {df['birth_year'].max()}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94cfa7ed-5d78-47f7-aeca-67e63ac771d8"
      },
      "source": [
        "So, 686 is the earliest year, and 1991 is the latest. This is a difference of 1991 - 656 = 1305 years, which if we divide by 3 this gives us 435 years per era. So, the \"ancient\" royals are those born between 686 and 1121, the \"middle_years\" royals are those born between 1121 and 1555, and the \"modern\" royals are those born after 1555. (Not all that modern!) We can assign these three eras with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0068ddf2-0f86-467c-8296-cd4f80d92c25"
      },
      "outputs": [],
      "source": [
        "df.loc[df['birth_year'] < 1122, 'era'] = 'ancient'  # 686 – 1121\n",
        "df.loc[(df['birth_year'] >= 1122) & (df['birth_year'] <= 1555), 'era'] = 'middle_years'  # 1122 – 1555\n",
        "df.loc[df['birth_year'] > 1555, 'era'] = 'modern'  # after 1555\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ceba278-db49-45b1-897f-b6086da1a254"
      },
      "source": [
        "We could have also done this using a custom function and the \"map\" utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5cb2f42-9cd2-4ee3-acd7-6a256a071696"
      },
      "source": [
        "### Reshaping with Pivot, Pivot_Table, Groupby, and Transpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "756b6461-07b4-4d8d-bdff-8929618869ff"
      },
      "source": [
        "Frequently it is convenient or informative to restructure data contained in a dataframe, effectively organizing the data into a different shape or format. This section will cover the most common reshaping functions provided by Pandas.\n",
        "\n",
        "The pivot function addresses the situation in which separate categories of a dataset feature are enumerated and highlighted using a cross tabular format. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ace91f6c-37ad-4512-8a51-cb58c106ceb9"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'Car': [1, 1, 2, 2],\n",
        "                   'Type': ['new', 'used', 'new', 'used'],\n",
        "                   'Price': [10, 5, 12, 7]})\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd19ee9b-3c66-4c23-9d6c-d3c293de8123"
      },
      "source": [
        "Calling the pivot function on this dataframe will reword the data into a more compact and usable format. In the example above, we want to reshape the data such that each car brand is represented on a single row. A use case for this particular reorganization would be a car salesperson who needs to quickly view all the prices of a given car brand for the different 'Type' categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef132774-64d0-451a-95aa-2163987debe3"
      },
      "outputs": [],
      "source": [
        "p = df.pivot(index='Car', columns='Type', values='Price')\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868611f7-6fb4-44ce-9abe-d5f717601e49"
      },
      "source": [
        "The pivot function only works if there is either zero or one entries per cell in the result. Suppose we have the following dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9306a1d9-6a6e-4080-881d-1e282a42242e"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'Car': [1, 1, 2, 2, 2],\n",
        "                   'Type': ['new', 'used', 'new', 'used', 'used'],\n",
        "                   'Price': [10, 5, 12, 7, 6]})\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ceb0a4-6648-44db-a401-a2dbde89ef59"
      },
      "source": [
        "Invoking the pivot function on this dataframe will generate an error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4447587-e73d-41a9-8099-a200cecf99e6"
      },
      "outputs": [],
      "source": [
        "p = df.pivot(index='Car', columns='Type', values='Price') #This will fail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81e495e-a25a-42c8-8d12-58c37a69a80a"
      },
      "source": [
        "The reason for this is there are two price entries for the used version of car 2. In this case what we could do is use the \"pivot_table\" function with an aggregator, which specifies how to combine values when more than 1 occurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38956dc7-1293-4af9-b72d-a6bd2cf4342a"
      },
      "outputs": [],
      "source": [
        "p = df.pivot_table(index='Car', columns='Type', values='Price', aggfunc=np.mean)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3637a44-ff6f-43ac-8c48-df8d561f37a1"
      },
      "source": [
        "Pivot tables can result in immensely complex tabular formats with multiple indexes, multiple columns, and various aggregation functions specified. Today, we demonstrate only the basic single-index, single-column case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec939a61-a9a6-491f-b1db-b9fbdb7d3c49"
      },
      "source": [
        "Here's another example of a pivot_table using our royal family history dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eac1a6b0-526c-4b45-8618-cbb610625c93"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(url, index_col='ID')\n",
        "df['birth_year'] = df['birth_date'].map(get_year)\n",
        "\n",
        "df.dropna(inplace=True, subset=['title', 'sex', 'birth_year'])\n",
        "p = df.pivot_table(index='title', columns='sex', values='birth_year', aggfunc='mean')\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66a2aedc-a075-4c95-8716-bca21ed7542a"
      },
      "source": [
        "Here are two more examples. The first fills blank entries in the resulting pivot table after aggregation with $0$ instead of NaN, and uses two aggregate functions, mean and count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0701ade-9595-40f7-ada7-a730a5191edd"
      },
      "outputs": [],
      "source": [
        "p = df.pivot_table(index='title', columns='sex', values='birth_year', aggfunc=['mean', 'count'], fill_value=0)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dff160ec-5473-4f0a-bf31-798f6908b3f3"
      },
      "source": [
        "The second is similar to the first, but instead declares two indexes and two columns producing a much more complicated, nested output result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c2e14b5-7f09-4aa5-8cf9-ec225eb8dec5"
      },
      "outputs": [],
      "source": [
        "p = df.pivot_table(index=['title', 'first_name'], columns=['sex', 'last_name'], values='birth_year', aggfunc=['mean', 'count'], fill_value=0)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bde3f764-e02d-4289-9267-b22b7b3b8e57"
      },
      "source": [
        "The groupby function's recasting of information is very similar to that of the pivot_table function. In general, the main difference is how the resulting output is shaped. Note it's a common mistake to create a group object without specifying an aggregating function like mean, sum, or std.\n",
        "\n",
        "Consider the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14213b7b-daca-4e93-88d5-83ed7b150bad"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'Car': [1, 1, 2, 2, 2],\n",
        "                   'Type': ['new', 'used', 'new', 'used', 'used'],\n",
        "                   'Price': [10, 5, 12, 7, 6]})\n",
        "\n",
        "g = df.groupby(by='Car')\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441819a3-b014-41d2-a386-b51f7ddf12ae"
      },
      "source": [
        "That's not particularly helpful. However, if we group by 'Car' and invoke the 'mean' function, we obtain a more useful result."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = df.groupby(by='Car').mean() #Thil will fail\n",
        "print(g)"
      ],
      "metadata": {
        "id": "VijbcPQn_7fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whoa! What happened there? Well, it's trying to apply the aggregate function to both the price, which is fine, and the type, which is not. This used to result in a warning and dropping the type column, but in more recent versions of Pandas it gives an error.\n",
        "\n",
        "How can we get around this? Well, we can drop the *Type* column first."
      ],
      "metadata": {
        "id": "AA977Hde_-an"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "978c04b3-45a2-4598-accd-618b31a1917a"
      },
      "outputs": [],
      "source": [
        "g = df[['Car','Price']].groupby(by='Car').mean() #Need to toss out the \"Type\" column, or it will try to take its mean.\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74dec7c6-a91d-42c5-ab9f-d22fafc62598"
      },
      "source": [
        "If instead of mean we wanted to use the more robust count we don't need to toss out the type:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65f9fd99-6504-4e9f-9223-b000ccf94118"
      },
      "outputs": [],
      "source": [
        "g = df.groupby(by='Car').count()\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2ea6ce-11cd-4fda-b0d2-ed70b2d5f6f0"
      },
      "source": [
        "If we wanted to group by both 'Car' and 'Type', and use two different aggregation function, we could do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb46c83a-9d69-4fcf-b7e0-a671fc187c28"
      },
      "outputs": [],
      "source": [
        "g = df.groupby(by=['Car','Type']).agg(['mean','count'])\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "418f6c3e-0347-4dfb-99ab-8a1785f16809"
      },
      "source": [
        "Finally, and with NumPy arrays, the transpose function (or simply T) transposes a dataframe. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2814ac4-c10f-4c0f-8f1b-af8aca43ae2c"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.arange(6).reshape(3, 2), columns=['A', 'B'])\n",
        "print(f'Original\\n{df}')\n",
        "\n",
        "df = df.transpose()  # or df.T\n",
        "\n",
        "print(f'\\nTransposed:\\n{df}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "0d0kk2pldg4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of most of the lecture notes I like to provide references for further reading. Please note these are generally here in case you're interested, but you're not required to check them out.\n",
        "\n",
        "Sometimes they may explore a topic in more depth, sometimes they might provide additional learning resources, and sometimes they might just be fun (I'll try to provide a link to a song each lecture).\n",
        "\n",
        "* [Introduction to NumPy](https://numpy.org/doc/stable/user/absolute_beginners.html)\n",
        "\n",
        "* [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
        "\n",
        "* Song Of The Day (SOTD) - [Royals](https://youtu.be/nlcIKh6sBtcsi=duDiNtjAzRyh5OUJ) by Lorde"
      ],
      "metadata": {
        "id": "ZeYCZ3wVeE5f"
      }
    }
  ]
}